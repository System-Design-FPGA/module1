{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"IoT Edge Computing Framework All the course material will be provided in power point slides format. you can find them here . Course Instructor: Amin Sahebi Email: sahebi.amin@gmail.com In any case you need furthur explaination or material Shoot Me an Email. Note Each individual needs to discuss about the final presentation/seminar topic. We will discuss generally for all aspects, a list of topics are available at the beginning of the course. If you are confident about the topic you can go forward and register your name for it. Otherwise, we need to discuss and find the alternative subjects. Tutorials and webpages These tutorials help to extend your knowledge about vivado and Xilinx projects, I suggest to have a look and try some of their in your leisure time if you want to learn more! instruction to build a simple IoT framework using RPi Xilinx Zynq-7000 Confluence Xilinx Zynq-7000 Example Designs Xilinx Zynq-7000 Embedded Design Tutorial Sample codes There are some sample codes prepared for the course and is aligned with the curriculum, Session1, Session2, etc. You can download or clone them and study them before the session started! Please note that these resources are sample! Note using these resources in your final project is allowed! Download Sample Design Materials in an archived file or Clone the Github repository Course outcome By the end of the module, students should be able to: Beside the practical use of VHDL/Verilog, understanding of the theory of combination and sequential circuits and how these combine in the design of digital computing circuits. Evaluate the digital design flow as currently practised professionally in industry, with reference to FPGAs, and at a more general level, to custom reconfigurable architectures. Develop and implement a real-world program which augmented with the basic IP blocks created through the course and can present a real-world activity. Transferable skills Learn how to deal with modern architectures, modern boards and e-learning methods. Communicate (written and oral; to technical and non-technical audiences) and work with others. Plan self-learning and improve performance, as the foundation for lifelong learning/CPD. Exercise initiative and personal responsibility, including time management, which may be as a team member or leader. Overcome difficulties by employing skills, knowledge and understanding in a flexible manner.","title":"Home"},{"location":"#iot-edge-computing-framework","text":"All the course material will be provided in power point slides format. you can find them here . Course Instructor: Amin Sahebi Email: sahebi.amin@gmail.com In any case you need furthur explaination or material Shoot Me an Email. Note Each individual needs to discuss about the final presentation/seminar topic. We will discuss generally for all aspects, a list of topics are available at the beginning of the course. If you are confident about the topic you can go forward and register your name for it. Otherwise, we need to discuss and find the alternative subjects.","title":"IoT Edge Computing Framework"},{"location":"#tutorials-and-webpages","text":"These tutorials help to extend your knowledge about vivado and Xilinx projects, I suggest to have a look and try some of their in your leisure time if you want to learn more! instruction to build a simple IoT framework using RPi Xilinx Zynq-7000 Confluence Xilinx Zynq-7000 Example Designs Xilinx Zynq-7000 Embedded Design Tutorial","title":"Tutorials and webpages"},{"location":"#sample-codes","text":"There are some sample codes prepared for the course and is aligned with the curriculum, Session1, Session2, etc. You can download or clone them and study them before the session started! Please note that these resources are sample! Note using these resources in your final project is allowed! Download Sample Design Materials in an archived file or Clone the Github repository","title":"Sample codes"},{"location":"#course-outcome","text":"By the end of the module, students should be able to: Beside the practical use of VHDL/Verilog, understanding of the theory of combination and sequential circuits and how these combine in the design of digital computing circuits. Evaluate the digital design flow as currently practised professionally in industry, with reference to FPGAs, and at a more general level, to custom reconfigurable architectures. Develop and implement a real-world program which augmented with the basic IP blocks created through the course and can present a real-world activity.","title":"Course outcome"},{"location":"#transferable-skills","text":"Learn how to deal with modern architectures, modern boards and e-learning methods. Communicate (written and oral; to technical and non-technical audiences) and work with others. Plan self-learning and improve performance, as the foundation for lifelong learning/CPD. Exercise initiative and personal responsibility, including time management, which may be as a team member or leader. Overcome difficulties by employing skills, knowledge and understanding in a flexible manner.","title":"Transferable skills"},{"location":"activities/","text":"Objectives and activities Each session of the source includes a Laboratory and coursework. The students are suppose to learn the objectives within the provided hands-on and do the laboratory based on what they have learned. For the laboratory project you will connect through the Laboratory within you credentials, then you will be divided in group of 4 people, you should be able to do the lab project within the time given to you. The project for each session will be given and discussed in the course and you are supposed to fill the uncompleted project and execute it properly. Note If in any case you couldn't make the project working, do not worry, you still have time to make it work and submit your results before starting of the next session.","title":"CV"},{"location":"activities/#objectives-and-activities","text":"Each session of the source includes a Laboratory and coursework. The students are suppose to learn the objectives within the provided hands-on and do the laboratory based on what they have learned. For the laboratory project you will connect through the Laboratory within you credentials, then you will be divided in group of 4 people, you should be able to do the lab project within the time given to you. The project for each session will be given and discussed in the course and you are supposed to fill the uncompleted project and execute it properly. Note If in any case you couldn't make the project working, do not worry, you still have time to make it work and submit your results before starting of the next session.","title":"Objectives and activities"},{"location":"housekeeping/","text":"Housekeeping for ABP core tools Guidelines ABP core codes (listed here ) should fulfil guidelines provided here . Survey Situation and progress are tracked in this spreadsheet .","title":"Housekeeping for ABP core tools"},{"location":"housekeeping/#housekeeping-for-abp-core-tools","text":"","title":"Housekeeping for ABP core tools"},{"location":"housekeeping/#guidelines","text":"ABP core codes (listed here ) should fulfil guidelines provided here .","title":"Guidelines"},{"location":"housekeeping/#survey","text":"Situation and progress are tracked in this spreadsheet .","title":"Survey"},{"location":"meetings/","text":"Meetings The agenda and Zoom links of ABP Computing Meetings can be find in the corresponding indico category . Notes from the meetings can be found here . e-group You can be included in the meetings invitation list by subscribing to the abp-cp-meetings e-group at this link .","title":"Meetings"},{"location":"meetings/#meetings","text":"The agenda and Zoom links of ABP Computing Meetings can be find in the corresponding indico category . Notes from the meetings can be found here .","title":"Meetings"},{"location":"meetings/#e-group","text":"You can be included in the meetings invitation list by subscribing to the abp-cp-meetings e-group at this link .","title":"e-group"},{"location":"trello/","text":"Trello tasks The best website to learn Trello and how it does work is the following website, Please keep reading it and once the invitation sent to you, follow based on the instructions. https://trello.com/en/guide","title":"Trello tasks"},{"location":"trello/#trello-tasks","text":"The best website to learn Trello and how it does work is the following website, Please keep reading it and once the invitation sent to you, follow based on the instructions. https://trello.com/en/guide","title":"Trello tasks"},{"location":"webtodo/","text":"A todo list for this website We need a basic git guide,eg https://product.hubspot.com/blog/git-and-github-tutorial-for-beginners","title":"A todo list for this website"},{"location":"webtodo/#a-todo-list-for-this-website","text":"We need a basic git guide,eg https://product.hubspot.com/blog/git-and-github-tutorial-for-beginners","title":"A todo list for this website"},{"location":"codes/softwarelist_core/","text":"Core codes developed by ABP Optics design MAD-X MAD-NG cpymad Single-particle tracking Xsuite SixTrack SixDesk Sixtracklib Mask/pymask DistLib ( repo ) Frequency analysis harpy NAFFLIB Collective effects macroparticle tools PyHEADTAIL COMBI PyECLOUD PyPIC RFTrack Collective effect Vlasov solvers and tools DELPHI PySSD PyRADISE BimBim Impedance computation tools ImpedanceWake2D Tools for linear colliders and recirculating machines (ER-Linacs, Combiner Rings) Guinea Pig Placet Placet 2 MapClass Tools for luminosity modeling LumiMod CTE MBS Tools for hadron linacs PATH [TO BE FILLED!]","title":"Core codes developed by ABP"},{"location":"codes/softwarelist_core/#core-codes-developed-by-abp","text":"","title":"Core codes developed by ABP"},{"location":"codes/softwarelist_core/#optics-design","text":"MAD-X MAD-NG cpymad","title":"Optics design"},{"location":"codes/softwarelist_core/#single-particle-tracking","text":"Xsuite SixTrack SixDesk Sixtracklib Mask/pymask DistLib ( repo )","title":"Single-particle tracking"},{"location":"codes/softwarelist_core/#frequency-analysis","text":"harpy NAFFLIB","title":"Frequency analysis"},{"location":"codes/softwarelist_core/#collective-effects-macroparticle-tools","text":"PyHEADTAIL COMBI PyECLOUD PyPIC RFTrack","title":"Collective effects macroparticle tools"},{"location":"codes/softwarelist_core/#collective-effect-vlasov-solvers-and-tools","text":"DELPHI PySSD PyRADISE BimBim","title":"Collective effect Vlasov solvers and tools"},{"location":"codes/softwarelist_core/#impedance-computation-tools","text":"ImpedanceWake2D","title":"Impedance computation tools"},{"location":"codes/softwarelist_core/#tools-for-linear-colliders-and-recirculating-machines-er-linacs-combiner-rings","text":"Guinea Pig Placet Placet 2 MapClass","title":"Tools for linear colliders and recirculating machines (ER-Linacs, Combiner Rings)"},{"location":"codes/softwarelist_core/#tools-for-luminosity-modeling","text":"LumiMod CTE MBS","title":"Tools for luminosity modeling"},{"location":"codes/softwarelist_core/#tools-for-hadron-linacs","text":"PATH [TO BE FILLED!]","title":"Tools for hadron linacs"},{"location":"codes/softwarelist_external/","text":"Beam physics software tools - external ABCI ACE3P CST particle studio GdFiDl HFSS MOSES Ninja Onix PyORBIT IBSimu pytimber","title":"Beam physics software tools - external"},{"location":"codes/softwarelist_external/#beam-physics-software-tools-external","text":"ABCI ACE3P CST particle studio GdFiDl HFSS MOSES Ninja Onix PyORBIT IBSimu pytimber","title":"Beam physics software tools - external"},{"location":"codes/softwarelist_other/","text":"New developments, R&D and legacy tools TLWall PyRADISE PHOTON pyoptics TRAIN SIRE BimBim Fastion Footprint viewer LHC Online Model PageStore SUSSIX","title":"New developments, R&D and legacy tools"},{"location":"codes/softwarelist_other/#new-developments-rd-and-legacy-tools","text":"TLWall PyRADISE PHOTON pyoptics TRAIN SIRE BimBim Fastion Footprint viewer LHC Online Model PageStore SUSSIX","title":"New developments, R&amp;D and legacy tools"},{"location":"codes/codes_pages/ABCI/","text":"ABCI Short description ABCI (Azimuthal Beam Cavity Interaction) is a code used for impedance and wakefield calculations created and maintained by Yong Ho Chin (KEK, Japan). It is a time domain solver of electromagnetic fields when a bunched beam goes through an axi-symmetric structure (on or off axis). An arbitrary charge distribution can be defined by the user (default=Gaussian). Web resources Home page and source code: http://abci.kek.jp/abci.htm Technical information Operating systems: Linux and Windows (works at least on XP and 7) Other information Developed by: Y. H. Chin License: freeware Contact persons at CERN: Benoit Salvant Being actively developed and supported: Yes","title":"**ABCI**"},{"location":"codes/codes_pages/ABCI/#abci","text":"","title":"ABCI"},{"location":"codes/codes_pages/ABCI/#short-description","text":"ABCI (Azimuthal Beam Cavity Interaction) is a code used for impedance and wakefield calculations created and maintained by Yong Ho Chin (KEK, Japan). It is a time domain solver of electromagnetic fields when a bunched beam goes through an axi-symmetric structure (on or off axis). An arbitrary charge distribution can be defined by the user (default=Gaussian).","title":"Short description"},{"location":"codes/codes_pages/ABCI/#web-resources","text":"Home page and source code: http://abci.kek.jp/abci.htm","title":"Web resources"},{"location":"codes/codes_pages/ABCI/#technical-information","text":"Operating systems: Linux and Windows (works at least on XP and 7)","title":"Technical information"},{"location":"codes/codes_pages/ABCI/#other-information","text":"Developed by: Y. H. Chin License: freeware Contact persons at CERN: Benoit Salvant Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/ACE3P/","text":"ACE3P Short description ACE3P is a complete package for electrodynamics simulations in accelerator components, including eigenmode (Omega3P), S-parameters (S3P), Time domain wakefields etc. (T3P), Particle in cell injectors etc. (Pic3P), Tracking charged particles in eigenmode fields for multipacting and dark current (Track3P), and multi-physics for detuning due to thermal expansion from wall losses and mechanical forces (Temp3P). The solvers run on the external computer clusters at NERSC . For post-processing, the package uses ParaView , which can be installed through the package manager on most Linux systems, in addition to the \"acdtool\" post-processor. For mesh generation, the package uses Trellis or Cubit - please contact the contact persons for more information. Trellis can be installed on most Linux systems as a .rpm (Fedora, Scientific Linux and other RedHat based systems) or .deb (Debian, Ubuntu, etc.), and can also be installed on most Windows and Mac systems. Web resources ACE3P Homepage at SLAC Technical information Programming Languages used for implementation: C++ (mainly) Parallelization strategy: MPI Operating systems: Solvers: Linux ACDTOOL pre- and post-processing: Linux Paraview: Any modern desktop OS Trellis/CUBIT: Any modern desktop OS Other prerequisites: You need a NERSC account in order to run the solvers. Other information Developed by: The SLAC ACD (advanced computations) group License: Unclear, source code private to SLAC Contact persons: https://phonebook.cern.ch/phonebook/#search/?query=Kyrre+Ness+Sjobaek+BE-ABP-HSS (ABP) and Nikolay Schwerg (in general at CERN) Being actively developed and supported: Yes Main.KyrreSjobak - 2016-11-25","title":"**ACE3P**"},{"location":"codes/codes_pages/ACE3P/#ace3p","text":"","title":"ACE3P"},{"location":"codes/codes_pages/ACE3P/#short-description","text":"ACE3P is a complete package for electrodynamics simulations in accelerator components, including eigenmode (Omega3P), S-parameters (S3P), Time domain wakefields etc. (T3P), Particle in cell injectors etc. (Pic3P), Tracking charged particles in eigenmode fields for multipacting and dark current (Track3P), and multi-physics for detuning due to thermal expansion from wall losses and mechanical forces (Temp3P). The solvers run on the external computer clusters at NERSC . For post-processing, the package uses ParaView , which can be installed through the package manager on most Linux systems, in addition to the \"acdtool\" post-processor. For mesh generation, the package uses Trellis or Cubit - please contact the contact persons for more information. Trellis can be installed on most Linux systems as a .rpm (Fedora, Scientific Linux and other RedHat based systems) or .deb (Debian, Ubuntu, etc.), and can also be installed on most Windows and Mac systems.","title":"Short description"},{"location":"codes/codes_pages/ACE3P/#web-resources","text":"ACE3P Homepage at SLAC","title":"Web resources"},{"location":"codes/codes_pages/ACE3P/#technical-information","text":"Programming Languages used for implementation: C++ (mainly) Parallelization strategy: MPI Operating systems: Solvers: Linux ACDTOOL pre- and post-processing: Linux Paraview: Any modern desktop OS Trellis/CUBIT: Any modern desktop OS Other prerequisites: You need a NERSC account in order to run the solvers.","title":"Technical information"},{"location":"codes/codes_pages/ACE3P/#other-information","text":"Developed by: The SLAC ACD (advanced computations) group License: Unclear, source code private to SLAC Contact persons: https://phonebook.cern.ch/phonebook/#search/?query=Kyrre+Ness+Sjobaek+BE-ABP-HSS (ABP) and Nikolay Schwerg (in general at CERN) Being actively developed and supported: Yes Main.KyrreSjobak - 2016-11-25","title":"Other information"},{"location":"codes/codes_pages/BimBim/","text":"BimBim (Beam-Beam and IMpedance) Short description Semi-analytical derivation of the coherent modes of oscillation based on the derivation and diagonalisation of the coherent one turn map for multiple bunches of one or two beams including the effect of the lattice (Q, Q' and Q''), the transverse feedback, head-on beam-beam interaction with a crossing angle, long-range beam-beam interactions and the transverse dipolar and quadrupolar impedance. Web resources Sources CWG presentation Technical information Programming Languages used for implementation: Python 2.6 or higher Operating systems: tested exclusivey on Linux (Ubuntu 12.04 and SLC 5) Other prerequisites: Libraries: numpy, scipy Other informations Developed by : CERN License : CERN Copyright Contact persons : Xavier Buffat","title":"**BimBim (Beam-Beam and IMpedance)**"},{"location":"codes/codes_pages/BimBim/#bimbim-beam-beam-and-impedance","text":"","title":"BimBim (Beam-Beam and IMpedance)"},{"location":"codes/codes_pages/BimBim/#short-description","text":"Semi-analytical derivation of the coherent modes of oscillation based on the derivation and diagonalisation of the coherent one turn map for multiple bunches of one or two beams including the effect of the lattice (Q, Q' and Q''), the transverse feedback, head-on beam-beam interaction with a crossing angle, long-range beam-beam interactions and the transverse dipolar and quadrupolar impedance.","title":"Short description"},{"location":"codes/codes_pages/BimBim/#web-resources","text":"Sources CWG presentation","title":"Web resources"},{"location":"codes/codes_pages/BimBim/#technical-information","text":"Programming Languages used for implementation: Python 2.6 or higher Operating systems: tested exclusivey on Linux (Ubuntu 12.04 and SLC 5) Other prerequisites: Libraries: numpy, scipy","title":"Technical information"},{"location":"codes/codes_pages/BimBim/#other-informations","text":"Developed by : CERN License : CERN Copyright Contact persons : Xavier Buffat","title":"Other informations"},{"location":"codes/codes_pages/COMBI/","text":"COMBI (COherent Multibunch Beam-beam Interactions) Short description COMBI stands for COherent Multibunch Beam-beam Interaction. The code simulates interactions of two counter rotating beams. Several \u201cactions\u201d can be included in the simulation: linear transport, head-on and long-range beam-beam interactions, noise sources, collimators, impedances, linear detuning, transverse feedback, synctrotron radiation. Web resources Sources Technical information Programming Languages used for implementation: FORTRAN, C and C++ Operating systems: tested exclusivey on Linux (Ubuntu 12.04,18.04, SLC 5 and CENTOS7) Other prerequisites: Libraries: fftw, GSL Other informations Developed by : CERN License : CERN Copyright Contact persons : Xavier Buffat","title":"**COMBI (COherent Multibunch Beam-beam Interactions)**"},{"location":"codes/codes_pages/COMBI/#combi-coherent-multibunch-beam-beam-interactions","text":"","title":"COMBI (COherent Multibunch Beam-beam Interactions)"},{"location":"codes/codes_pages/COMBI/#short-description","text":"COMBI stands for COherent Multibunch Beam-beam Interaction. The code simulates interactions of two counter rotating beams. Several \u201cactions\u201d can be included in the simulation: linear transport, head-on and long-range beam-beam interactions, noise sources, collimators, impedances, linear detuning, transverse feedback, synctrotron radiation.","title":"Short description"},{"location":"codes/codes_pages/COMBI/#web-resources","text":"Sources","title":"Web resources"},{"location":"codes/codes_pages/COMBI/#technical-information","text":"Programming Languages used for implementation: FORTRAN, C and C++ Operating systems: tested exclusivey on Linux (Ubuntu 12.04,18.04, SLC 5 and CENTOS7) Other prerequisites: Libraries: fftw, GSL","title":"Technical information"},{"location":"codes/codes_pages/COMBI/#other-informations","text":"Developed by : CERN License : CERN Copyright Contact persons : Xavier Buffat","title":"Other informations"},{"location":"codes/codes_pages/CSTParticleStudio/","text":"CST Particle Studio Short description 3D electromagnetic simulations with and without exciting source beam (wakefield solver, eigenmode solver, time domain solver, frequency domain solver) Web resources Installer: installation through CMF or from \\\\cern.ch\\DFS\\Applications\\CST Product webpage: https://www.cst.com/ Technical information Parallelization strategy: MPI (requires license) and multithreading (\"for free\") Operating systems: Windows and Linux (optimized so far primarily for Windows) Other prerequisites: requires valid viewer, solver and acceleration licenses can run on standard PC for small scale study, but requires dedicated servers for large scale. Other information License: commercial license Contact persons: Benoit Salvant, Monika Balk (at CST), and support line via tickets Being actively developed and supported: Yes, by the company","title":"**CST Particle Studio**"},{"location":"codes/codes_pages/CSTParticleStudio/#cst-particle-studio","text":"","title":"CST Particle Studio"},{"location":"codes/codes_pages/CSTParticleStudio/#short-description","text":"3D electromagnetic simulations with and without exciting source beam (wakefield solver, eigenmode solver, time domain solver, frequency domain solver)","title":"Short description"},{"location":"codes/codes_pages/CSTParticleStudio/#web-resources","text":"Installer: installation through CMF or from \\\\cern.ch\\DFS\\Applications\\CST Product webpage: https://www.cst.com/","title":"Web resources"},{"location":"codes/codes_pages/CSTParticleStudio/#technical-information","text":"Parallelization strategy: MPI (requires license) and multithreading (\"for free\") Operating systems: Windows and Linux (optimized so far primarily for Windows) Other prerequisites: requires valid viewer, solver and acceleration licenses can run on standard PC for small scale study, but requires dedicated servers for large scale.","title":"Technical information"},{"location":"codes/codes_pages/CSTParticleStudio/#other-information","text":"License: commercial license Contact persons: Benoit Salvant, Monika Balk (at CST), and support line via tickets Being actively developed and supported: Yes, by the company","title":"Other information"},{"location":"codes/codes_pages/DELPHI/","text":"DELPHI Short description DELPHI (Discrete Expansion over Laguerre Polynomials and HeadtaIl modes) allows to evaluate the transverse beam stability wrt. the machine impedance. It is a semi-analytical Vlasov solver which allows to perform fast scans overs parameters such as chromaticity, bunch intensity, damper gain... Web resources Source code: GitLab Code presentation: PDF presentation by N.Mounet Technical information Programming Languages used for implementation: C++, Python Operating systems: Linux (SLC5, Ubuntu 12.04+) Other prerequisites: Python 2.7 NumPy, SciPy (installation of Anaconda2 recommended) Other information Developed by: N.Mounet, N.Biancacci, D.Amorim License: CERN Copyright Contact persons: N.Mounet Being actively developed and supported: Yes","title":"**DELPHI**"},{"location":"codes/codes_pages/DELPHI/#delphi","text":"","title":"DELPHI"},{"location":"codes/codes_pages/DELPHI/#short-description","text":"DELPHI (Discrete Expansion over Laguerre Polynomials and HeadtaIl modes) allows to evaluate the transverse beam stability wrt. the machine impedance. It is a semi-analytical Vlasov solver which allows to perform fast scans overs parameters such as chromaticity, bunch intensity, damper gain...","title":"Short description"},{"location":"codes/codes_pages/DELPHI/#web-resources","text":"Source code: GitLab Code presentation: PDF presentation by N.Mounet","title":"Web resources"},{"location":"codes/codes_pages/DELPHI/#technical-information","text":"Programming Languages used for implementation: C++, Python Operating systems: Linux (SLC5, Ubuntu 12.04+) Other prerequisites: Python 2.7 NumPy, SciPy (installation of Anaconda2 recommended)","title":"Technical information"},{"location":"codes/codes_pages/DELPHI/#other-information","text":"Developed by: N.Mounet, N.Biancacci, D.Amorim License: CERN Copyright Contact persons: N.Mounet Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/Fastion/","text":"FASTION Short description FASTION is a 2D macro-particle simulation code for modelling the fast beam-ion instability in electron machines. Web resources Source code: https://github.com/lmether/FASTION Technical information Programming Languages used for implementation: Mainly C, some routines implemented in FORTRAN. Parallelization strategy: None Operating systems: Linux (experience on Ubuntu 12.04 or more recent, and SLC 5 or more recent) Other prerequisites: FFTW Other information Developed by: Giovanni Rumolo et al. License: GPLv3 Contact persons: Lotta Mether, Giovanni Rumolo Being actively developed and supported: Yes","title":"**FASTION**"},{"location":"codes/codes_pages/Fastion/#fastion","text":"","title":"FASTION"},{"location":"codes/codes_pages/Fastion/#short-description","text":"FASTION is a 2D macro-particle simulation code for modelling the fast beam-ion instability in electron machines.","title":"Short description"},{"location":"codes/codes_pages/Fastion/#web-resources","text":"Source code: https://github.com/lmether/FASTION","title":"Web resources"},{"location":"codes/codes_pages/Fastion/#technical-information","text":"Programming Languages used for implementation: Mainly C, some routines implemented in FORTRAN. Parallelization strategy: None Operating systems: Linux (experience on Ubuntu 12.04 or more recent, and SLC 5 or more recent) Other prerequisites: FFTW","title":"Technical information"},{"location":"codes/codes_pages/Fastion/#other-information","text":"Developed by: Giovanni Rumolo et al. License: GPLv3 Contact persons: Lotta Mether, Giovanni Rumolo Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/FootprintViewer/","text":"Online Footprint Viewer","title":"**Online Footprint Viewer**"},{"location":"codes/codes_pages/FootprintViewer/#online-footprint-viewer","text":"","title":"Online Footprint Viewer"},{"location":"codes/codes_pages/GdFiDl/","text":"GdFiDl Short description Commercial 3D electromagnetic solver for wakefield and impedance. Web resources [Link web resources available for your software. For example:] Web page: http://www.gdfidl.de/ Technical information Parallelization strategy: runs on dedicated servers Operating systems: UNIX-like systems Other prerequisites: Other information Developed by: Warner Bruns License: site-wide license purchased by CERN Contact persons: Benoit, Salvant, Alexej Grudiev (BE-RF) Being actively developed and supported: Yes","title":"**GdFiDl**"},{"location":"codes/codes_pages/GdFiDl/#gdfidl","text":"","title":"GdFiDl"},{"location":"codes/codes_pages/GdFiDl/#short-description","text":"Commercial 3D electromagnetic solver for wakefield and impedance.","title":"Short description"},{"location":"codes/codes_pages/GdFiDl/#web-resources","text":"[Link web resources available for your software. For example:] Web page: http://www.gdfidl.de/","title":"Web resources"},{"location":"codes/codes_pages/GdFiDl/#technical-information","text":"Parallelization strategy: runs on dedicated servers Operating systems: UNIX-like systems Other prerequisites:","title":"Technical information"},{"location":"codes/codes_pages/GdFiDl/#other-information","text":"Developed by: Warner Bruns License: site-wide license purchased by CERN Contact persons: Benoit, Salvant, Alexej Grudiev (BE-RF) Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/Guinea-Pig/","text":"GUINEA-PIG Short description GUINEA-PIG++ (Generator of Unwanted Interactions for Numerical Experiment Analysis - Program Interfaced to GEANT) simulates the beam-beam interaction in electron-positron linear colliders. It uses a strong-strong model of the colliding beams and includes the emission of beamstrahlung, production of incoherent and coherent pairs as well as hadronic background. Web resources Source code: https://gitlab.cern.ch/clic-software/guinea-pig Documentation The appendix of Daniel's PhD thesis: http://flash.desy.de/sites2009/site_vuvfel/content/e403/e1644/e1314/e1316/infoboxContent1932/tesla1997-08.pdf Technical information [Provide the following information] Programming Languages used for implementation: C / C++ Parallelization strategy: no parallelism Operating systems: Linux, MacOSX, Cygwin Other prerequisites: fftw Other information Developed by: Daniel Schulte et al. License: CERN Licence Contact persons: Daniel Schulte Being actively developed and supported: Yes","title":"**GUINEA-PIG**"},{"location":"codes/codes_pages/Guinea-Pig/#guinea-pig","text":"","title":"GUINEA-PIG"},{"location":"codes/codes_pages/Guinea-Pig/#short-description","text":"GUINEA-PIG++ (Generator of Unwanted Interactions for Numerical Experiment Analysis - Program Interfaced to GEANT) simulates the beam-beam interaction in electron-positron linear colliders. It uses a strong-strong model of the colliding beams and includes the emission of beamstrahlung, production of incoherent and coherent pairs as well as hadronic background.","title":"Short description"},{"location":"codes/codes_pages/Guinea-Pig/#web-resources","text":"Source code: https://gitlab.cern.ch/clic-software/guinea-pig Documentation The appendix of Daniel's PhD thesis: http://flash.desy.de/sites2009/site_vuvfel/content/e403/e1644/e1314/e1316/infoboxContent1932/tesla1997-08.pdf Technical information [Provide the following information] Programming Languages used for implementation: C / C++ Parallelization strategy: no parallelism Operating systems: Linux, MacOSX, Cygwin Other prerequisites: fftw","title":"Web resources"},{"location":"codes/codes_pages/Guinea-Pig/#other-information","text":"Developed by: Daniel Schulte et al. License: CERN Licence Contact persons: Daniel Schulte Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/HFSS/","text":"Ansys HFSS Short description ANSYS Electronics Desktop (formerly known as HFSS) solves Maxwell's equations in RF / microwave structures with the absence of a particle beam. It can solve problems of four types: Modal, Terminal, Transient, and Eigenmode. An example of a typical task for HFSS is finding electromagnetic modes and their properties in a closed RF cavity (Eigenmode solver). Another example is finding S-parameters of an RF system (S11, S21, etc) at a frequency specified by the user (Modal solver). HFSS is often compared to CST Microwave Studio. The two codes use two fundamentally different ways to solve Maxwell's equations, hence an agreement between them usually means reliability of the results. Web resources [Link web resources available for your software. For example:] Installer: available on CMF Wiki pages: http://www.ansys.com/Products/Electronics/ANSYS-HFSS Technical information [Provide the following information] Programming Languages used for implementation: commercial software Parallelization strategy: Operating systems: windows and Linux Other prerequisites: requires license (bought by CERN) Other information Developed by: License: limited number of licenses Contact persons: Sergey Arsenyev (ABP-LAT) Being actively developed and supported: Yes","title":"**Ansys HFSS**"},{"location":"codes/codes_pages/HFSS/#ansys-hfss","text":"","title":"Ansys HFSS"},{"location":"codes/codes_pages/HFSS/#short-description","text":"ANSYS Electronics Desktop (formerly known as HFSS) solves Maxwell's equations in RF / microwave structures with the absence of a particle beam. It can solve problems of four types: Modal, Terminal, Transient, and Eigenmode. An example of a typical task for HFSS is finding electromagnetic modes and their properties in a closed RF cavity (Eigenmode solver). Another example is finding S-parameters of an RF system (S11, S21, etc) at a frequency specified by the user (Modal solver). HFSS is often compared to CST Microwave Studio. The two codes use two fundamentally different ways to solve Maxwell's equations, hence an agreement between them usually means reliability of the results.","title":"Short description"},{"location":"codes/codes_pages/HFSS/#web-resources","text":"[Link web resources available for your software. For example:] Installer: available on CMF Wiki pages: http://www.ansys.com/Products/Electronics/ANSYS-HFSS","title":"Web resources"},{"location":"codes/codes_pages/HFSS/#technical-information","text":"[Provide the following information] Programming Languages used for implementation: commercial software Parallelization strategy: Operating systems: windows and Linux Other prerequisites: requires license (bought by CERN)","title":"Technical information"},{"location":"codes/codes_pages/HFSS/#other-information","text":"Developed by: License: limited number of licenses Contact persons: Sergey Arsenyev (ABP-LAT) Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/IBSimu/","text":"IBSimu Short description Ion Beam Simulator or IBSimu is an ion optical computer simulation package for ion optics, plasma extraction and space charge dominated ion beam transport using Vlasov iteration. Web resources Source code: http://ibsimu.sourceforge.net/download.html Wiki pages: http://ibsimu.sourceforge.net/ Technical information Programming Languages used for implementation: C++ Parallelization strategy: none Operating systems: Linux and Windows Other prerequisites: Third party libraries, please see http://ibsimu.sourceforge.net/installation.html Other information Developed by: Taneli Kalvas (University of Jyv\u00e4skyl\u00e4, Finland) License: GNU General Public Licence Contact persons: Jacques Lettry Being actively developed and supported: Yes","title":"**IBSimu**"},{"location":"codes/codes_pages/IBSimu/#ibsimu","text":"","title":"IBSimu"},{"location":"codes/codes_pages/IBSimu/#short-description","text":"Ion Beam Simulator or IBSimu is an ion optical computer simulation package for ion optics, plasma extraction and space charge dominated ion beam transport using Vlasov iteration.","title":"Short description"},{"location":"codes/codes_pages/IBSimu/#web-resources","text":"Source code: http://ibsimu.sourceforge.net/download.html Wiki pages: http://ibsimu.sourceforge.net/","title":"Web resources"},{"location":"codes/codes_pages/IBSimu/#technical-information","text":"Programming Languages used for implementation: C++ Parallelization strategy: none Operating systems: Linux and Windows Other prerequisites: Third party libraries, please see http://ibsimu.sourceforge.net/installation.html","title":"Technical information"},{"location":"codes/codes_pages/IBSimu/#other-information","text":"Developed by: Taneli Kalvas (University of Jyv\u00e4skyl\u00e4, Finland) License: GNU General Public Licence Contact persons: Jacques Lettry Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/ImpedanceWake2D/","text":"ImpedanceWake2D","title":"**ImpedanceWake2D**"},{"location":"codes/codes_pages/ImpedanceWake2D/#impedancewake2d","text":"","title":"ImpedanceWake2D"},{"location":"codes/codes_pages/LHCOnlineModel/","text":"LHCOnlineModel Short description The beam based model, aka online model, extends the (ideal) LHC accelerator model by including measured parameters of the machine. Orbit Optics (beta beating) Applied corrections (tune, orbit corrector magnets) Powering errors Magnetic field deviations (new implementation of WISE as Java app) Alignment errors Such model is more accurate and it provides higher predictive power. It allows calculating, for example, Feed-down effects due to imperfect orbit Modification of global parameters due to beta function beating or alignment errors The project aims at providing tools that allow Easy access to more accurate and up-to-date beam parameters. The principal example is the aperture meter application that displays available aperture along the machine taking into account the measured orbit and beta beating. Simulating more accurately the beam behavior upon a knob or parameter change. Easy comparison between the model and the available measurements. Facilitating data analysis throughout all different machine configurations hopefully lets understand the sources of the discrepancies and leads to even more accurate machine model. Web resources Web page: http://lhcmodel.web.cern.ch/lhcmodel/ Source codes: https://svnweb.cern.ch/cern/wsvn/lhcmodel https://svnweb.cern.ch/cern/wsvn/acc-co/trunk/accsoft/om/ https://svnweb.cern.ch/cern/wsvn/acc-co/trunk/lhc/lhc-model-extractor/ https://svnweb.cern.ch/cern/wsvn/acc-co/trunk/lhc/lhc-model-wise-errorestimators/ https://svnweb.cern.ch/cern/wsvn/acc-co/trunk/lhc/lhc-model-wise-ui/ https://svnweb.cern.ch/cern/wsvn/acc-co/trunk/lhc/lhc-model-fidel-extractor/ Wiki pages: https://wikis.cern.ch/display/OnlineModel/Home Technical information [Provide the following information] Programming Languages used for implementation: Java Python Parallelization strategy: None Operating systems: Linux, inside cern only, and in some cases on technical network only Other prerequisites: Other information Developed by: [Piotr Skowronski] License: Default CERN Contact persons: Piotr Skowronski, Tobias Persson Being actively developed and supported: Yes","title":"**LHCOnlineModel**"},{"location":"codes/codes_pages/LHCOnlineModel/#lhconlinemodel","text":"","title":"LHCOnlineModel"},{"location":"codes/codes_pages/LHCOnlineModel/#short-description","text":"The beam based model, aka online model, extends the (ideal) LHC accelerator model by including measured parameters of the machine. Orbit Optics (beta beating) Applied corrections (tune, orbit corrector magnets) Powering errors Magnetic field deviations (new implementation of WISE as Java app) Alignment errors Such model is more accurate and it provides higher predictive power. It allows calculating, for example, Feed-down effects due to imperfect orbit Modification of global parameters due to beta function beating or alignment errors The project aims at providing tools that allow Easy access to more accurate and up-to-date beam parameters. The principal example is the aperture meter application that displays available aperture along the machine taking into account the measured orbit and beta beating. Simulating more accurately the beam behavior upon a knob or parameter change. Easy comparison between the model and the available measurements. Facilitating data analysis throughout all different machine configurations hopefully lets understand the sources of the discrepancies and leads to even more accurate machine model.","title":"Short description"},{"location":"codes/codes_pages/LHCOnlineModel/#web-resources","text":"Web page: http://lhcmodel.web.cern.ch/lhcmodel/ Source codes: https://svnweb.cern.ch/cern/wsvn/lhcmodel https://svnweb.cern.ch/cern/wsvn/acc-co/trunk/accsoft/om/ https://svnweb.cern.ch/cern/wsvn/acc-co/trunk/lhc/lhc-model-extractor/ https://svnweb.cern.ch/cern/wsvn/acc-co/trunk/lhc/lhc-model-wise-errorestimators/ https://svnweb.cern.ch/cern/wsvn/acc-co/trunk/lhc/lhc-model-wise-ui/ https://svnweb.cern.ch/cern/wsvn/acc-co/trunk/lhc/lhc-model-fidel-extractor/ Wiki pages: https://wikis.cern.ch/display/OnlineModel/Home","title":"Web resources"},{"location":"codes/codes_pages/LHCOnlineModel/#technical-information","text":"[Provide the following information] Programming Languages used for implementation: Java Python Parallelization strategy: None Operating systems: Linux, inside cern only, and in some cases on technical network only Other prerequisites:","title":"Technical information"},{"location":"codes/codes_pages/LHCOnlineModel/#other-information","text":"Developed by: [Piotr Skowronski] License: Default CERN Contact persons: Piotr Skowronski, Tobias Persson Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/MOSES/","text":"MOSES Short description MOSES (MOde-coupling Single bunch instabilities in an Electron Storage ring) is a program, which computes complex coherent betatron tune shifts as a function of the bunch current for a Gaussian beam interacting with a resonator impedance. Web resources Source code and home page: http://abci.kek.jp/moses.htm Technical information Operating systems: Windows (works on 7) and Linux (not tested) Other prerequisites: e.g. libraries needed to install/use your software e.g. libraries needed to install/use your software Special hardware needs: e.g. GPU, resources for multithreading/multiprocessing. Special hardware needs: e.g. GPU, resources for multithreading/multiprocessing. Other information Developed by: [Name of developers] License: [Licence policy] Contact persons: [Contact persons for the code] Being actively developed and supported: [Yes/No]","title":"**MOSES**"},{"location":"codes/codes_pages/MOSES/#moses","text":"","title":"MOSES"},{"location":"codes/codes_pages/MOSES/#short-description","text":"MOSES (MOde-coupling Single bunch instabilities in an Electron Storage ring) is a program, which computes complex coherent betatron tune shifts as a function of the bunch current for a Gaussian beam interacting with a resonator impedance.","title":"Short description"},{"location":"codes/codes_pages/MOSES/#web-resources","text":"Source code and home page: http://abci.kek.jp/moses.htm","title":"Web resources"},{"location":"codes/codes_pages/MOSES/#technical-information","text":"Operating systems: Windows (works on 7) and Linux (not tested) Other prerequisites: e.g. libraries needed to install/use your software e.g. libraries needed to install/use your software Special hardware needs: e.g. GPU, resources for multithreading/multiprocessing. Special hardware needs: e.g. GPU, resources for multithreading/multiprocessing.","title":"Technical information"},{"location":"codes/codes_pages/MOSES/#other-information","text":"Developed by: [Name of developers] License: [Licence policy] Contact persons: [Contact persons for the code] Being actively developed and supported: [Yes/No]","title":"Other information"},{"location":"codes/codes_pages/MadX/","text":"MAD-X Short description MAD-X is an application used world-wide with a long history going back to the 80's in the field of high energy beam physics (i.e. MAD8, MAD9, MADX). It is an all-in-one application with its own scripting language used to design, simulate and optimize particle accelerators: lattice description, machine survey, single particles 6D tracking, optics modeling, beam simulation & analysis, machine optimisation, errors handling, orbit correction, aperture margin and emittance equilibrium. Web resources The MAD-X website provides access to information, documentation (PDF), releases (binaries), source code (tarball), e-groups, versioning system (SVN), issues tracking system (Trac), and more... The user manual can be downloaded here . Technical information Programming Languages used for implementation: Fortran 77, Fortran 90, C, C++ for a total of about 180000 lines of codes. MAD-X scripting language is garbage collected (based on GC Boehm). Build system entirely done with make. Tested every night on server equiped with VMs for supported OS and architectures. Parallelization strategy: Particle tracking (track command) is parallelized with OpenMP if compiled with the proper flags. This is not the default as it does not bring much speed gain (old-style Fortran 77). Compilers vectorize strings, arrays, vectors and matrices computations, and perform loop unrolling plus many other common optimizations done by modern compilers on modern CPU/FPU. Operating systems: Supported on MAC OSX (10.8 or above), Windows (7 or above), Linux (static binary). 32 bit and 64 bit architectures are avalaible. Other prerequisites: No libraries. Scatter plots use gnuplot. Other information Delivery: 2-3 releases per year. Developed by: Main developpers were H. Grote (MAD8, MADX) and C. Iselin (MAD8, MAD9) in the 90's. Many people contributed to the project since then, see the contributors section on the website. License: Open source software under CERN Copyrights. Contact persons: mad at cern dot ch (see the website for the persons members of the MAD team). Being actively developed and supported: Yes.","title":"**MAD-X**"},{"location":"codes/codes_pages/MadX/#mad-x","text":"","title":"MAD-X"},{"location":"codes/codes_pages/MadX/#short-description","text":"MAD-X is an application used world-wide with a long history going back to the 80's in the field of high energy beam physics (i.e. MAD8, MAD9, MADX). It is an all-in-one application with its own scripting language used to design, simulate and optimize particle accelerators: lattice description, machine survey, single particles 6D tracking, optics modeling, beam simulation & analysis, machine optimisation, errors handling, orbit correction, aperture margin and emittance equilibrium.","title":"Short description"},{"location":"codes/codes_pages/MadX/#web-resources","text":"The MAD-X website provides access to information, documentation (PDF), releases (binaries), source code (tarball), e-groups, versioning system (SVN), issues tracking system (Trac), and more... The user manual can be downloaded here .","title":"Web resources"},{"location":"codes/codes_pages/MadX/#technical-information","text":"Programming Languages used for implementation: Fortran 77, Fortran 90, C, C++ for a total of about 180000 lines of codes. MAD-X scripting language is garbage collected (based on GC Boehm). Build system entirely done with make. Tested every night on server equiped with VMs for supported OS and architectures. Parallelization strategy: Particle tracking (track command) is parallelized with OpenMP if compiled with the proper flags. This is not the default as it does not bring much speed gain (old-style Fortran 77). Compilers vectorize strings, arrays, vectors and matrices computations, and perform loop unrolling plus many other common optimizations done by modern compilers on modern CPU/FPU. Operating systems: Supported on MAC OSX (10.8 or above), Windows (7 or above), Linux (static binary). 32 bit and 64 bit architectures are avalaible. Other prerequisites: No libraries. Scatter plots use gnuplot.","title":"Technical information"},{"location":"codes/codes_pages/MadX/#other-information","text":"Delivery: 2-3 releases per year. Developed by: Main developpers were H. Grote (MAD8, MADX) and C. Iselin (MAD8, MAD9) in the 90's. Many people contributed to the project since then, see the contributors section on the website. License: Open source software under CERN Copyrights. Contact persons: mad at cern dot ch (see the website for the persons members of the MAD team). Being actively developed and supported: Yes.","title":"Other information"},{"location":"codes/codes_pages/MapClass/","text":"MapClass Short description MapClass and its successor MapClass2 are two codes written in Python and C++ conceived to optimize the non-linear aberrations in beam lines. First and second order momenta of the beam at the end of the given beam line are used as figure of merits. MapClass takes the transfer map from PTC while MapClass2 is equipped with a simple integrator to produce the transfer map up to the desired order. Web resources Source code: https://github.com/pylhc/MapClass2 MapClass documentation: https://cds.cern.ch/record/944769/files/ab-note-2006-017.pdf MapClass2 documentation: https://cds.cern.ch/record/1491228 GPU parallelization: http://www.sciencedirect.com/science/article/pii/S1877050916306573 Technical information Programming Languages used for implementation: Python C++ Parallelization strategy: GPU in C++ Operating systems: Linux Other prerequisites: Boost libraries for communication between C++ and python GPU optionally Other information Developed by: Rogelio Tomas , Eduardo Marin Lacoma, David Martinez, Alice Rosam, Hector Garcia Morales and Andrea Popescu. License: Open source Contact persons: Rogelio Tomas Being actively developed and supported: Yes","title":"**MapClass**"},{"location":"codes/codes_pages/MapClass/#mapclass","text":"","title":"MapClass"},{"location":"codes/codes_pages/MapClass/#short-description","text":"MapClass and its successor MapClass2 are two codes written in Python and C++ conceived to optimize the non-linear aberrations in beam lines. First and second order momenta of the beam at the end of the given beam line are used as figure of merits. MapClass takes the transfer map from PTC while MapClass2 is equipped with a simple integrator to produce the transfer map up to the desired order.","title":"Short description"},{"location":"codes/codes_pages/MapClass/#web-resources","text":"Source code: https://github.com/pylhc/MapClass2 MapClass documentation: https://cds.cern.ch/record/944769/files/ab-note-2006-017.pdf MapClass2 documentation: https://cds.cern.ch/record/1491228 GPU parallelization: http://www.sciencedirect.com/science/article/pii/S1877050916306573","title":"Web resources"},{"location":"codes/codes_pages/MapClass/#technical-information","text":"Programming Languages used for implementation: Python C++ Parallelization strategy: GPU in C++ Operating systems: Linux Other prerequisites: Boost libraries for communication between C++ and python GPU optionally","title":"Technical information"},{"location":"codes/codes_pages/MapClass/#other-information","text":"Developed by: Rogelio Tomas , Eduardo Marin Lacoma, David Martinez, Alice Rosam, Hector Garcia Morales and Andrea Popescu. License: Open source Contact persons: Rogelio Tomas Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/Ninja/","text":"NINJA Short description NINJA is a 2.5 D Implicit Particle-In-Cell Monte Carlo Collision code for the simulation of inductively coupled plasmas. The model includes a self-consistent description of the coupling between the electromagnetic field generated by the radio-frequency antenna and the plasma response, composed of the particles\u2019 motion and the collisions between them. Web resources [Link web resources available for your software. For example:] Source code: [link1] Wiki pages: [link2] Publication in preparation Technical information Programming Languages used for implementation: Fortran 90 Parallelization strategy: MPI, domain decomposition Operating systems: Linux Other prerequisites: Third party libraries: NITSOL, BLAS, LAPACK, SPARSKIT Other information Developed by: Stefano Mattei, in collaboration with KEIO University (Japan) License: [Licence policy] Contact persons: Stefano Mattei, Jacques Lettry Being actively developed and supported: Yes","title":"**NINJA**"},{"location":"codes/codes_pages/Ninja/#ninja","text":"","title":"NINJA"},{"location":"codes/codes_pages/Ninja/#short-description","text":"NINJA is a 2.5 D Implicit Particle-In-Cell Monte Carlo Collision code for the simulation of inductively coupled plasmas. The model includes a self-consistent description of the coupling between the electromagnetic field generated by the radio-frequency antenna and the plasma response, composed of the particles\u2019 motion and the collisions between them.","title":"Short description"},{"location":"codes/codes_pages/Ninja/#web-resources","text":"[Link web resources available for your software. For example:] Source code: [link1] Wiki pages: [link2] Publication in preparation","title":"Web resources"},{"location":"codes/codes_pages/Ninja/#technical-information","text":"Programming Languages used for implementation: Fortran 90 Parallelization strategy: MPI, domain decomposition Operating systems: Linux Other prerequisites: Third party libraries: NITSOL, BLAS, LAPACK, SPARSKIT","title":"Technical information"},{"location":"codes/codes_pages/Ninja/#other-information","text":"Developed by: Stefano Mattei, in collaboration with KEIO University (Japan) License: [Licence policy] Contact persons: Stefano Mattei, Jacques Lettry Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/ONIX/","text":"ONIX Short description ONIX (Orsay Negative Ion eXtraction) is a 3D particle-in-cell Monte Carlo Collision electrostatic code to simulate the particle transport in electronegative plasmas, in the vicinity of the extraction electrode of an ion source. Web resources References: http://iopscience.iop.org/article/10.1088/1367-2630/18/8/085011 http://scitation.aip.org/content/aip/journal/jap/111/11/10.1063/1.4727969 Technical information Programming Languages used for implementation: Fortran Parallelization strategy: MPI, domain decomposition Operating systems: Linux Other information Developed by: Serhiy Mochalskyy (IPP Garching, Germany) License: Contact persons: Jacques Lettry Being actively developed and supported: Yes","title":"**ONIX**"},{"location":"codes/codes_pages/ONIX/#onix","text":"","title":"ONIX"},{"location":"codes/codes_pages/ONIX/#short-description","text":"ONIX (Orsay Negative Ion eXtraction) is a 3D particle-in-cell Monte Carlo Collision electrostatic code to simulate the particle transport in electronegative plasmas, in the vicinity of the extraction electrode of an ion source.","title":"Short description"},{"location":"codes/codes_pages/ONIX/#web-resources","text":"References: http://iopscience.iop.org/article/10.1088/1367-2630/18/8/085011 http://scitation.aip.org/content/aip/journal/jap/111/11/10.1063/1.4727969","title":"Web resources"},{"location":"codes/codes_pages/ONIX/#technical-information","text":"Programming Languages used for implementation: Fortran Parallelization strategy: MPI, domain decomposition Operating systems: Linux","title":"Technical information"},{"location":"codes/codes_pages/ONIX/#other-information","text":"Developed by: Serhiy Mochalskyy (IPP Garching, Germany) License: Contact persons: Jacques Lettry Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/PATH/","text":"SoftwareName Short description [Insert a short description of your software.] Web resources [Link web resources available for your software. For example:] Source code: [link1] Wiki pages: [link2] [If you want you can insert the documentation directly in this wiki, feel free to create as many pages as you need ;-)] Technical information [Provide the following information] Programming Languages used for implementation: Language 1 Language 1 Language 2 Language 2 Parallelization strategy: e.g. MPI/multithreading.. e.g. MPI/multithreading.. Operating systems: Which operating systems have been tested and are supported Which operating systems have been tested and are supported Other prerequisites: e.g. libraries needed to install/use your software e.g. libraries needed to install/use your software Special hardware needs: e.g. GPU, resources for multithreading/multiprocessing. Special hardware needs: e.g. GPU, resources for multithreading/multiprocessing. Other information Developed by: [Name of developers] License: [Licence policy] Contact persons: [Contact persons for the code] Being actively developed and supported: [Yes/No]","title":"**SoftwareName**"},{"location":"codes/codes_pages/PATH/#softwarename","text":"","title":"SoftwareName"},{"location":"codes/codes_pages/PATH/#short-description","text":"[Insert a short description of your software.]","title":"Short description"},{"location":"codes/codes_pages/PATH/#web-resources","text":"[Link web resources available for your software. For example:] Source code: [link1] Wiki pages: [link2] [If you want you can insert the documentation directly in this wiki, feel free to create as many pages as you need ;-)]","title":"Web resources"},{"location":"codes/codes_pages/PATH/#technical-information","text":"[Provide the following information] Programming Languages used for implementation: Language 1 Language 1 Language 2 Language 2 Parallelization strategy: e.g. MPI/multithreading.. e.g. MPI/multithreading.. Operating systems: Which operating systems have been tested and are supported Which operating systems have been tested and are supported Other prerequisites: e.g. libraries needed to install/use your software e.g. libraries needed to install/use your software Special hardware needs: e.g. GPU, resources for multithreading/multiprocessing. Special hardware needs: e.g. GPU, resources for multithreading/multiprocessing.","title":"Technical information"},{"location":"codes/codes_pages/PATH/#other-information","text":"Developed by: [Name of developers] License: [Licence policy] Contact persons: [Contact persons for the code] Being actively developed and supported: [Yes/No]","title":"Other information"},{"location":"codes/codes_pages/PHOTON/","text":"SoftwareName PHOTON Short description The PHOTON program performs a Monte-Carlo simulation of photon flux on the vacuum-chamber wall around a storage ring or transport line. The simulation includes synchrotron radiation in dipoles and quadrupoles, the effects of closed orbit distortions, beam size and divergence, etc. The photon energies are determined following the algorithm proposed by Roy. The program requires 4 input files: (1) a look-up table for the SR spectrum, (2) a MAD twiss tape file which describes the beam optics - this can of course include optics imperfections - , (3) a file with magnet apertures, (4) a parameter input file. The latter contains, e.g., the beam emittances, energy, particle type, and various simulation flags. Web resources Home page: http://wwwslap.cern.ch/collective/electron-cloud/Programs/Photon/photon.html Source code: https://wwwslap.cern.ch/collective/electron-cloud/Programs/Photon/photon.tar References: https://care-hhh.web.cern.ch/care-hhh/Simulation-Codes/References%20pages/Ref_PHOTON.htm Technical information Programming Languages used for implementation: FORTRAN77 Parallelization strategy: none Operating systems: linux Other prerequisites: needs a MAD output file Other information Developed by: Frank Zimmermann frank.zimmermann@cernNOSPAMPLEASE.ch License: CERN Contact persons: Frank Zimmermann Being actively developed and supported: No","title":"**SoftwareName**"},{"location":"codes/codes_pages/PHOTON/#softwarename","text":"PHOTON","title":"SoftwareName"},{"location":"codes/codes_pages/PHOTON/#short-description","text":"The PHOTON program performs a Monte-Carlo simulation of photon flux on the vacuum-chamber wall around a storage ring or transport line. The simulation includes synchrotron radiation in dipoles and quadrupoles, the effects of closed orbit distortions, beam size and divergence, etc. The photon energies are determined following the algorithm proposed by Roy. The program requires 4 input files: (1) a look-up table for the SR spectrum, (2) a MAD twiss tape file which describes the beam optics - this can of course include optics imperfections - , (3) a file with magnet apertures, (4) a parameter input file. The latter contains, e.g., the beam emittances, energy, particle type, and various simulation flags.","title":"Short description"},{"location":"codes/codes_pages/PHOTON/#web-resources","text":"Home page: http://wwwslap.cern.ch/collective/electron-cloud/Programs/Photon/photon.html Source code: https://wwwslap.cern.ch/collective/electron-cloud/Programs/Photon/photon.tar References: https://care-hhh.web.cern.ch/care-hhh/Simulation-Codes/References%20pages/Ref_PHOTON.htm","title":"Web resources"},{"location":"codes/codes_pages/PHOTON/#technical-information","text":"Programming Languages used for implementation: FORTRAN77 Parallelization strategy: none Operating systems: linux Other prerequisites: needs a MAD output file","title":"Technical information"},{"location":"codes/codes_pages/PHOTON/#other-information","text":"Developed by: Frank Zimmermann frank.zimmermann@cernNOSPAMPLEASE.ch License: CERN Contact persons: Frank Zimmermann Being actively developed and supported: No","title":"Other information"},{"location":"codes/codes_pages/PageStore/","text":"PageStore Short description Library to store massive timeseries data on commodity file system. Web resources Source code: PageStore GitHub Pages Technical information Programming Languages used for implementation: Python Operating systems: Multiple OS Other prerequisites: numpy Other information Developed by: Riccardo De Maria License: No Explicit License Contact persons: Riccardo De Maria Being actively developed and supported: Yes","title":"**PageStore**"},{"location":"codes/codes_pages/PageStore/#pagestore","text":"","title":"PageStore"},{"location":"codes/codes_pages/PageStore/#short-description","text":"Library to store massive timeseries data on commodity file system.","title":"Short description"},{"location":"codes/codes_pages/PageStore/#web-resources","text":"Source code: PageStore GitHub Pages","title":"Web resources"},{"location":"codes/codes_pages/PageStore/#technical-information","text":"Programming Languages used for implementation: Python Operating systems: Multiple OS Other prerequisites: numpy","title":"Technical information"},{"location":"codes/codes_pages/PageStore/#other-information","text":"Developed by: Riccardo De Maria License: No Explicit License Contact persons: Riccardo De Maria Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/Placet/","text":"PLACET Short description PLACET (Program for Linear Accelerator Correction and Efficiency Tests) is a code that simulates the dynamics of a beam in the main accelerating or decelerating part of a linac in the presence of wakefields. It allows to investigate single- and multi-bunch effects, and to simulate normal cavities with relatively low group velocities as well as the special power extraction and transfer structures specific to CLIC. A number of correction schemes are implemented, to test what emittance growth must be expected for given prealignment errors. Web resources Source code: https://gitlab.cern.ch/clic-software/placet Project page: https://clicsw.web.cern.ch/clicsw Technical information Programming Languages used for implementation: C / C++ Parallelization strategy: openMP, optionally MPI Operating systems: Linux, MacOS, Cygwin Other prerequisites: fftw, gsl, tcl/tk user interface: Tcl/Tk, Octave, Python Run PLACET on HTCondor Guide to submit PLACET jobs to HTCondor available here Other information Developed by: Daniel Schulte, Andrea Latina, et al. License: CERN Licence Contact persons: Andrea Latina Being actively developed and supported: [Yes]","title":"**PLACET**"},{"location":"codes/codes_pages/Placet/#placet","text":"","title":"PLACET"},{"location":"codes/codes_pages/Placet/#short-description","text":"PLACET (Program for Linear Accelerator Correction and Efficiency Tests) is a code that simulates the dynamics of a beam in the main accelerating or decelerating part of a linac in the presence of wakefields. It allows to investigate single- and multi-bunch effects, and to simulate normal cavities with relatively low group velocities as well as the special power extraction and transfer structures specific to CLIC. A number of correction schemes are implemented, to test what emittance growth must be expected for given prealignment errors.","title":"Short description"},{"location":"codes/codes_pages/Placet/#web-resources","text":"Source code: https://gitlab.cern.ch/clic-software/placet Project page: https://clicsw.web.cern.ch/clicsw","title":"Web resources"},{"location":"codes/codes_pages/Placet/#technical-information","text":"Programming Languages used for implementation: C / C++ Parallelization strategy: openMP, optionally MPI Operating systems: Linux, MacOS, Cygwin Other prerequisites: fftw, gsl, tcl/tk user interface: Tcl/Tk, Octave, Python Run PLACET on HTCondor Guide to submit PLACET jobs to HTCondor available here","title":"Technical information"},{"location":"codes/codes_pages/Placet/#other-information","text":"Developed by: Daniel Schulte, Andrea Latina, et al. License: CERN Licence Contact persons: Andrea Latina Being actively developed and supported: [Yes]","title":"Other information"},{"location":"codes/codes_pages/PyECLOUD/","text":"PyECLOUD Short description PyECLOUD is a 2D macro-particle code for the simulation of electron cloud effects in particle accelerators. It can be used for two purposes: in stand-alone mode for the simulation of the e-cloud buildup at a certain section of an accelerator (it this case the beam is rigid and feels no effect from from the cloud); in combination with the PyHEADTAIL code for the simulation of the e-cloud effects on the beam dynamics. Web resources Git repository: https://github.com/PyCOMPLETE/PyECLOUD Getting started guides: Installation , Usage Examples: Available in Getting-started guides. Documentation: : https://github.com/PyCOMPLETE/PyECLOUD/wiki Technical information Programming Languages used for implementation: Mainly Python. Computationally intensive routines are implemented in FORTRAN (and linked via f2py) or C (and linked via cython). Parallelization strategy: PyECLOUD-PyHEADTAIL simulations can be paralleled using the PyPARIS layer. Operating systems: tested exclusively on Linux. Other prerequisites: Python 3.6+ Libraries: numpy, scipy, cython, h5py Other information Developed by: Giovanni Iadarola License: CERN Copyright Contact persons: Giovanni Iadarola","title":"**PyECLOUD**"},{"location":"codes/codes_pages/PyECLOUD/#pyecloud","text":"","title":"PyECLOUD"},{"location":"codes/codes_pages/PyECLOUD/#short-description","text":"PyECLOUD is a 2D macro-particle code for the simulation of electron cloud effects in particle accelerators. It can be used for two purposes: in stand-alone mode for the simulation of the e-cloud buildup at a certain section of an accelerator (it this case the beam is rigid and feels no effect from from the cloud); in combination with the PyHEADTAIL code for the simulation of the e-cloud effects on the beam dynamics.","title":"Short description"},{"location":"codes/codes_pages/PyECLOUD/#web-resources","text":"Git repository: https://github.com/PyCOMPLETE/PyECLOUD Getting started guides: Installation , Usage Examples: Available in Getting-started guides. Documentation: : https://github.com/PyCOMPLETE/PyECLOUD/wiki","title":"Web resources"},{"location":"codes/codes_pages/PyECLOUD/#technical-information","text":"Programming Languages used for implementation: Mainly Python. Computationally intensive routines are implemented in FORTRAN (and linked via f2py) or C (and linked via cython). Parallelization strategy: PyECLOUD-PyHEADTAIL simulations can be paralleled using the PyPARIS layer. Operating systems: tested exclusively on Linux. Other prerequisites: Python 3.6+ Libraries: numpy, scipy, cython, h5py","title":"Technical information"},{"location":"codes/codes_pages/PyECLOUD/#other-information","text":"Developed by: Giovanni Iadarola License: CERN Copyright Contact persons: Giovanni Iadarola","title":"Other information"},{"location":"codes/codes_pages/PyHEADTAIL/","text":"PyHEADTAIL Short description Python macroparticle simulation code library for modeling collective effects beam dynamics in circular accelerators. Web resources Source code: https://github.com/PyCOMPLETE/PyHEADTAIL Wiki pages: https://github.com/PyCOMPLETE/PyHEADTAIL/wiki Technical information Programming Languages used for implementation: Python, C, C++, Cython, CUDA Parallelization strategy: MPI, OpenMP, GPGPU Operating systems: Linux (experience on SLC5+, Ubuntu 12+, OpenSUSE 12+) Other prerequisites: Python 2.7+ (minor compatibility issues with Python 3 expected) Third party libraries: Cython, h5py, NumPy, SciPy Special hardware needs: NVidia GPU for CUDA parts Other information Developed by: Kevin Li et al. License: CERN Copyright Contact persons: Kevin Li Being actively developed and supported: Yes","title":"**PyHEADTAIL**"},{"location":"codes/codes_pages/PyHEADTAIL/#pyheadtail","text":"","title":"PyHEADTAIL"},{"location":"codes/codes_pages/PyHEADTAIL/#short-description","text":"Python macroparticle simulation code library for modeling collective effects beam dynamics in circular accelerators.","title":"Short description"},{"location":"codes/codes_pages/PyHEADTAIL/#web-resources","text":"Source code: https://github.com/PyCOMPLETE/PyHEADTAIL Wiki pages: https://github.com/PyCOMPLETE/PyHEADTAIL/wiki","title":"Web resources"},{"location":"codes/codes_pages/PyHEADTAIL/#technical-information","text":"Programming Languages used for implementation: Python, C, C++, Cython, CUDA Parallelization strategy: MPI, OpenMP, GPGPU Operating systems: Linux (experience on SLC5+, Ubuntu 12+, OpenSUSE 12+) Other prerequisites: Python 2.7+ (minor compatibility issues with Python 3 expected) Third party libraries: Cython, h5py, NumPy, SciPy Special hardware needs: NVidia GPU for CUDA parts","title":"Technical information"},{"location":"codes/codes_pages/PyHEADTAIL/#other-information","text":"Developed by: Kevin Li et al. License: CERN Copyright Contact persons: Kevin Li Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/PyORBIT/","text":"PyORBIT Short description PyORBIT is a Python/C++ implementation of the ORBIT (Objective Ring Beam Injection and Tracking) code. PyORBIT software is an open environment for simulations of diverse physical processes related to particle accelerators. The original ORBIT has the Super Code driver shell which is replaced by Python in PyORBIT. At this moment only few capabilities of the original ORBIT are implemented. Particle tracking can be done in two ways, either with the built-in Teapot tracker module or using a special version of the PTC library. Web resources Source code: https://sourceforge.net/projects/py-orbit/ Wiki pages: https://sourceforge.net/p/py-orbit/wiki/?source=navbar Technical information Programming Languages used for implementation: Top layer in Python All computationally intensive routines are implemented in C++ Parallelization strategy: Based on MPI, communication between cores at each space charge interaction node. Operating systems: Tested on Linux and MAC OSX (Windows), at CERN used on lxplus Other prerequisites: Python 2.6 PTC Other information Developed by: A. Shishlo, J. Holmes, S. Cousineau, SNS Oakridge, contributions from CERN License: MIT License Contact person at CERN: Hannes Bartosik Being actively developed and supported: Yes","title":"**PyORBIT**"},{"location":"codes/codes_pages/PyORBIT/#pyorbit","text":"","title":"PyORBIT"},{"location":"codes/codes_pages/PyORBIT/#short-description","text":"PyORBIT is a Python/C++ implementation of the ORBIT (Objective Ring Beam Injection and Tracking) code. PyORBIT software is an open environment for simulations of diverse physical processes related to particle accelerators. The original ORBIT has the Super Code driver shell which is replaced by Python in PyORBIT. At this moment only few capabilities of the original ORBIT are implemented. Particle tracking can be done in two ways, either with the built-in Teapot tracker module or using a special version of the PTC library.","title":"Short description"},{"location":"codes/codes_pages/PyORBIT/#web-resources","text":"Source code: https://sourceforge.net/projects/py-orbit/ Wiki pages: https://sourceforge.net/p/py-orbit/wiki/?source=navbar","title":"Web resources"},{"location":"codes/codes_pages/PyORBIT/#technical-information","text":"Programming Languages used for implementation: Top layer in Python All computationally intensive routines are implemented in C++ Parallelization strategy: Based on MPI, communication between cores at each space charge interaction node. Operating systems: Tested on Linux and MAC OSX (Windows), at CERN used on lxplus Other prerequisites: Python 2.6 PTC","title":"Technical information"},{"location":"codes/codes_pages/PyORBIT/#other-information","text":"Developed by: A. Shishlo, J. Holmes, S. Cousineau, SNS Oakridge, contributions from CERN License: MIT License Contact person at CERN: Hannes Bartosik Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/PyOptics/","text":"PyOptics Short description Python library to read common optics data files (TFS tables, madx input, SDDS binary files, Yasp) and perform optics analysis (plots, queries, tunes, harmonic driving terms, footprint). Web resources PyOptics GitHub pages Technical information Programming Languages used for implementation: Python Operating systems: Multiple OS Other prerequisites: numpy, scipy, matplotlib Other information Developed by: Riccardo De Maria License: No Explicit License at the moment Contact persons: Riccardo De Maria Being actively developed and supported: Yes","title":"**PyOptics**"},{"location":"codes/codes_pages/PyOptics/#pyoptics","text":"","title":"PyOptics"},{"location":"codes/codes_pages/PyOptics/#short-description","text":"Python library to read common optics data files (TFS tables, madx input, SDDS binary files, Yasp) and perform optics analysis (plots, queries, tunes, harmonic driving terms, footprint).","title":"Short description"},{"location":"codes/codes_pages/PyOptics/#web-resources","text":"PyOptics GitHub pages","title":"Web resources"},{"location":"codes/codes_pages/PyOptics/#technical-information","text":"Programming Languages used for implementation: Python Operating systems: Multiple OS Other prerequisites: numpy, scipy, matplotlib","title":"Technical information"},{"location":"codes/codes_pages/PyOptics/#other-information","text":"Developed by: Riccardo De Maria License: No Explicit License at the moment Contact persons: Riccardo De Maria Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/PyPIC/","text":"PyPIC Short description PyPIC is a python library featuring different 2D Particle in Cell Solvers. It includes advanced features like the accurate modelling of curved boundary using the Shortley-Weller approximation, and improved accuracy based on nested grids. PyPIC it is used to simulate electron cloud and space charge effects within PyECLOUD and PyHEADTAIL . Web resources Source code: https://github.com/PyCOMPLETE/PyPIC Additional information: https://github.com/PyCOMPLETE/PyPIC/wiki Technical information Programming Languages used for implementation: Mainly Python Computationally intensive routines are implemented in FORTRAN (and linked via f2py) or C/C++ (and linked via cython). Parallelization strategy: None Operating systems: Tested exclusivey on Linux (experience on Ubuntu 12.04 or more recent, and SLC 5 or more recent) Other prerequisites: Python 2.7+ (never tested on Python 3) Libraries: numpy, cython, f2py. Optionally: pyfftw, KLU interface). Other information Developed by: Giovanni Iadarola , Kevin Li, Eleonora Belli, Lotta Mether License: CERN Copyright Contact persons: Giovanni Iadarola Being actively developed and supported: Yes","title":"**PyPIC**"},{"location":"codes/codes_pages/PyPIC/#pypic","text":"","title":"PyPIC"},{"location":"codes/codes_pages/PyPIC/#short-description","text":"PyPIC is a python library featuring different 2D Particle in Cell Solvers. It includes advanced features like the accurate modelling of curved boundary using the Shortley-Weller approximation, and improved accuracy based on nested grids. PyPIC it is used to simulate electron cloud and space charge effects within PyECLOUD and PyHEADTAIL .","title":"Short description"},{"location":"codes/codes_pages/PyPIC/#web-resources","text":"Source code: https://github.com/PyCOMPLETE/PyPIC Additional information: https://github.com/PyCOMPLETE/PyPIC/wiki","title":"Web resources"},{"location":"codes/codes_pages/PyPIC/#technical-information","text":"Programming Languages used for implementation: Mainly Python Computationally intensive routines are implemented in FORTRAN (and linked via f2py) or C/C++ (and linked via cython). Parallelization strategy: None Operating systems: Tested exclusivey on Linux (experience on Ubuntu 12.04 or more recent, and SLC 5 or more recent) Other prerequisites: Python 2.7+ (never tested on Python 3) Libraries: numpy, cython, f2py. Optionally: pyfftw, KLU interface).","title":"Technical information"},{"location":"codes/codes_pages/PyPIC/#other-information","text":"Developed by: Giovanni Iadarola , Kevin Li, Eleonora Belli, Lotta Mether License: CERN Copyright Contact persons: Giovanni Iadarola Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/PyRADISE/","text":"PyRADISE Short description PyRADISE is a Python code for studying RAdial DIffusion and Stability Evolution. It numerically solves a PDE (diffusion equation) using a FVM scheme. Then the Evolving stability is evaluated by using PySSD to numerically solve the dipersion integral based on the bunch distribution function from the PDE solver and amplitude detuning. Web resources Git repository: https://gitlab.cern.ch/IRIS/pyradise Getting started guides: Examples: Documentation: : Technical information Programming Languages used for implementation: Python Parallelization strategy: None Operating systems: Tested on Linux (Ubuntu, CENTOS7) Other prerequisites: Numpy, scipy and PySSD Other information Developed by: S.V. Furuseth License: CERN Copyright Contact persons: S.V. Furuseth , X. Buffat","title":"**PyRADISE**"},{"location":"codes/codes_pages/PyRADISE/#pyradise","text":"","title":"PyRADISE"},{"location":"codes/codes_pages/PyRADISE/#short-description","text":"PyRADISE is a Python code for studying RAdial DIffusion and Stability Evolution. It numerically solves a PDE (diffusion equation) using a FVM scheme. Then the Evolving stability is evaluated by using PySSD to numerically solve the dipersion integral based on the bunch distribution function from the PDE solver and amplitude detuning.","title":"Short description"},{"location":"codes/codes_pages/PyRADISE/#web-resources","text":"Git repository: https://gitlab.cern.ch/IRIS/pyradise Getting started guides: Examples: Documentation: :","title":"Web resources"},{"location":"codes/codes_pages/PyRADISE/#technical-information","text":"Programming Languages used for implementation: Python Parallelization strategy: None Operating systems: Tested on Linux (Ubuntu, CENTOS7) Other prerequisites: Numpy, scipy and PySSD","title":"Technical information"},{"location":"codes/codes_pages/PyRADISE/#other-information","text":"Developed by: S.V. Furuseth License: CERN Copyright Contact persons: S.V. Furuseth , X. Buffat","title":"Other information"},{"location":"codes/codes_pages/PySSD/","text":"PySSD (Python Solver for Stability Diagrams) Short description Numerical solver for stability diagrams based on the beam distribution and the amplitude detuning from analytical formulas or interfaced with tracking codes, respectively SixTrack and MadX . Web resources Sources CWG presentation Technical information Programming Languages used for implementation: Python 2.6 or higher Operating systems: tested exclusivey on Linux (Ubuntu 12.04 and SLC 5) Other prerequisites: Libraries: numpy, scipy Other informations Developed by : CERN License : CERN Copyright Contact persons : Xavier Buffat","title":"**PySSD (Python Solver for Stability Diagrams)**"},{"location":"codes/codes_pages/PySSD/#pyssd-python-solver-for-stability-diagrams","text":"","title":"PySSD (Python Solver for Stability Diagrams)"},{"location":"codes/codes_pages/PySSD/#short-description","text":"Numerical solver for stability diagrams based on the beam distribution and the amplitude detuning from analytical formulas or interfaced with tracking codes, respectively SixTrack and MadX .","title":"Short description"},{"location":"codes/codes_pages/PySSD/#web-resources","text":"Sources CWG presentation","title":"Web resources"},{"location":"codes/codes_pages/PySSD/#technical-information","text":"Programming Languages used for implementation: Python 2.6 or higher Operating systems: tested exclusivey on Linux (Ubuntu 12.04 and SLC 5) Other prerequisites: Libraries: numpy, scipy","title":"Technical information"},{"location":"codes/codes_pages/PySSD/#other-informations","text":"Developed by : CERN License : CERN Copyright Contact persons : Xavier Buffat","title":"Other informations"},{"location":"codes/codes_pages/PyTimber/","text":"PyTimber Short description Python Wrapper to CERN Logging Java API (CALS) to extract machine data (LHC, SPS, ...) from the CERN Logging service. Web resources Documentation : PyTimber Wiki (CERN internal link) Installation: PyTimber Installation Guide (CERN internal link) Source code: PyTimber GitLab Pages Technical information [Provide the following information] Programming Languages used for implementation: Python Operating systems: Multiple OS Other prerequisites: jdk 1.8, jpype, numpy, (optional matplotlib) Other information Developed by: P. Sowinski, P. Elson, R. De Maria, T. Levens, C. Hernalsteens, M. Betz. License: No Explicit License Contact persons: P. Sowinski Being actively developed and supported: Yes","title":"**PyTimber**"},{"location":"codes/codes_pages/PyTimber/#pytimber","text":"","title":"PyTimber"},{"location":"codes/codes_pages/PyTimber/#short-description","text":"Python Wrapper to CERN Logging Java API (CALS) to extract machine data (LHC, SPS, ...) from the CERN Logging service.","title":"Short description"},{"location":"codes/codes_pages/PyTimber/#web-resources","text":"Documentation : PyTimber Wiki (CERN internal link) Installation: PyTimber Installation Guide (CERN internal link) Source code: PyTimber GitLab Pages","title":"Web resources"},{"location":"codes/codes_pages/PyTimber/#technical-information","text":"[Provide the following information] Programming Languages used for implementation: Python Operating systems: Multiple OS Other prerequisites: jdk 1.8, jpype, numpy, (optional matplotlib)","title":"Technical information"},{"location":"codes/codes_pages/PyTimber/#other-information","text":"Developed by: P. Sowinski, P. Elson, R. De Maria, T. Levens, C. Hernalsteens, M. Betz. License: No Explicit License Contact persons: P. Sowinski Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/RF-Track/","text":"RF-Track Short description RF-Track is a novel tracking code developed for the optimization of low-energy linacs in presence of space-charge effects. RF-Track can transport beams of particles with arbitrary mass and charge even mixed together, solving fully relativistic equations of motion. It implements direct space-charge effects in a physically consistent manner, using parallel algorithms. It can simulate bunched beams as well as continuous beams, and transport through conventional elements as well as through field maps of oscillating electromagnetic fields. RF-Track is written in optimized and parallel C++, and it uses the scripting languages Octave and Python as user interfaces. RF-Track has been tested successfully in several cases: TULIP, a backward-traveling-wave linac for medical applications; 750 MHz CERN's radio-frequency quadrupole; the transfer line for low energy anti-protons of the ELENA ring; the CLIC positron injector, and the AWAKE injector linac. Web resources Source code: not yet published Short Presentation: https://indico.cern.ch/event/514848/contributions/2037361/attachments/1250710/1844703/ABP_Information_2016_Mar_31.pdf Technical information Programming Languages used for implementation: C++, SWIG Parallelization strategy: C++11 threads, multi-core parallelism Operating systems: Linux, MacOS Other prerequisites: GSL, fftw User interface: octave, python Other information Developed by: Andrea Latina License: CERN License Contact persons: Andrea Latina Being actively developed and supported: Yes","title":"**RF-Track**"},{"location":"codes/codes_pages/RF-Track/#rf-track","text":"","title":"RF-Track"},{"location":"codes/codes_pages/RF-Track/#short-description","text":"RF-Track is a novel tracking code developed for the optimization of low-energy linacs in presence of space-charge effects. RF-Track can transport beams of particles with arbitrary mass and charge even mixed together, solving fully relativistic equations of motion. It implements direct space-charge effects in a physically consistent manner, using parallel algorithms. It can simulate bunched beams as well as continuous beams, and transport through conventional elements as well as through field maps of oscillating electromagnetic fields. RF-Track is written in optimized and parallel C++, and it uses the scripting languages Octave and Python as user interfaces. RF-Track has been tested successfully in several cases: TULIP, a backward-traveling-wave linac for medical applications; 750 MHz CERN's radio-frequency quadrupole; the transfer line for low energy anti-protons of the ELENA ring; the CLIC positron injector, and the AWAKE injector linac.","title":"Short description"},{"location":"codes/codes_pages/RF-Track/#web-resources","text":"Source code: not yet published Short Presentation: https://indico.cern.ch/event/514848/contributions/2037361/attachments/1250710/1844703/ABP_Information_2016_Mar_31.pdf","title":"Web resources"},{"location":"codes/codes_pages/RF-Track/#technical-information","text":"Programming Languages used for implementation: C++, SWIG Parallelization strategy: C++11 threads, multi-core parallelism Operating systems: Linux, MacOS Other prerequisites: GSL, fftw User interface: octave, python","title":"Technical information"},{"location":"codes/codes_pages/RF-Track/#other-information","text":"Developed by: Andrea Latina License: CERN License Contact persons: Andrea Latina Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/SIRE/","text":"SIRE (Software for IBS and Radiation Effects) Short description The SIRE was inspired by MOCAC (MOnte CArlo Code). After specifying the beam distribution and the optics along a lattice, SIRE iteratively computes intrabeam collisions between pairs of macro-particles. If requested it also evaluates the effects of synchrotron radiation damping and quantum excitation. The beam distribution is updated and the rms beam emittances are recomputed, giving finally as output the emittance evolution in time. Web resources Source code: can be downoloaded here Documentation: in preparation... Technical information Programming Languages used for implementation: C Parallelization strategy: None Operating systems: Linux, Windows Other prerequisites: None Other information Developed by: M. Martini, A. Vivoli License: CERN Copyright Contact persons: F. Antoniou, S. Papadopoulou, Y. Papaphilippou Being actively developed and supported: Yes","title":"**SIRE (Software for IBS and Radiation Effects)**"},{"location":"codes/codes_pages/SIRE/#sire-software-for-ibs-and-radiation-effects","text":"","title":"SIRE (Software for IBS and Radiation Effects)"},{"location":"codes/codes_pages/SIRE/#short-description","text":"The SIRE was inspired by MOCAC (MOnte CArlo Code). After specifying the beam distribution and the optics along a lattice, SIRE iteratively computes intrabeam collisions between pairs of macro-particles. If requested it also evaluates the effects of synchrotron radiation damping and quantum excitation. The beam distribution is updated and the rms beam emittances are recomputed, giving finally as output the emittance evolution in time.","title":"Short description"},{"location":"codes/codes_pages/SIRE/#web-resources","text":"Source code: can be downoloaded here Documentation: in preparation...","title":"Web resources"},{"location":"codes/codes_pages/SIRE/#technical-information","text":"Programming Languages used for implementation: C Parallelization strategy: None Operating systems: Linux, Windows Other prerequisites: None","title":"Technical information"},{"location":"codes/codes_pages/SIRE/#other-information","text":"Developed by: M. Martini, A. Vivoli License: CERN Copyright Contact persons: F. Antoniou, S. Papadopoulou, Y. Papaphilippou Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/SUSSIX/","text":"sussix Interpolated FFT for up 9999 cases in bpm.XXXX files or 32 SixTrack binary files [Provide the following information] Programming Languages used for implementation: Fortran Parallelization strategy: none Operating systems: LINUX Other prerequisites: none Developed by: [R. Bartolini and F. Schmidt] License: none Contact persons: F. Schmidt Being actively developed and supported: [Yes]","title":"**sussix**"},{"location":"codes/codes_pages/SUSSIX/#sussix","text":"","title":"sussix"},{"location":"codes/codes_pages/SUSSIX/#interpolated-fft-for-up-9999-cases-in-bpmxxxx-files-or-32-sixtrack-binary-files","text":"","title":"Interpolated FFT for up 9999 cases in bpm.XXXX files or 32 SixTrack binary files"},{"location":"codes/codes_pages/SixTrack/","text":"SixTrack Short description SixTrack is a single particle 6D symplectic tracking code optimized for long term tracking in high energy rings. It is mainly used for the LHC for dynamic aperture studies, tune optimization, collimation studies. Web resources The main website contains links to source code, manual and documentation. Technical information Programming Languages used for implementation: Fortran (f90) C Parallelization strategy: Vectorization of loops Operating systems: Linux, Windows, Mac Other information Developed by: Frank Schmidt, Eric McIntosh et al. License: LGPLv2 Contact persons: riccardo.de.maria@cern.ch Being actively developed and supported: Yes","title":"**SixTrack**"},{"location":"codes/codes_pages/SixTrack/#sixtrack","text":"","title":"SixTrack"},{"location":"codes/codes_pages/SixTrack/#short-description","text":"SixTrack is a single particle 6D symplectic tracking code optimized for long term tracking in high energy rings. It is mainly used for the LHC for dynamic aperture studies, tune optimization, collimation studies.","title":"Short description"},{"location":"codes/codes_pages/SixTrack/#web-resources","text":"The main website contains links to source code, manual and documentation.","title":"Web resources"},{"location":"codes/codes_pages/SixTrack/#technical-information","text":"Programming Languages used for implementation: Fortran (f90) C Parallelization strategy: Vectorization of loops Operating systems: Linux, Windows, Mac","title":"Technical information"},{"location":"codes/codes_pages/SixTrack/#other-information","text":"Developed by: Frank Schmidt, Eric McIntosh et al. License: LGPLv2 Contact persons: riccardo.de.maria@cern.ch Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/TRAIN/","text":"TRAIN Short description In part of the straight sections of the LHC the two beams share a common beam tube. Therefore the bunches cross each other not only at the interaction point, but as well at many places on either side, with a typical transverse separation of 10 times the transverse beam size. These \"parasitic\" encounters lead to orbit distortions and tune shifts, in addition to higher order effects. Since the string of bunches from the injection machine contains gaps, not all possible 3564 \"buckets\" around the machine are filled, but only about 3000. This in turn causes some bunches to not always encounter bunches in the opposite beam at one or several parasitic collision points (so-called \"pacman\" bunches), or even at the head-on interaction point (\"super-pacman\" bunches). With this special program self-consistent orbits in the LHC have been calculated for the first time with the full beam-beam collision scheme resulting from various injection scenarios. The offsets at the interaction points, and the tune shifts are shown to be small enough to be easily controlled. Web resources Source code: http://gitlab.cern.ch/agorzaws/train Old webpage: http://lhc-beam-beam.web.cern.ch/lhc-beam-beam/train_welcome.html ABP CWG presentation : https://indico.cern.ch/event/631880/contributions/2557453/attachments/1450659/2236799/2017-04-27_TRAIN-expanded.pdf TRAIN wiki page Source code (PyTRAIN): https://gitlab.cern.ch/mihostet/pytrain PyTRAIN presentation : https://indico.cern.ch/event/658908/contributions/2686544/attachments/1507510/2350662/emitscan_train_mad.pdf Technical information [Provide the following information] Programming Languages used for implementation: Fortran Parallelization strategy: None Operating systems: SLC6 Other prerequisites: None Other information Developed by: F.C. Iselin, E. Keil, H. Grote, W. Herr, A. Gorzawski License: None Contact persons: A. Gorzawski, T. Pieloni, X. Buffat Being actively developed and supported: No","title":"**TRAIN**"},{"location":"codes/codes_pages/TRAIN/#train","text":"","title":"TRAIN"},{"location":"codes/codes_pages/TRAIN/#short-description","text":"In part of the straight sections of the LHC the two beams share a common beam tube. Therefore the bunches cross each other not only at the interaction point, but as well at many places on either side, with a typical transverse separation of 10 times the transverse beam size. These \"parasitic\" encounters lead to orbit distortions and tune shifts, in addition to higher order effects. Since the string of bunches from the injection machine contains gaps, not all possible 3564 \"buckets\" around the machine are filled, but only about 3000. This in turn causes some bunches to not always encounter bunches in the opposite beam at one or several parasitic collision points (so-called \"pacman\" bunches), or even at the head-on interaction point (\"super-pacman\" bunches). With this special program self-consistent orbits in the LHC have been calculated for the first time with the full beam-beam collision scheme resulting from various injection scenarios. The offsets at the interaction points, and the tune shifts are shown to be small enough to be easily controlled.","title":"Short description"},{"location":"codes/codes_pages/TRAIN/#web-resources","text":"Source code: http://gitlab.cern.ch/agorzaws/train Old webpage: http://lhc-beam-beam.web.cern.ch/lhc-beam-beam/train_welcome.html ABP CWG presentation : https://indico.cern.ch/event/631880/contributions/2557453/attachments/1450659/2236799/2017-04-27_TRAIN-expanded.pdf TRAIN wiki page Source code (PyTRAIN): https://gitlab.cern.ch/mihostet/pytrain PyTRAIN presentation : https://indico.cern.ch/event/658908/contributions/2686544/attachments/1507510/2350662/emitscan_train_mad.pdf","title":"Web resources"},{"location":"codes/codes_pages/TRAIN/#technical-information","text":"[Provide the following information] Programming Languages used for implementation: Fortran Parallelization strategy: None Operating systems: SLC6 Other prerequisites: None","title":"Technical information"},{"location":"codes/codes_pages/TRAIN/#other-information","text":"Developed by: F.C. Iselin, E. Keil, H. Grote, W. Herr, A. Gorzawski License: None Contact persons: A. Gorzawski, T. Pieloni, X. Buffat Being actively developed and supported: No","title":"Other information"},{"location":"computing_resources/abpstorage/","text":"ABP data storage EOS storage ABP has a general storage space for measurements and simulation data in \\eos\\project\\a\\abpdata Please contact abp.computing[AT]cern.ch for further information.","title":"ABP data storage"},{"location":"computing_resources/abpstorage/#abp-data-storage","text":"","title":"ABP data storage"},{"location":"computing_resources/abpstorage/#eos-storage","text":"ABP has a general storage space for measurements and simulation data in \\eos\\project\\a\\abpdata Please contact abp.computing[AT]cern.ch for further information.","title":"EOS storage"},{"location":"computing_resources/cernbatch/","text":"HTCondor batch system Computing jobs that run on individual nodes with up to 32 CPU cores per node can be submitted to the CERN batch service (lxbatch). The jobs are submitted and managed using the HTCcondor platform. Access All users having access to the CERN linux service and the AFS filesystem (which can be self-enabled at https://resources.web.cern.ch ) can submit jobs to HTCondor, but have by default rather low priority. ABP users working on computationally intensive tasks can be granted higher priority, by being added to one of the following e-groups (based on their section): Section name e-group name BE-ABP-CEI batch-u-abp-cei BE-ABP-HSL batch-u-abp-hsl BE-ABP-INC batch-u-abp-inc BE-ABP-LAF batch-u-abp-laf BE-ABP-LNO batch-u-abp-lno BE-ABP-NDC batch-u-abp-ndc The section leaders and the ABP-CP members have admin rights to add users to the e-group of their section. All these e-groups are mapped to a single computing group called \u201cgroup_u_BE.ABP.NORMAL\u201d. Usage Detailed documentation managed by the IT department can be found here: https://batchdocs.web.cern.ch/index.html A quick start guide can be found here: https://batchdocs.web.cern.ch/local/quick.html GUIs for monitoring the clusters can be found here: https://batch-carbon.cern.ch/grafana/dashboard/db/user-batch-jobs https://batch-carbon.cern.ch/grafana/dashboard/db/cluster-batch-jobs https://monit-grafana.cern.ch/d/000000865/experiment-batch-details https://monit-grafana.cern.ch/d/000000868/schedds GPUs Graphics Processing Units are available in the system. To use GPUs please follow the instructions available here: https://batchdocs.web.cern.ch/tutorial/exercise10.html An example submit file is: executable = job_name/job_name.sh arguments = $(ClusterId) $(ProcId) output = job_name/htcondor.out error = job_name/htcondor.err log = job_name/htcondor.log transfer_input_files = job_name requirements = regexp(\"V100\", Target.CUDADeviceName) request_GPUs = 1 request_CPUs = 1 +MaxRunTime = 86400 queue The line requirements = regexp(\"V100\", Target.CUDADeviceName) will find nodes that only have a V100 GPU. Nodes with T4 GPUs also exist. Nodes with large memory Some nodes are equipped with a larger number of cores and memory, namely (31 Oct 2019): - a few nodes with 24 physical cores and 1Tb of memory - 6 nodes with 48 cores (hyperthreaded) and 512Gb of memory These can be used via HTCondor by adding the appropriate lines to the submit file, e.g.: RequestCpus = 24 +BigMemJob = True Members of the accounting group group_u_BE.ABP.NORMAL should have access to run on these nodes. Access can be granted to other users in the group. Specific applications Example on how HTCondor is used to manage PyECLOUD, PyHEADTAIL and PLACET simulations can be found here: https://indico.cern.ch/event/637703/contributions/2583365/ https://indico.cern.ch/event/580885/contributions/2355043/ https://twiki.cern.ch/twiki/pub/ABPComputing/Placet/Run_placet_on_HTCondor.pdf For administrators The shares of the different groups can be monitored on https://haggis.cern.ch/ (available only inside the CERN network). Search for \"be\" to see our shares. FAQs Scheduler Not Replying From time to time it happens that the scheduler does not reply. In general, it is a temporary problem; if this is not the case, open IT ticket . At the same time, you may try changing the scheduler you are assigned by default. This can be accomplished by one of two ways: setting the two environment variables: _condor_SCHEDD_HOST and _condor_CREDD_HOST . E.g.: tcsh setenv _condor_SCHEDD_HOST bigbird02.cern.ch setenv _condor_CREDD_HOST bigbird02.cern.ch bash export _condor_SCHEDD_HOST = \"bigbird02.cern.ch\" export _condor_CREDD_HOST = \"bigbird02.cern.ch\" In the output of a simple call to condor_q you can find the scheduler name. If you don't set these variables, the reported scheduler name is the one assigned to you by default; otherwise, you should find the one that you have set via the previous variables. Please keep in mind that these statements, if typed on terminal, will apply only to that session. For instance, in case you log out or the lxplus session expires, you have to re-set those two variables if you want them also in the new session. So, please remember the scheduler that you have requested, otherwise you won't be able to retrieve the results form HTCondor . Calling condor commands with -name parameter. You can use another than your defined schedular by addressing it directly in your commands, e.g. condor_q -name bigbird15.cern.ch This should work for any of the condor -commands (e.g. condor_q , condor_submit , etc.). The scheduler is then used only for this command. Jobs Being Taken Very Slowly It may happen that you see your jobs queueing for too long. This might be simply due to overload of the batch system (please check the batch GUI ); more rarely, it can be also a problem with priorities. Indeed, it may happen that your jobs are assigned (by mistake) an accounting group with very low priority. Hence, you can check if your jobs are assigned the wrong accounting group via (an example output is shown): $ condor_q owner $LOGNAME -long | grep '^AccountingGroup' | sort | uniq -c 9 AccountingGroup = \"group_u_ATLAS.u_zp.nkarast\" 1496 AccountingGroup = \"group_u_BE.UNIX.u_pz.nkarast\" You can force the use of the high priority accounting group modifying your .sub script as: +AccountingGroup = \"group_u_BE.ABP.NORMAL\" Advanced features Spool Option The -spool option can be used at condor_submit level, e.g. condor_submit -spool htcondor.sub In this case, all the output files ( transfer_output_files ) and the error , log and output files are not generated once the jobs finishes, but only when requested by the user, after the job is over. Retrieval can be done via the following command: condor_transfer_data $LOGNAME -const 'JobStatus == 4' In the above example, the files from all completed job in each cluster will be retrieved. In this case, the jobs may not automatically disappear from condor_q . Job removal takes place after 10 days the job has finished. Submitting Jobs to HTCondor from a local Machine The recommended way of using HTCondor is to submit jobs by logging to lxplus.cern.ch. It is also possible to configure your own computer to manage HTCondor jobs, as described in this guide .","title":"HTCondor batch system"},{"location":"computing_resources/cernbatch/#htcondor-batch-system","text":"Computing jobs that run on individual nodes with up to 32 CPU cores per node can be submitted to the CERN batch service (lxbatch). The jobs are submitted and managed using the HTCcondor platform.","title":"HTCondor batch system"},{"location":"computing_resources/cernbatch/#access","text":"All users having access to the CERN linux service and the AFS filesystem (which can be self-enabled at https://resources.web.cern.ch ) can submit jobs to HTCondor, but have by default rather low priority. ABP users working on computationally intensive tasks can be granted higher priority, by being added to one of the following e-groups (based on their section): Section name e-group name BE-ABP-CEI batch-u-abp-cei BE-ABP-HSL batch-u-abp-hsl BE-ABP-INC batch-u-abp-inc BE-ABP-LAF batch-u-abp-laf BE-ABP-LNO batch-u-abp-lno BE-ABP-NDC batch-u-abp-ndc The section leaders and the ABP-CP members have admin rights to add users to the e-group of their section. All these e-groups are mapped to a single computing group called \u201cgroup_u_BE.ABP.NORMAL\u201d.","title":"Access"},{"location":"computing_resources/cernbatch/#usage","text":"Detailed documentation managed by the IT department can be found here: https://batchdocs.web.cern.ch/index.html A quick start guide can be found here: https://batchdocs.web.cern.ch/local/quick.html GUIs for monitoring the clusters can be found here: https://batch-carbon.cern.ch/grafana/dashboard/db/user-batch-jobs https://batch-carbon.cern.ch/grafana/dashboard/db/cluster-batch-jobs https://monit-grafana.cern.ch/d/000000865/experiment-batch-details https://monit-grafana.cern.ch/d/000000868/schedds","title":"Usage"},{"location":"computing_resources/cernbatch/#gpus","text":"Graphics Processing Units are available in the system. To use GPUs please follow the instructions available here: https://batchdocs.web.cern.ch/tutorial/exercise10.html An example submit file is: executable = job_name/job_name.sh arguments = $(ClusterId) $(ProcId) output = job_name/htcondor.out error = job_name/htcondor.err log = job_name/htcondor.log transfer_input_files = job_name requirements = regexp(\"V100\", Target.CUDADeviceName) request_GPUs = 1 request_CPUs = 1 +MaxRunTime = 86400 queue The line requirements = regexp(\"V100\", Target.CUDADeviceName) will find nodes that only have a V100 GPU. Nodes with T4 GPUs also exist.","title":"GPUs"},{"location":"computing_resources/cernbatch/#nodes-with-large-memory","text":"Some nodes are equipped with a larger number of cores and memory, namely (31 Oct 2019): - a few nodes with 24 physical cores and 1Tb of memory - 6 nodes with 48 cores (hyperthreaded) and 512Gb of memory These can be used via HTCondor by adding the appropriate lines to the submit file, e.g.: RequestCpus = 24 +BigMemJob = True Members of the accounting group group_u_BE.ABP.NORMAL should have access to run on these nodes. Access can be granted to other users in the group.","title":"Nodes with large memory"},{"location":"computing_resources/cernbatch/#specific-applications","text":"Example on how HTCondor is used to manage PyECLOUD, PyHEADTAIL and PLACET simulations can be found here: https://indico.cern.ch/event/637703/contributions/2583365/ https://indico.cern.ch/event/580885/contributions/2355043/ https://twiki.cern.ch/twiki/pub/ABPComputing/Placet/Run_placet_on_HTCondor.pdf","title":"Specific applications"},{"location":"computing_resources/cernbatch/#for-administrators","text":"The shares of the different groups can be monitored on https://haggis.cern.ch/ (available only inside the CERN network). Search for \"be\" to see our shares.","title":"For administrators"},{"location":"computing_resources/cernbatch/#faqs","text":"","title":"FAQs"},{"location":"computing_resources/cernbatch/#scheduler-not-replying","text":"From time to time it happens that the scheduler does not reply. In general, it is a temporary problem; if this is not the case, open IT ticket . At the same time, you may try changing the scheduler you are assigned by default. This can be accomplished by one of two ways: setting the two environment variables: _condor_SCHEDD_HOST and _condor_CREDD_HOST . E.g.: tcsh setenv _condor_SCHEDD_HOST bigbird02.cern.ch setenv _condor_CREDD_HOST bigbird02.cern.ch bash export _condor_SCHEDD_HOST = \"bigbird02.cern.ch\" export _condor_CREDD_HOST = \"bigbird02.cern.ch\" In the output of a simple call to condor_q you can find the scheduler name. If you don't set these variables, the reported scheduler name is the one assigned to you by default; otherwise, you should find the one that you have set via the previous variables. Please keep in mind that these statements, if typed on terminal, will apply only to that session. For instance, in case you log out or the lxplus session expires, you have to re-set those two variables if you want them also in the new session. So, please remember the scheduler that you have requested, otherwise you won't be able to retrieve the results form HTCondor . Calling condor commands with -name parameter. You can use another than your defined schedular by addressing it directly in your commands, e.g. condor_q -name bigbird15.cern.ch This should work for any of the condor -commands (e.g. condor_q , condor_submit , etc.). The scheduler is then used only for this command.","title":"Scheduler Not Replying"},{"location":"computing_resources/cernbatch/#jobs-being-taken-very-slowly","text":"It may happen that you see your jobs queueing for too long. This might be simply due to overload of the batch system (please check the batch GUI ); more rarely, it can be also a problem with priorities. Indeed, it may happen that your jobs are assigned (by mistake) an accounting group with very low priority. Hence, you can check if your jobs are assigned the wrong accounting group via (an example output is shown): $ condor_q owner $LOGNAME -long | grep '^AccountingGroup' | sort | uniq -c 9 AccountingGroup = \"group_u_ATLAS.u_zp.nkarast\" 1496 AccountingGroup = \"group_u_BE.UNIX.u_pz.nkarast\" You can force the use of the high priority accounting group modifying your .sub script as: +AccountingGroup = \"group_u_BE.ABP.NORMAL\"","title":"Jobs Being Taken Very Slowly"},{"location":"computing_resources/cernbatch/#advanced-features","text":"","title":"Advanced features"},{"location":"computing_resources/cernbatch/#spool-option","text":"The -spool option can be used at condor_submit level, e.g. condor_submit -spool htcondor.sub In this case, all the output files ( transfer_output_files ) and the error , log and output files are not generated once the jobs finishes, but only when requested by the user, after the job is over. Retrieval can be done via the following command: condor_transfer_data $LOGNAME -const 'JobStatus == 4' In the above example, the files from all completed job in each cluster will be retrieved. In this case, the jobs may not automatically disappear from condor_q . Job removal takes place after 10 days the job has finished.","title":"Spool Option"},{"location":"computing_resources/cernbatch/#submitting-jobs-to-htcondor-from-a-local-machine","text":"The recommended way of using HTCondor is to submit jobs by logging to lxplus.cern.ch. It is also possible to configure your own computer to manage HTCondor jobs, as described in this guide .","title":"Submitting Jobs to HTCondor from a local Machine"},{"location":"computing_resources/cernts/","text":"","title":"Cernts"},{"location":"computing_resources/hpc_cern/","text":"High Performance Computing clusters at CERN Since end 2017, two new HPC cluster have been setup here at CERN by the IT department for users of MPI applications. Access ABP members who need access to the cluster can request it by writing to abp.computing[AT]cern.ch in order to be added to the relevant e-group (service-hpc-be). Please note that these are dedicated facilities for parallel computing applications, i.e. applications running with MPI across more than 32 cores for each simulation (hence requiring multiple nodes). Simulations requiring less than 32 cores should be submitted to the CERN batch service . Documentation and other resources Documentation on the cluster and its usage can be found at: https://batchdocs.web.cern.ch/linuxhpc/index.html Support and further info can be found in the Service Portal: https://cern.service-now.com/service-portal?id=service_element&name=High-Performance-Computing A dedicated workshop in two sessions took place in 2020. The corresponding indico pages can be found at: https://indico.cern.ch/event/867459/ https://indico.cern.ch/event/916903/ Regular meetings are held by IT to share information on the facility development: https://indico.cern.ch/category/7950/ Some further information can be found in dedicated presentations in past ABP-CWG meetings: https://indico.cern.ch/event/676152 https://indico.cern.ch/event/724162/","title":"High Performance Computing clusters at CERN"},{"location":"computing_resources/hpc_cern/#high-performance-computing-clusters-at-cern","text":"Since end 2017, two new HPC cluster have been setup here at CERN by the IT department for users of MPI applications.","title":"High Performance Computing clusters at CERN"},{"location":"computing_resources/hpc_cern/#access","text":"ABP members who need access to the cluster can request it by writing to abp.computing[AT]cern.ch in order to be added to the relevant e-group (service-hpc-be). Please note that these are dedicated facilities for parallel computing applications, i.e. applications running with MPI across more than 32 cores for each simulation (hence requiring multiple nodes). Simulations requiring less than 32 cores should be submitted to the CERN batch service .","title":"Access"},{"location":"computing_resources/hpc_cern/#documentation-and-other-resources","text":"Documentation on the cluster and its usage can be found at: https://batchdocs.web.cern.ch/linuxhpc/index.html Support and further info can be found in the Service Portal: https://cern.service-now.com/service-portal?id=service_element&name=High-Performance-Computing A dedicated workshop in two sessions took place in 2020. The corresponding indico pages can be found at: https://indico.cern.ch/event/867459/ https://indico.cern.ch/event/916903/ Regular meetings are held by IT to share information on the facility development: https://indico.cern.ch/category/7950/ Some further information can be found in dedicated presentations in past ABP-CWG meetings: https://indico.cern.ch/event/676152 https://indico.cern.ch/event/724162/","title":"Documentation and other resources"},{"location":"computing_resources/hpc_cnaf/","text":"High Performance Computing clusters at INFN-CNAF An HPC cluster dedicated to CERN studies is available at INFN-CNAF (Bologna, Italy) allowing for MPI applications across multiple nodes. The cluster presently features a total of about 800 CPU-cores. Access Access to the cluster can be requested by sending an email to abp.computing[AT]cern.ch including the following information: The filled in form . Please add Daniele Cesini as contact person and \"Access to the hpc_acc Cluster to run High Energy Physics simulation codes\" as the reason. A copy of identity card or other valid identification document (e.g. passport). Usage Users should connect via ssh to bastion.cnaf.infn.it and from there access the cluster front-end called ui-hpc2.cr.cnaf.infn.it . The LSF system is installed on the cluster to manage job submissions. Jobs should be submitted to the \u201dhpc-acc\u201d queue. If you have inquiries about running your jobs on this cluster or need technical support, please write an email to hpc-support@listsNOSPAMPLEASE.cnaf.infn.it A node called hpc-201-11-35 is equipped with four V100 GPUs , and can be used interactively. Other links Relevant links to obtain further information on this cluster are: Presentation by Antonio Falabella at ABP-CWG meeting on 20 July, 2017 Presentation by Annalisa Romano on how to submit jobs at ABP-CWG meeting on 4 July 2017 Note on outcome of the acceptance test to proceed to the second phase of the cluster procurement and installation: Performance of space charge simulations using High Performance Computing (HPC) cluster CERN-ACC-NOTE-2017-0048 Support Please note that Antonio Falabella ( antonio.falabella@cnafNOSPAMPLEASE.infn.it ) is available to assist you, should you encounter any problem in connecting or running your jobs in the present configuration.","title":"High Performance Computing clusters at INFN-CNAF"},{"location":"computing_resources/hpc_cnaf/#high-performance-computing-clusters-at-infn-cnaf","text":"An HPC cluster dedicated to CERN studies is available at INFN-CNAF (Bologna, Italy) allowing for MPI applications across multiple nodes. The cluster presently features a total of about 800 CPU-cores.","title":"High Performance Computing clusters at INFN-CNAF"},{"location":"computing_resources/hpc_cnaf/#access","text":"Access to the cluster can be requested by sending an email to abp.computing[AT]cern.ch including the following information: The filled in form . Please add Daniele Cesini as contact person and \"Access to the hpc_acc Cluster to run High Energy Physics simulation codes\" as the reason. A copy of identity card or other valid identification document (e.g. passport).","title":"Access"},{"location":"computing_resources/hpc_cnaf/#usage","text":"Users should connect via ssh to bastion.cnaf.infn.it and from there access the cluster front-end called ui-hpc2.cr.cnaf.infn.it . The LSF system is installed on the cluster to manage job submissions. Jobs should be submitted to the \u201dhpc-acc\u201d queue. If you have inquiries about running your jobs on this cluster or need technical support, please write an email to hpc-support@listsNOSPAMPLEASE.cnaf.infn.it A node called hpc-201-11-35 is equipped with four V100 GPUs , and can be used interactively.","title":"Usage"},{"location":"computing_resources/hpc_cnaf/#other-links","text":"Relevant links to obtain further information on this cluster are: Presentation by Antonio Falabella at ABP-CWG meeting on 20 July, 2017 Presentation by Annalisa Romano on how to submit jobs at ABP-CWG meeting on 4 July 2017 Note on outcome of the acceptance test to proceed to the second phase of the cluster procurement and installation: Performance of space charge simulations using High Performance Computing (HPC) cluster CERN-ACC-NOTE-2017-0048","title":"Other links"},{"location":"computing_resources/hpc_cnaf/#support","text":"Please note that Antonio Falabella ( antonio.falabella@cnafNOSPAMPLEASE.infn.it ) is available to assist you, should you encounter any problem in connecting or running your jobs in the present configuration.","title":"Support"},{"location":"computing_resources/lxplus/","text":"LXPLUS service LXPLUS (Linux Public Login User Service) is the interactive logon service to Linux for all CERN users. The cluster LXPLUS consists of public machines provided by the IT Department for interactive work. Detailed documentation maintained by the IT Department is available here: https://lxplusdoc.web.cern.ch Access In order to access LXPLUS you need to request the activation of \"AFS Workspaces\" and of \"LXPLUS and linux\" for your account. This can be done on the CERN Resource Portal .","title":"LXPLUS service"},{"location":"computing_resources/lxplus/#lxplus-service","text":"LXPLUS (Linux Public Login User Service) is the interactive logon service to Linux for all CERN users. The cluster LXPLUS consists of public machines provided by the IT Department for interactive work. Detailed documentation maintained by the IT Department is available here: https://lxplusdoc.web.cern.ch","title":"LXPLUS service"},{"location":"computing_resources/lxplus/#access","text":"In order to access LXPLUS you need to request the activation of \"AFS Workspaces\" and of \"LXPLUS and linux\" for your account. This can be done on the CERN Resource Portal .","title":"Access"},{"location":"computing_resources/workstations/","text":"ABP workstations Workstations hosted at CERN The following workstations are avaiable for development and production. - Access is granted based on the use case. - For further information and to get access please contact abp.computing[AT]cern.ch. Host name CPU RAM GPU pcbe-abp-gpu001 AMD Ryzen Threadripper 2970WX 24-Core 128 GB 4x Titan V pcbe-abp-hpc001 AMD Ryzen Threadripper 3990X 64-Core 256 GB GeForce GTX 1660 Ti pcbe-abp-hpc002 AMD Ryzen Threadripper 3990X 64-Core 256 GB GeForce GTX 1660 Ti liupsgpu 2x Intel Xeon E5-2630 6-core 256 GB 3x Tesla C2075 When possible the CERN batch system (HTCondor) should be used for production studies while these workstation should be used for special cases (e.g. development, performance benchmarking, computations requiring very large amount of memory etc.). For administrators A user can change the password by simply typing passwd An administrator can add a user by: sudo adduser username An administrator can change the password for a user by typing: sudo passwd username An administrator can give sudo rights to an user by typing: usermod -aG sudo username Workstations hosted at INFN-CNAF GPUs are available also at CNAF (for users having a CNAF HPC account - more info available here ). You can launch GPU jobs interactively with the following steps: Connect with SSH into the login server: bastion.cnaf.infn.it ; Connect wiht SSH to the node that has the 4x Nvidia Tesla V100 GPUs: hpc-201-11-35 ; Enable newer versions of gcc, cmake etc. with scl enable devtoolset-7 bash","title":"ABP workstations"},{"location":"computing_resources/workstations/#abp-workstations","text":"","title":"ABP workstations"},{"location":"computing_resources/workstations/#workstations-hosted-at-cern","text":"The following workstations are avaiable for development and production. - Access is granted based on the use case. - For further information and to get access please contact abp.computing[AT]cern.ch. Host name CPU RAM GPU pcbe-abp-gpu001 AMD Ryzen Threadripper 2970WX 24-Core 128 GB 4x Titan V pcbe-abp-hpc001 AMD Ryzen Threadripper 3990X 64-Core 256 GB GeForce GTX 1660 Ti pcbe-abp-hpc002 AMD Ryzen Threadripper 3990X 64-Core 256 GB GeForce GTX 1660 Ti liupsgpu 2x Intel Xeon E5-2630 6-core 256 GB 3x Tesla C2075 When possible the CERN batch system (HTCondor) should be used for production studies while these workstation should be used for special cases (e.g. development, performance benchmarking, computations requiring very large amount of memory etc.).","title":"Workstations hosted at CERN"},{"location":"computing_resources/workstations/#for-administrators","text":"A user can change the password by simply typing passwd An administrator can add a user by: sudo adduser username An administrator can change the password for a user by typing: sudo passwd username An administrator can give sudo rights to an user by typing: usermod -aG sudo username","title":"For administrators"},{"location":"computing_resources/workstations/#workstations-hosted-at-infn-cnaf","text":"GPUs are available also at CNAF (for users having a CNAF HPC account - more info available here ). You can launch GPU jobs interactively with the following steps: Connect with SSH into the login server: bastion.cnaf.infn.it ; Connect wiht SSH to the node that has the 4x Nvidia Tesla V100 GPUs: hpc-201-11-35 ; Enable newer versions of gcc, cmake etc. with scl enable devtoolset-7 bash","title":"Workstations hosted at INFN-CNAF"},{"location":"guides/HLS/","text":"High Level Synthesize (HLS) Guide Slides and other materials given during the course will be uploaded here, Please send me your feedback/comments if you found something interesting to mention in the materials.","title":"HLS References"},{"location":"guides/HLS/#high-level-synthesize-hls-guide","text":"Slides and other materials given during the course will be uploaded here, Please send me your feedback/comments if you found something interesting to mention in the materials.","title":"High Level Synthesize (HLS) Guide"},{"location":"guides/Laboratory/","text":"Guidelines for Laboratory projects and your Final Project Minimal requirements Software projects should comply with the following basic guidelines: The source code should be hosted in an online git repository . Our course git repository is: ( https://github.com/System-Design-FPGA ) You can clone the repository on your file system, please don't forgot to pull and keep it updated, The other solution is to use our github repository by web browser, once you need a file you can copy it from the web and copy it in your file system. Since viavdo project will create many small files, it is not a good practice to keep all viavdo project files on a git repo. One or more \"ready-to-run\" examples should be made available in the repository, illustrating the main features of the code. Please make sure that the example are still updated and working when making modifications to the code! A simple \"Getting started guide\" should be made available, illustrating how to install the code and run an example. This can be provided as: A simple README file within the repository in text or markdown. Alternatively the GiHub wiki space associated with the repository can be used. A general paper describing a set of good computing practices that every researcher can adopt, regardless of their current level of computational skill is \"Good enough practices in scientific computing\" . Vivado installation A hands-on to install vivado on Linux and Windows platform will be created, however, it is easier to connect through the Laboratory and use university credentials to use vivado. hello world! program For each session, there is going to be a simple hello world program, then there are some sample codes, you can try them by your own and based on hands-on tutorials. It is appreciated to discuss them and any other problem you faced within these exercises. What to submit? The example of how and what to submit is available in the repository, please read the instruction carefully and submit what is expected and try to avoid unnecessary efforts.","title":"Guidelines for Laboratory projects and your Final Project"},{"location":"guides/Laboratory/#guidelines-for-laboratory-projects-and-your-final-project","text":"","title":"Guidelines for Laboratory projects and your Final Project"},{"location":"guides/Laboratory/#minimal-requirements","text":"Software projects should comply with the following basic guidelines: The source code should be hosted in an online git repository . Our course git repository is: ( https://github.com/System-Design-FPGA ) You can clone the repository on your file system, please don't forgot to pull and keep it updated, The other solution is to use our github repository by web browser, once you need a file you can copy it from the web and copy it in your file system. Since viavdo project will create many small files, it is not a good practice to keep all viavdo project files on a git repo. One or more \"ready-to-run\" examples should be made available in the repository, illustrating the main features of the code. Please make sure that the example are still updated and working when making modifications to the code! A simple \"Getting started guide\" should be made available, illustrating how to install the code and run an example. This can be provided as: A simple README file within the repository in text or markdown. Alternatively the GiHub wiki space associated with the repository can be used. A general paper describing a set of good computing practices that every researcher can adopt, regardless of their current level of computational skill is \"Good enough practices in scientific computing\" .","title":"Minimal requirements"},{"location":"guides/Laboratory/#vivado-installation","text":"A hands-on to install vivado on Linux and Windows platform will be created, however, it is easier to connect through the Laboratory and use university credentials to use vivado.","title":"Vivado installation"},{"location":"guides/Laboratory/#hello-world-program","text":"For each session, there is going to be a simple hello world program, then there are some sample codes, you can try them by your own and based on hands-on tutorials. It is appreciated to discuss them and any other problem you faced within these exercises.","title":"hello world! program"},{"location":"guides/Laboratory/#what-to-submit","text":"The example of how and what to submit is available in the repository, please read the instruction carefully and submit what is expected and try to avoid unnecessary efforts.","title":"What to submit?"},{"location":"guides/accpy/","text":"Acc-Py Information Creating Virtual Environments This is a page copying part of the content of https://wikis.cern.ch/display/ACCPY/Getting+started+with+Acc-Py To access it from home you can use sshuttle . In order to run your own application in the Acc-Py environment, it is likely you will want to install some specific packages which are suited to the task at hand. The most appropriate way to customise your Python environment is to use Python virtual environments. These are essentially a directory in which you have permission to install Python packages using the Python package manager, and which has its own Python executable and associated files. To enable it for the lifetime of your current shell a few key environment variables must be set by sourcing the setup script (from the technical network) source /acc/local/share/python/acc-py/pro/setup.sh Then you can create your virtual environment with a command similar to acc-py venv ~/venv/mypy and you can activate it by source ~/venv/mypy/bin/activate Now that you have a full Python environment with write permission, you can install Python packages into it: python -m pip install pyarrow That's it. Acc-Py Repository Packages and External Tools Recently, GPN functionality Python packages such as pyjapc , cmmnbuild-dep-manager , pjlsa , jpype1 and pytimber have been made installable from the acc-py repo only, which requires to install from inside the CERN GPN, and cannot be fetched from PyPI . However, some python projects need to reconcile using functionality from these packages while also being installable from outside the CERN GPN - to, say, be accessible to collaborators. These packages will find their CI/CD setup failing, and will also be faced with the impossibility of deploying to PyPI . As there are no plans to make packages from the acc-py repository installable from outside the CERN GPN in the foreseeable future, the current workaround is to declare these packages as optional dependencies (by creating an extra in your setup.py or pyproject.toml ), and to gate or mock their import whenever needed. This comes with the caveat that PyPI will only accept the deployment of a package if its dependencies are also registered on PyPI , so the workaround necessitates that the maintainers of pytimber , pjlsa or any python package deployed on the acc-py repository keep a shallow clone of their packages on PyPI . It is also recommended to keep shallow clones for security reasons , as it would be easy for any attacker to register a package under the same name on PyPI containing malicious code. Depending on the version number or the accessibility of the acc-py repository, this malware would then be installed instead of the acc-py package. For further reading, see this interesting article on medium.com and this article about PyPI removing malicious packages . An example of the mocking, as implemented in omc3 , can be seen below: import importlib class CERNNetworkMockPackage : \"\"\" Mock class to raise an error if the desired package functionality is called when the package is not actually installed. Designed for packages installable only from inside the CERN network, that are declared as an extra dependency. \"\"\" def __init__ ( self , name : str ): self . name = name def __getattr__ ( self , item ): raise ImportError ( f \"The ' { self . name } ' package does not seem to be installed but is needed for this function. \" \"Install it with the 'cern' extra dependency, which requires to be on the CERN network and to \" \"install from the acc-py package index. Refer to the documentation for more information.\" ) def cern_network_import ( package : str ): \"\"\" Convenience function to try and import packages only available (and installable) on the CERN network. If installed, the module is returned, otherwise a mock class is returned, which will raise an insightful ``ImportError`` on attempted use. Args: package (str): name of the package to try and import. \"\"\" try : return importlib . import_module ( package ) except ImportError : return CERNNetworkMockPackage ( package ) The usage is then: from your.mock.module import cern_network_import pytimber = cern_network_import ( \"pytimber\" ) db = pytimber . LoggingDB ( source = \"nxcals\" ) # will raise if pytimber not installed","title":"Acc-Py Information"},{"location":"guides/accpy/#acc-py-information","text":"","title":"Acc-Py Information"},{"location":"guides/accpy/#creating-virtual-environments","text":"This is a page copying part of the content of https://wikis.cern.ch/display/ACCPY/Getting+started+with+Acc-Py To access it from home you can use sshuttle . In order to run your own application in the Acc-Py environment, it is likely you will want to install some specific packages which are suited to the task at hand. The most appropriate way to customise your Python environment is to use Python virtual environments. These are essentially a directory in which you have permission to install Python packages using the Python package manager, and which has its own Python executable and associated files. To enable it for the lifetime of your current shell a few key environment variables must be set by sourcing the setup script (from the technical network) source /acc/local/share/python/acc-py/pro/setup.sh Then you can create your virtual environment with a command similar to acc-py venv ~/venv/mypy and you can activate it by source ~/venv/mypy/bin/activate Now that you have a full Python environment with write permission, you can install Python packages into it: python -m pip install pyarrow That's it.","title":"Creating Virtual Environments"},{"location":"guides/accpy/#acc-py-repository-packages-and-external-tools","text":"Recently, GPN functionality Python packages such as pyjapc , cmmnbuild-dep-manager , pjlsa , jpype1 and pytimber have been made installable from the acc-py repo only, which requires to install from inside the CERN GPN, and cannot be fetched from PyPI . However, some python projects need to reconcile using functionality from these packages while also being installable from outside the CERN GPN - to, say, be accessible to collaborators. These packages will find their CI/CD setup failing, and will also be faced with the impossibility of deploying to PyPI . As there are no plans to make packages from the acc-py repository installable from outside the CERN GPN in the foreseeable future, the current workaround is to declare these packages as optional dependencies (by creating an extra in your setup.py or pyproject.toml ), and to gate or mock their import whenever needed. This comes with the caveat that PyPI will only accept the deployment of a package if its dependencies are also registered on PyPI , so the workaround necessitates that the maintainers of pytimber , pjlsa or any python package deployed on the acc-py repository keep a shallow clone of their packages on PyPI . It is also recommended to keep shallow clones for security reasons , as it would be easy for any attacker to register a package under the same name on PyPI containing malicious code. Depending on the version number or the accessibility of the acc-py repository, this malware would then be installed instead of the acc-py package. For further reading, see this interesting article on medium.com and this article about PyPI removing malicious packages . An example of the mocking, as implemented in omc3 , can be seen below: import importlib class CERNNetworkMockPackage : \"\"\" Mock class to raise an error if the desired package functionality is called when the package is not actually installed. Designed for packages installable only from inside the CERN network, that are declared as an extra dependency. \"\"\" def __init__ ( self , name : str ): self . name = name def __getattr__ ( self , item ): raise ImportError ( f \"The ' { self . name } ' package does not seem to be installed but is needed for this function. \" \"Install it with the 'cern' extra dependency, which requires to be on the CERN network and to \" \"install from the acc-py package index. Refer to the documentation for more information.\" ) def cern_network_import ( package : str ): \"\"\" Convenience function to try and import packages only available (and installable) on the CERN network. If installed, the module is returned, otherwise a mock class is returned, which will raise an insightful ``ImportError`` on attempted use. Args: package (str): name of the package to try and import. \"\"\" try : return importlib . import_module ( package ) except ImportError : return CERNNetworkMockPackage ( package ) The usage is then: from your.mock.module import cern_network_import pytimber = cern_network_import ( \"pytimber\" ) db = pytimber . LoggingDB ( source = \"nxcals\" ) # will raise if pytimber not installed","title":"Acc-Py Repository Packages and External Tools"},{"location":"guides/coursework/","text":"Guidelines for coursework project Template for report To submit your report please follow the template available as following: https://www.ieee.org/conferences/publishing/templates.html Note Is a best practice to learn and work with latex if you already did not work with latex try to start it now! There are two types of templates, MS Word and Latex. Both templates are acceptable to be submitted. In any case if the above link didn't work you can directly download them from links below: Latex template Latex Bibliography tutorial MS Word","title":"Guidelines for coursework project"},{"location":"guides/coursework/#guidelines-for-coursework-project","text":"","title":"Guidelines for coursework project"},{"location":"guides/coursework/#template-for-report","text":"To submit your report please follow the template available as following: https://www.ieee.org/conferences/publishing/templates.html Note Is a best practice to learn and work with latex if you already did not work with latex try to start it now! There are two types of templates, MS Word and Latex. Both templates are acceptable to be submitted. In any case if the above link didn't work you can directly download them from links below: Latex template Latex Bibliography tutorial MS Word","title":"Template for report"},{"location":"guides/eos_on_mac/","text":"(from an codiMD page of Ilias, https://codimd.web.cern.ch/s/vorpxehxj ) Using EOS in my Mac To effectively use my mac for LHC data analysis, I need to have access to EOS repositories: my personal one under /eos/home-i some project repositories in /eos/project-l From CERN IT there are two solutions proposed: use the CERNBOX client mount directly the EOS volumes I tried and have both running, below my experience from using them. Using the CERNBOX client The installation is straightforward, you can download the package and install it. Then you can configure the account and add folders to synchronize. pros You can have multiple folders synchronized with your Mac. Can be user folders, projects or other shared folders you have access to with the declared account. For each folder you can select which sub-folders you want to add to the syncronization. The program works well and I've very rarely see it crashing or not being able to do the synchronization. It is smart and can ignore certain types of files - there is an Ignored Files list that you can edit. cons Not reliable synchronization: sometimes takes long to trigger the synchronization, I've observed that sometimes some files are not included in the first synch loop, Very often \"small\" changes to files are not noticed. Eventually the synchronization works but is not immediate. For example editing locally the files using a smart editor (like VsCODE) and run my scripts in SWAN becomes a frustrating process. No dynamic synchronization: As user I have to select which files to synchronize and since my space in EOS can be as big as 1TB while in my Mac I have only 256GBytes, I need to constantly select and update the folders I synchronize. A better solution would have been a dynamic allocation of CERNBOX space in my Mac where files are included or removed upon use, as other programs do. This would also offer an access to the EOS directory structure which presently is limited to the ones selected for synchronization. Using FUSE for direct mount I followed the instructions found in some of the CERN IT pages. The installation is relatively easy: install OSXFUSE from osxfuse.github.io make a directory to be the local mount point for EOS can be either /eos or as in my case /User/ilias/eos then get the kerberos token using: kinit myuser@CERN.CH define the environment variables to EOS: export EOS_MGM_URL = root://eoshome-i.cern.ch or export EOS_MGM_URL = root://eosproject-l.cern.ch finally do the mount : eos fuse mount /Users/ilias/eos . That's it! In case the mount did not work you can restart it doing: killall eosd eos fuse umount /User/ilias/eos and then try mounting it again. pros Straightforward procedure, typically works You don't have to select directories to synchronize Full access to the directory tree Fast reaction to changes - the files become almost immediately available to remote Could access the EOS directories from my editor (vsCODE). cons It seems I have to redo the mount once the kerberos tokens expire I dit not manage to mount both a user directory and a project EOS space. Ilias suggested in the mattermost channel https://mattermost.web.cern.ch/abpcomputing/pl/5p54ieirofdxf8caejuy64emuy to use the following script #!/bin/bash # # Script to initialize and mount the EOS # # (c) Ilias Efthymiopoulos - Feb 2020 # # Arguments : [1] my CERN password # # # -- get access to EOS from previously stored kerberos password kinit -kt ~/.keytab efthymio@CERN.CH export EOS_MGM_URL = root://eoshome-e.cern.ch export EOS_FUSE_MGM_ALIAS = eoshome-e.cern.ch export EOS_FUSE_MOUNTDIR = /Users/iliasefthymiopoulos/eoshome-e eos fuse mount /Users/iliasefthymiopoulos/eoshome-e/ export EOS_MGM_URL = root://eosproject-l.cern.ch export EOS_FUSE_MGM_ALIAS = eosproject-l.cern.ch export EOS_FUSE_MOUNTDIR = /Users/iliasefthymiopoulos/eosproject-l eos fuse mount /Users/iliasefthymiopoulos/eosproject-l # --- define alias for unmount alias umounteos = 'function _umnteos() { killall eosd; eos fuse umount /Users/iliasefthymiopoulos/$1 }; _umnteos' and Joschua proposed to use also a function approach to simplify it function umounteos () { killall eosd ; eos fuse umount /Users/iliasefthymiopoulos/eos $1 } ; alias umounteosall = 'umounteos project-l; umounteos home-e' or even function mounteos () { export EOS_MGM_URL = root://eos $1 .cern.ch export EOS_FUSE_MGM_ALIAS = eos $1 .cern.ch export EOS_FUSE_MOUNTDIR = /Users/iliasefthymiopoulos/eos $1 eos fuse mount /Users/iliasefthymiopoulos/eos $1 } ; alias mounteosall = 'mounteos project-l; mounteos home-e'","title":"Eos on mac"},{"location":"guides/eos_on_mac/#using-eos-in-my-mac","text":"To effectively use my mac for LHC data analysis, I need to have access to EOS repositories: my personal one under /eos/home-i some project repositories in /eos/project-l From CERN IT there are two solutions proposed: use the CERNBOX client mount directly the EOS volumes I tried and have both running, below my experience from using them.","title":"Using EOS in my Mac"},{"location":"guides/eos_on_mac/#using-the-cernbox-client","text":"The installation is straightforward, you can download the package and install it. Then you can configure the account and add folders to synchronize.","title":"Using the CERNBOX client"},{"location":"guides/eos_on_mac/#pros","text":"You can have multiple folders synchronized with your Mac. Can be user folders, projects or other shared folders you have access to with the declared account. For each folder you can select which sub-folders you want to add to the syncronization. The program works well and I've very rarely see it crashing or not being able to do the synchronization. It is smart and can ignore certain types of files - there is an Ignored Files list that you can edit.","title":"pros"},{"location":"guides/eos_on_mac/#cons","text":"Not reliable synchronization: sometimes takes long to trigger the synchronization, I've observed that sometimes some files are not included in the first synch loop, Very often \"small\" changes to files are not noticed. Eventually the synchronization works but is not immediate. For example editing locally the files using a smart editor (like VsCODE) and run my scripts in SWAN becomes a frustrating process. No dynamic synchronization: As user I have to select which files to synchronize and since my space in EOS can be as big as 1TB while in my Mac I have only 256GBytes, I need to constantly select and update the folders I synchronize. A better solution would have been a dynamic allocation of CERNBOX space in my Mac where files are included or removed upon use, as other programs do. This would also offer an access to the EOS directory structure which presently is limited to the ones selected for synchronization.","title":"cons"},{"location":"guides/eos_on_mac/#using-fuse-for-direct-mount","text":"I followed the instructions found in some of the CERN IT pages. The installation is relatively easy: install OSXFUSE from osxfuse.github.io make a directory to be the local mount point for EOS can be either /eos or as in my case /User/ilias/eos then get the kerberos token using: kinit myuser@CERN.CH define the environment variables to EOS: export EOS_MGM_URL = root://eoshome-i.cern.ch or export EOS_MGM_URL = root://eosproject-l.cern.ch finally do the mount : eos fuse mount /Users/ilias/eos . That's it! In case the mount did not work you can restart it doing: killall eosd eos fuse umount /User/ilias/eos and then try mounting it again.","title":"Using FUSE for direct mount"},{"location":"guides/eos_on_mac/#pros_1","text":"Straightforward procedure, typically works You don't have to select directories to synchronize Full access to the directory tree Fast reaction to changes - the files become almost immediately available to remote Could access the EOS directories from my editor (vsCODE).","title":"pros"},{"location":"guides/eos_on_mac/#cons_1","text":"It seems I have to redo the mount once the kerberos tokens expire I dit not manage to mount both a user directory and a project EOS space. Ilias suggested in the mattermost channel https://mattermost.web.cern.ch/abpcomputing/pl/5p54ieirofdxf8caejuy64emuy to use the following script #!/bin/bash # # Script to initialize and mount the EOS # # (c) Ilias Efthymiopoulos - Feb 2020 # # Arguments : [1] my CERN password # # # -- get access to EOS from previously stored kerberos password kinit -kt ~/.keytab efthymio@CERN.CH export EOS_MGM_URL = root://eoshome-e.cern.ch export EOS_FUSE_MGM_ALIAS = eoshome-e.cern.ch export EOS_FUSE_MOUNTDIR = /Users/iliasefthymiopoulos/eoshome-e eos fuse mount /Users/iliasefthymiopoulos/eoshome-e/ export EOS_MGM_URL = root://eosproject-l.cern.ch export EOS_FUSE_MGM_ALIAS = eosproject-l.cern.ch export EOS_FUSE_MOUNTDIR = /Users/iliasefthymiopoulos/eosproject-l eos fuse mount /Users/iliasefthymiopoulos/eosproject-l # --- define alias for unmount alias umounteos = 'function _umnteos() { killall eosd; eos fuse umount /Users/iliasefthymiopoulos/$1 }; _umnteos' and Joschua proposed to use also a function approach to simplify it function umounteos () { killall eosd ; eos fuse umount /Users/iliasefthymiopoulos/eos $1 } ; alias umounteosall = 'umounteos project-l; umounteos home-e' or even function mounteos () { export EOS_MGM_URL = root://eos $1 .cern.ch export EOS_FUSE_MGM_ALIAS = eos $1 .cern.ch export EOS_FUSE_MOUNTDIR = /Users/iliasefthymiopoulos/eos $1 eos fuse mount /Users/iliasefthymiopoulos/eos $1 } ; alias mounteosall = 'mounteos project-l; mounteos home-e'","title":"cons"},{"location":"guides/exam/","text":"Exam guide tasks","title":"Exam"},{"location":"guides/exam/#exam-guide-tasks","text":"","title":"Exam guide tasks"},{"location":"guides/finalproject/","text":"Guidelines for Laboratory projects and your Final Project Minimal requirements Software projects should comply with the following basic guidelines: The source code should be hosted in an online git repository . Our course git repository is: ( https://github.com/System-Design-FPGA ) You can clone the repository on your file system, please don't forgot to pull and keep it updated, The other solution is to use our github repository by web browser, once you need a file you can copy it from the web and copy it in your file system. Since viavdo project will create many small files, it is not a good practice to keep all viavdo project files on a git repo. One or more \"ready-to-run\" examples should be made available in the repository, illustrating the main features of the code. Please make sure that the example are still updated and working when making modifications to the code! A simple \"Getting started guide\" should be made available, illustrating how to install the code and run an example. This can be provided as: A simple README file within the repository in text or markdown. Alternatively the GiHub wiki space associated with the repository can be used. A general paper describing a set of good computing practices that every researcher can adopt, regardless of their current level of computational skill is \"Good enough practices in scientific computing\" . Vivado installation A hands-on to install vivado on Linux and Windows platform will be created, however, it is easier to connect through the Laboratory and use university credentials to use vivado. hello world! program For each session, there is going to be a simple hello world program, then there are some sample codes, you can try them by your own and based on hands-on tutorials. It is appreciated to discuss them and any other problem you faced within these exercises. What to submit? The example of how and what to submit is available in the repository, please read the instruction carefully and submit what is expected and try to avoid unnecessary efforts.","title":"Guidelines for Laboratory projects and your Final Project"},{"location":"guides/finalproject/#guidelines-for-laboratory-projects-and-your-final-project","text":"","title":"Guidelines for Laboratory projects and your Final Project"},{"location":"guides/finalproject/#minimal-requirements","text":"Software projects should comply with the following basic guidelines: The source code should be hosted in an online git repository . Our course git repository is: ( https://github.com/System-Design-FPGA ) You can clone the repository on your file system, please don't forgot to pull and keep it updated, The other solution is to use our github repository by web browser, once you need a file you can copy it from the web and copy it in your file system. Since viavdo project will create many small files, it is not a good practice to keep all viavdo project files on a git repo. One or more \"ready-to-run\" examples should be made available in the repository, illustrating the main features of the code. Please make sure that the example are still updated and working when making modifications to the code! A simple \"Getting started guide\" should be made available, illustrating how to install the code and run an example. This can be provided as: A simple README file within the repository in text or markdown. Alternatively the GiHub wiki space associated with the repository can be used. A general paper describing a set of good computing practices that every researcher can adopt, regardless of their current level of computational skill is \"Good enough practices in scientific computing\" .","title":"Minimal requirements"},{"location":"guides/finalproject/#vivado-installation","text":"A hands-on to install vivado on Linux and Windows platform will be created, however, it is easier to connect through the Laboratory and use university credentials to use vivado.","title":"Vivado installation"},{"location":"guides/finalproject/#hello-world-program","text":"For each session, there is going to be a simple hello world program, then there are some sample codes, you can try them by your own and based on hands-on tutorials. It is appreciated to discuss them and any other problem you faced within these exercises.","title":"hello world! program"},{"location":"guides/finalproject/#what-to-submit","text":"The example of how and what to submit is available in the repository, please read the instruction carefully and submit what is expected and try to avoid unnecessary efforts.","title":"What to submit?"},{"location":"guides/gitinfo/","text":"","title":"Gitinfo"},{"location":"guides/htcondor/","text":"Submitting Jobs to HTCondor from a local Machine The recommended way of using HTCondor is to submit jobs by logging to lxplus.cern.ch. It is also possible to configure your own computer to manage HTCondor jobs, as described in this guide. These notes refer to Ubuntu 16.04 LTS, 18.04 LTS and 20.04 LTS and includes possible caveats. If you have a different Linux distribution, steps might be the same, but syntax may change. Sudo rights are needed. As pre-requisite, you will need to install a kerberos client on your desktop; afterwards, you can proceed with the installation of HTCondor Install kerberos install user and developer packages and add lxplus credential components (when asked, default realm is CERN.CH ): sudo apt install krb5-user libkrb5-dev libauthen-krb5-perl scp $USERNAME @lxplus.cern.ch:/usr/bin/batch_krb5_credential . chmod +x batch_krb5_credential sudo mv batch_krb5_credential /usr/bin/ scp $USERNAME @lxplus.cern.ch:/etc/ngauth_batch_crypt_pub.pem . sudo mv ngauth_batch_crypt_pub.pem /etc/ scp $USERNAME @lxplus.cern.ch:/etc/krb5.conf.no_rdns . sudo mv krb5.conf.no_rdns /etc/krb5.conf.no_rdns scp $USERNAME @lxplus.cern.ch:/etc/sysconfig/ngbauth-submit . sudo mkdir /etc/sysconfig/ sudo mv ngbauth-submit /etc/sysconfig/ Confirm installation Before this step make sure you have valid credentials already (i.e. run kinit ). Then check that the kerberos components are properly installed and set-up (the script will tell you the missing perl packages): /usr/bin/batch_krb5_credential There should be an output like: -----BEGIN NGAUTH COMPOSITE----- # LOTS OF LINES OF YOUR KEY -----END NGAUTH COMPOSITE----- and nothing else (i.e. no missing files or errors). Debugging the kerberos installation if the last step does not deliver the desired output, /usr/bin/batch_krb5_credential might have to be modified. Some things can be tried: change the line my $principalName = \"ngauth/SOMESERVER\" ; into my $principalName = \"ngauth/ngauth.cern.ch\" ; install missing perl components: perl -MCPAN -e 'install Authen::Krb5' on Ubuntu 20.04 neither of these steps helped, as the Authen:Krb5 package was not available. Try getting a new version of batch_krb5_credential directly from lxplus8 : scp $USERNAME @lxplus8.cern.ch:/usr/bin/batch_krb5_credential . chmod +x batch_krb5_credential sudo mv batch_krb5_credential /usr/bin/ or try manual fix of Authen:Krb5 issue by replacing the lines: my $newCreds = Authen::Krb5::cc_resolve ( \"FILE:\" . $tgt_fn ) ; $newCreds ->initialize ( $credCache ->get_principal ()) ; Authen::Krb5::cc_copy_creds ( $credCache , $newCreds ) ; with copy ( $tgt , $tgt_fn ) ; Install HTCondor On Ubuntu 18.04+ it is usually enough to install condor from the packaged version Ubuntu 20.04 sudo apt update sudo apt install htcondor Ubuntu 18.04 sudo apt-get update sudo apt-get install condor If you need a more recent version, or in case of Ubuntu 16.04, use the resources on the web-page of HTCondor . Debugging the HTCondor installation It may happen that at sudo apt-get update , you get the error message: N: Skipping acquire of configured file 'contrib/binary-i386/Packages' as repository 'http://research.cs.wisc.edu/htcondor/ubuntu/stable trusty InRelease' doesn 't support architecture ' i386 ' In case your system is actually 64bit, a common solution is to limit the research of the package distro to just 64 bit by introducing the [arch=amd64] in the list of sources (in /etc/apt/sources.list ), e.g. deb [ arch = amd64 ] http://research.cs.wisc.edu/htcondor/ubuntu/stable/ trusty contrib Configure HTCondor create the config file /etc/condor/config.d/10-local.config . Please set as scheduler ( SCHEDD_HOST ) the default one you get on lxplus , e.g. in your condor_q output. You can also find it out by running (on lxplus ): condor_config_val SCHEDD_HOST An example content is provided here: CONDOR_HOST = tweetybird03.cern.ch, tweetybird04.cern.ch COLLECTOR_HOST = tweetybird03.cern.ch, tweetybird04.cern.ch SCHEDD_HOST = bigbirdXX.cern.ch SCHEDD_NAME = $( SCHEDD_HOST ) SEC_CLIENT_AUTHENTICATION_METHODS = KERBEROS SEC_CREDENTIAL_PRODUCER = /usr/bin/batch_krb5_credential CREDD_HOST = $( SCHEDD_HOST ) FILESYSTEM_DOMAIN = cern.ch UID_DOMAIN = cern.ch restart HTCondor : /etc/init.d/condor restart Debugging the HTCondor configuration Useful: The full configuration can be checked by condor_config_val -dump If you have connection problems when running condor_q or condor_status , you might want to check your NETWORK_INTERFACE . condor_config_val NETWORK_INTERFACE In some cases it might be set to 127.0.0.1 or similar. Yet it should be set to * . If this is not the case, simply add the appropriate line at the end of your configuration file (from above): NETWORK_INTERFACE = * Don't forget to restart HTCondor . Pay attention to the couple COLLECTOR_HOST and SCHEDD_HOST , as, depending on the collector, you may be able to reach only a sub-set of the scheduler. To get the whole lists, please login to lxplus.cern.ch and type: schedulers condor_status -sched collectors condor_status -collector","title":"Submitting Jobs to HTCondor from a local Machine"},{"location":"guides/htcondor/#submitting-jobs-to-htcondor-from-a-local-machine","text":"The recommended way of using HTCondor is to submit jobs by logging to lxplus.cern.ch. It is also possible to configure your own computer to manage HTCondor jobs, as described in this guide. These notes refer to Ubuntu 16.04 LTS, 18.04 LTS and 20.04 LTS and includes possible caveats. If you have a different Linux distribution, steps might be the same, but syntax may change. Sudo rights are needed. As pre-requisite, you will need to install a kerberos client on your desktop; afterwards, you can proceed with the installation of HTCondor","title":"Submitting Jobs to HTCondor from a local Machine"},{"location":"guides/htcondor/#install-kerberos","text":"install user and developer packages and add lxplus credential components (when asked, default realm is CERN.CH ): sudo apt install krb5-user libkrb5-dev libauthen-krb5-perl scp $USERNAME @lxplus.cern.ch:/usr/bin/batch_krb5_credential . chmod +x batch_krb5_credential sudo mv batch_krb5_credential /usr/bin/ scp $USERNAME @lxplus.cern.ch:/etc/ngauth_batch_crypt_pub.pem . sudo mv ngauth_batch_crypt_pub.pem /etc/ scp $USERNAME @lxplus.cern.ch:/etc/krb5.conf.no_rdns . sudo mv krb5.conf.no_rdns /etc/krb5.conf.no_rdns scp $USERNAME @lxplus.cern.ch:/etc/sysconfig/ngbauth-submit . sudo mkdir /etc/sysconfig/ sudo mv ngbauth-submit /etc/sysconfig/","title":"Install kerberos"},{"location":"guides/htcondor/#confirm-installation","text":"Before this step make sure you have valid credentials already (i.e. run kinit ). Then check that the kerberos components are properly installed and set-up (the script will tell you the missing perl packages): /usr/bin/batch_krb5_credential There should be an output like: -----BEGIN NGAUTH COMPOSITE----- # LOTS OF LINES OF YOUR KEY -----END NGAUTH COMPOSITE----- and nothing else (i.e. no missing files or errors).","title":"Confirm installation"},{"location":"guides/htcondor/#debugging-the-kerberos-installation","text":"if the last step does not deliver the desired output, /usr/bin/batch_krb5_credential might have to be modified. Some things can be tried: change the line my $principalName = \"ngauth/SOMESERVER\" ; into my $principalName = \"ngauth/ngauth.cern.ch\" ; install missing perl components: perl -MCPAN -e 'install Authen::Krb5' on Ubuntu 20.04 neither of these steps helped, as the Authen:Krb5 package was not available. Try getting a new version of batch_krb5_credential directly from lxplus8 : scp $USERNAME @lxplus8.cern.ch:/usr/bin/batch_krb5_credential . chmod +x batch_krb5_credential sudo mv batch_krb5_credential /usr/bin/ or try manual fix of Authen:Krb5 issue by replacing the lines: my $newCreds = Authen::Krb5::cc_resolve ( \"FILE:\" . $tgt_fn ) ; $newCreds ->initialize ( $credCache ->get_principal ()) ; Authen::Krb5::cc_copy_creds ( $credCache , $newCreds ) ; with copy ( $tgt , $tgt_fn ) ;","title":"Debugging the kerberos installation"},{"location":"guides/htcondor/#install-htcondor","text":"On Ubuntu 18.04+ it is usually enough to install condor from the packaged version Ubuntu 20.04 sudo apt update sudo apt install htcondor Ubuntu 18.04 sudo apt-get update sudo apt-get install condor If you need a more recent version, or in case of Ubuntu 16.04, use the resources on the web-page of HTCondor .","title":"Install HTCondor"},{"location":"guides/htcondor/#debugging-the-htcondor-installation","text":"It may happen that at sudo apt-get update , you get the error message: N: Skipping acquire of configured file 'contrib/binary-i386/Packages' as repository 'http://research.cs.wisc.edu/htcondor/ubuntu/stable trusty InRelease' doesn 't support architecture ' i386 ' In case your system is actually 64bit, a common solution is to limit the research of the package distro to just 64 bit by introducing the [arch=amd64] in the list of sources (in /etc/apt/sources.list ), e.g. deb [ arch = amd64 ] http://research.cs.wisc.edu/htcondor/ubuntu/stable/ trusty contrib","title":"Debugging the HTCondor installation"},{"location":"guides/htcondor/#configure-htcondor","text":"create the config file /etc/condor/config.d/10-local.config . Please set as scheduler ( SCHEDD_HOST ) the default one you get on lxplus , e.g. in your condor_q output. You can also find it out by running (on lxplus ): condor_config_val SCHEDD_HOST An example content is provided here: CONDOR_HOST = tweetybird03.cern.ch, tweetybird04.cern.ch COLLECTOR_HOST = tweetybird03.cern.ch, tweetybird04.cern.ch SCHEDD_HOST = bigbirdXX.cern.ch SCHEDD_NAME = $( SCHEDD_HOST ) SEC_CLIENT_AUTHENTICATION_METHODS = KERBEROS SEC_CREDENTIAL_PRODUCER = /usr/bin/batch_krb5_credential CREDD_HOST = $( SCHEDD_HOST ) FILESYSTEM_DOMAIN = cern.ch UID_DOMAIN = cern.ch restart HTCondor : /etc/init.d/condor restart","title":"Configure HTCondor"},{"location":"guides/htcondor/#debugging-the-htcondor-configuration","text":"Useful: The full configuration can be checked by condor_config_val -dump If you have connection problems when running condor_q or condor_status , you might want to check your NETWORK_INTERFACE . condor_config_val NETWORK_INTERFACE In some cases it might be set to 127.0.0.1 or similar. Yet it should be set to * . If this is not the case, simply add the appropriate line at the end of your configuration file (from above): NETWORK_INTERFACE = * Don't forget to restart HTCondor . Pay attention to the couple COLLECTOR_HOST and SCHEDD_HOST , as, depending on the collector, you may be able to reach only a sub-set of the scheduler. To get the whole lists, please login to lxplus.cern.ch and type: schedulers condor_status -sched collectors condor_status -collector","title":"Debugging the HTCondor configuration"},{"location":"guides/references/","text":"References","title":"Text Books"},{"location":"guides/references/#references","text":"","title":"References"},{"location":"guides/slides/","text":"Slides Slides and other materials given during the course will be uploaded here, Please send me your feedback/comments if you found something interesting to mention in the materials.","title":"Slides"},{"location":"guides/slides/#slides","text":"Slides and other materials given during the course will be uploaded here, Please send me your feedback/comments if you found something interesting to mention in the materials.","title":"Slides"},{"location":"guides/spack/","text":"","title":"Spack"},{"location":"guides/useful/","text":"Some useful links and materials","title":"Other References"},{"location":"guides/useful/#some-useful-links-and-materials","text":"","title":"Some useful links and materials"},{"location":"guides/vivado/","text":"Create and build a vivado project from scratch","title":"Create a vivado project"},{"location":"guides/vivado/#create-and-build-a-vivado-project-from-scratch","text":"","title":"Create and build a vivado project from scratch"},{"location":"guides/zedboard/","text":"Sphinx - Read the Docs documentation G. Iadarola and R. De Maria For python packages, we found particularly convenient to use sphinx to document the code and to host the documentation on Read the Docs . The API documentation can be generated by sphinx from the docstrings in the code. The main steps to create a sphinx-rtd documentation are the following: Install sphinx and rtd-theme Install sphinx and the rdt-theme: $ pip install sphinx $ pip install sphinx-rtd-theme Start documentation pages The code to be documented needs to be hosted in a GitHub or GitLab account Add requirements.txt to your repository. In order to get documentation from the docstrings, Read the Docs will need to import your python package in a python virtual environment. It will install the dependencies based on the requirements file, which simply consists in the list of packages that need to be installed. For example: numpy scipy Create a docs folder inside your repository and populate it, for example with these example files . Edit conf.py to add to the python path the folder containing the python package to be documented (sphinx needs to import the package to access the docstrings).: import sys sys . path . insert ( 0 , os . path . abspath ( '../' )) # path relative to conf.py Many other customizations are possible using conf.py . Compile your project locally by typing: $ make html This generates a simple preview of your documentation (less advanced than rtd) in the folder _build . You can open _build/index.html in your browser to navigate the preview. On readthedocs.org Create an account on https://readthedocs.org (the easiest is to link it to your github account), it you don't already have one. Add a project linked to your GitHub/GitLab repository. Add requirements file by specifying requirements.txt under Admin > Advanced Settings -> Requirements file. Edit your documentation A lot of information about the RST format used in the documentation and about the sphinx doc generation can be found in these tutorials: https://sphinx-rtd-tutorial.readthedocs.io/en/latest/index.html https://sphinx-rtd-tutorial.readthedocs.io An example of documentation from our team can be found at: https://xsuite.readthedocs.io/en/latest/","title":"Deploy and run on a Zedboard"},{"location":"guides/zedboard/#sphinx-read-the-docs-documentation","text":"G. Iadarola and R. De Maria For python packages, we found particularly convenient to use sphinx to document the code and to host the documentation on Read the Docs . The API documentation can be generated by sphinx from the docstrings in the code. The main steps to create a sphinx-rtd documentation are the following:","title":"Sphinx - Read the Docs documentation"},{"location":"guides/zedboard/#install-sphinx-and-rtd-theme","text":"Install sphinx and the rdt-theme: $ pip install sphinx $ pip install sphinx-rtd-theme","title":"Install sphinx and rtd-theme"},{"location":"guides/zedboard/#start-documentation-pages","text":"The code to be documented needs to be hosted in a GitHub or GitLab account Add requirements.txt to your repository. In order to get documentation from the docstrings, Read the Docs will need to import your python package in a python virtual environment. It will install the dependencies based on the requirements file, which simply consists in the list of packages that need to be installed. For example: numpy scipy Create a docs folder inside your repository and populate it, for example with these example files . Edit conf.py to add to the python path the folder containing the python package to be documented (sphinx needs to import the package to access the docstrings).: import sys sys . path . insert ( 0 , os . path . abspath ( '../' )) # path relative to conf.py Many other customizations are possible using conf.py . Compile your project locally by typing: $ make html This generates a simple preview of your documentation (less advanced than rtd) in the folder _build . You can open _build/index.html in your browser to navigate the preview.","title":"Start documentation pages"},{"location":"guides/zedboard/#on-readthedocsorg","text":"Create an account on https://readthedocs.org (the easiest is to link it to your github account), it you don't already have one. Add a project linked to your GitHub/GitLab repository. Add requirements file by specifying requirements.txt under Admin > Advanced Settings -> Requirements file.","title":"On readthedocs.org"},{"location":"guides/zedboard/#edit-your-documentation","text":"A lot of information about the RST format used in the documentation and about the sphinx doc generation can be found in these tutorials: https://sphinx-rtd-tutorial.readthedocs.io/en/latest/index.html https://sphinx-rtd-tutorial.readthedocs.io An example of documentation from our team can be found at: https://xsuite.readthedocs.io/en/latest/","title":"Edit your documentation"}]}