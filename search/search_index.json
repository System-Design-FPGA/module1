{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"System Design usign FPGA Course structure Module 1 Part 1 All the course material will be provided in power point slides format. Course Instructor: Amin Sahebi Office hours: Thursday 10:00 - 13:00 Email: sahebi.amin@gmail.com In any case you need furthur explaination or material Shoot Me an Email. Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Formatting can also be applied to blocks by putting the opening and closing tags on separate lines and adding new lines between the tags and the content. Part 2 Course Organization Lorem ipsum dolor sit amet, consectetur adipiscing elit Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Evaluation some text and [here is possible to download the file in PDF][1] download this Timetable References Laboratory materials","title":"Home"},{"location":"#system-design-usign-fpga","text":"","title":"System Design usign FPGA"},{"location":"#course-structure","text":"","title":"Course structure"},{"location":"#module-1","text":"","title":"Module 1"},{"location":"#part-1","text":"All the course material will be provided in power point slides format. Course Instructor: Amin Sahebi Office hours: Thursday 10:00 - 13:00 Email: sahebi.amin@gmail.com In any case you need furthur explaination or material Shoot Me an Email. Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Formatting can also be applied to blocks by putting the opening and closing tags on separate lines and adding new lines between the tags and the content.","title":"Part 1"},{"location":"#part-2","text":"","title":"Part 2"},{"location":"#course-organization","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque","title":"Course Organization"},{"location":"#evaluation","text":"some text and [here is possible to download the file in PDF][1] download this","title":"Evaluation"},{"location":"#timetable","text":"","title":"Timetable"},{"location":"#references","text":"","title":"References"},{"location":"#laboratory-materials","text":"","title":"Laboratory materials"},{"location":"abpcp/","text":"Accelerator and Beam Physics Computing Panel Mandate The Accelerator and Beam Physics Computing Panel (ABP-CP) is responsible for the software tools for accelerator and beam physics developed in the BE-ABP group at CERN . It defines the roadmap and provides recommendations for strategic software and hardware choices, in order to fulfill the needs related to: the operation of CERN accelerators; upgrade plans (HL-LHC); design studies for future accelerators. The ABP-CP follows up the development and maintenance of the tools, ensuring an appropriate coordination and prioritization of the activities. It identifies the needs for hardware resources (e.g. batch computing, HPC clusters, GPUs) and coordinates the exploitation of the available computing facilities It liaises with the IT department and external computing centers, concerning the hardware resources and services, and with the controls and operations groups, concerning tools for machine studies and follow-up (e.g. operational displays, data analytics, machine learning). Members Giovanni Iadarola (chair) Xavier Buffat Roderik Bruce Riccardo De Maria Laurent Deniau John Farmer Davide Gamba Jean-Baptiste Lallement Andrea Latina Jacques Lettry Lotta Mether Nicolas Mounet Tobias Persson Guido Sterbini Frederik Van Der Veken","title":"Accelerator and Beam Physics Computing Panel"},{"location":"abpcp/#accelerator-and-beam-physics-computing-panel","text":"","title":"Accelerator and Beam Physics Computing Panel"},{"location":"abpcp/#mandate","text":"The Accelerator and Beam Physics Computing Panel (ABP-CP) is responsible for the software tools for accelerator and beam physics developed in the BE-ABP group at CERN . It defines the roadmap and provides recommendations for strategic software and hardware choices, in order to fulfill the needs related to: the operation of CERN accelerators; upgrade plans (HL-LHC); design studies for future accelerators. The ABP-CP follows up the development and maintenance of the tools, ensuring an appropriate coordination and prioritization of the activities. It identifies the needs for hardware resources (e.g. batch computing, HPC clusters, GPUs) and coordinates the exploitation of the available computing facilities It liaises with the IT department and external computing centers, concerning the hardware resources and services, and with the controls and operations groups, concerning tools for machine studies and follow-up (e.g. operational displays, data analytics, machine learning).","title":"Mandate"},{"location":"abpcp/#members","text":"Giovanni Iadarola (chair) Xavier Buffat Roderik Bruce Riccardo De Maria Laurent Deniau John Farmer Davide Gamba Jean-Baptiste Lallement Andrea Latina Jacques Lettry Lotta Mether Nicolas Mounet Tobias Persson Guido Sterbini Frederik Van Der Veken","title":"Members"},{"location":"codespecific_activities/","text":"Code-specific objectives and activities MAD-X Involved persons: Tobias, Laurent Improve support for misaligned and overlapping elements Support cpymad for python workflows Improve save/reload capabilities (full mad-x state) Milestones and timeline Timescale for objectives mentioned above: 2021 Pending bug-fixes (required for ABP activities) Calculation of crabcavity R and T matrix not correct in MADX and PTC. Required to include crab cavities in the HL-LHC sequence! Some functionality duplication between crabcavity and RFmultipole. We might want to to port all the functionality in RFMultipole and call it from CrabCavity for backward compatibility. Related GitHub issue here Current master segfaults on a long hl-lhc script when compiled with 'make' Related GitHub issue here MAD-NG Involved persons: Laurent Focus will be on building know how and develop tools for non-linear normal forms analysis and advanced tracking maps (see recommendations of MAD-NG review ). The long-term goal is to match the capabilities and performance of PTC and possibly do better. Provide python interface for integration with other ABP and operation tools (LSA, pyheadtail, sixtracklib, pymask) Milestones and timeline See presentation at MAD-NG forum Tracking tools Xtrack [NAME IS TEMPORARY] Involved people: Riccardo, Martin, Kostas, Frederik, Tobias, Gianni, Roderik Based on the sixtracklib experience, build a modular library (Xtrack) that performs tracking on CPUs and GPUs Requirements: Possibility of working in combination with other tools (PyHEADTAIL, PyPIC) for collective effects studies (instabilities, space-charge, e-cloud) Should be able to replace Sixtrack for conventional single-particle studies Run efficiently with HTCondor and BOINC (requires numerical reproducibility) Adopt design choices that facilitate future maintenance and development, as well as adaptation to new technologies (e.g. GPU standards) -> keep the code slim Allow the integration of advanced collimation capabilities (advanced aperture models, scattering, with FLUKA, tracking of fragments) Be extendable to model lepton rings. Design choices To keep a slim and flexible code, we choose to manage the memory and the simulation through python. We will use pyopencl to build a first version, then possibly extend to cupy and numba-cuda. Some modifications in PyPIC and PyHEADTAIL might be necessary. Description of the particles ensemble might be moved to a separate library ( Xpart ) and shared with PyHEADTAIL. Short-term tasks Build a first skeleton version. Collect a set of examples representative of all the use-cases, to make sure that the design covers all needs: Examples for DA and lifetime studies already available We need a set of examples illustrating che collimation us-cases (using the SixTrack) We need an example for the space charge studies (sixtracklib, pyheadtail, pypic) Milestones and timeline Skeleton code with main features Feb 2021 Collection of use-cases (e.g. space-charge, collimation) to be done in Q1-Q2 2021 By end of 2021 ABP-INC should be able to migrate their studies: requires all features available in sixtracklib 1.0, interface with PyPIC, dynamic elements (noise) Other features (aiming at matching SixTrack capabilities that we need to retain): 2022 For collimation features, the timescale will be defined only after reviewing all the use cases (might need to go beyond 2022, depending on allocated resources). Xpart [NAME IS TEMPORARY] Involved persons: Guido, Foteini, Kostas, Lotta Concentrate in a single library tools for the generation of particle ensembles. Capabilities are presently scattered over: SixDesk pysixdesk sixtrack (recent work by Riccardo, Tobias and co.) pysixtrack (Kostas' code) PyHEADTAIL (advanced longitudinal matching) This library should define a general and flexible particle description that can be imported and used in PyHEADTAIL, in the new tracking library and for other usages. Probably we can use PyHEADTAIL's Particles class as a starting point. Remember that masked access needs to be available for bunch slicing. Extraction of statistical properties (average position, r.m.s. sizes, optical functions, emittances) should be implemented (ported from PyHEADTAIL). Milestones and timeline Timescale for objectives mentioned above: 2021 SixDesk Involved persons: Guido, Frederik Restructure the tool to ease maintenance and further development (re-use as much as possible the pysixdesk code). Integrate new developments (pymask, Xtrack) Improve usage flexibility. Explore the usage of containers ( singularity or docker ) on BOINC for more flexible simulations (e.g. including python) Move generation of particles distribution to a separate library usable for other applications (synergy with PyHEADTAIL). Milestones and timeline Timescale for objectives mentioned above: 2021-2022 Updates An example with some python helpers (prepared by Sofia) can be found here A possible functionality breakdown can be found here , together with some notes. mask / pymask Involved persons: Guido, Sofia Kostoglou, NLD fellow (with Massimo) Extend to ions Strengthen automatic and interactive checks on error-routines, optics, knobs, etc. Validate beam 4 implementation. Plan transition to pymask for all applications. Milestones and timeline Timescale for objectives mentioned above: 2021-2022 Ion extension by Spring Validation of beam 4 and first set of checks by end-2021 Pending bug-fixes (required for ABP activities) Presently not working with machine imperfections! (Frederik is on it) Tools for collective effects COMBI Involved persons: Xavier, Sondre Port to python the top-level layer of COMBI that manages the interaction schedule, in order to profit from other libraries having a python interface (PyHEADTAIL, Xtrack) Build a \u201dproof-of-principle\u201d code to test and validate the main functionalities (e.g. asynchronous MPI calls) Then move to a well structured python library (reusing as much as possible modules from PyHEADTAIL) Expose to python features of COMBI that are not already available in other python packages (e.g. BTF, noise modeling). Milestones and timeline Timescale for objectives mentioned above: 2021-2022 \u201dproof-of-principle\u201d code with main functionalities in 2021 PyHEADTAIL Involved persons: Lotta Build a solid set of examples and tests covering the entire library Improve documentation, introduce a simple \u201cGetting started guide\u201d Finalize merge into master of the coupled-bunch branch Generation and description of particles ensembles should be moved to a separated library Xpart (shared with the Xtrack tracking library) Integrate with Xtrack library for advanced non-linear tracking. Introduce multithread CPU parallelization (numba, or cython), required for performance in COMBI-like simulations Review, document and update GPU features (need to stay compatible with Xtrack ). Integrate coasting beam capabilities (development from Nicolo'). Milestones and timeline Timescale for objectives mentioned above: 2021-2022 merge of coupled-bunch branch, example set, and documentation review to be done in 2021. ESRF would like to use the coupled bunch mode in python 3 (needs merged version). Tentatively this should be possible in June. PyPIC (becomes Xfields) Involved persons: Gianni Restructure the library to better integrate CPU-based FD solvers and GPU-based FFT solvers. Foresee combined usage with Xtrack for space-charge simulations. Move to standard python-package structure (pip installable). Milestones and timeline Timescale for objectives mentioned above: 2021 Design ideas and constraints Details here: https://github.com/giadarol/PyPIC/issues/1 Updates Development being done in this GitHub branch: https://github.com/giadarol/PyPIC/tree/reorganize Started sketching the structure Space-charge equations collected at: https://github.com/giadarol/PyPIC/tree/reorganize/doc PyECLOUD, PyPARIS Involved persons: Gianni, Lotta Move to standard python-package structure (pip installable). Milestones and timeline Timescale for objectives mentioned above: 2021 DELPHI Involved persons: Nicolas, Gianni, Sebastien, Sofia Johannesson Restructure and simplify the code (more flexible interface, more pythonic, easier to bypass convergence tests, etc.), Move to standard python-package structure (pip installable) Integrate different matrix calculation from temporary eDELPHI package (to handle e-cloud, detuning impedances, short wakes) Improve integration with PySSD Milestones and timeline Timescale for objectives mentioned above: 2021 Design constraints and first ideas (2020-01-15) Requirements: We need the possibility of enlarging the matrix for convergence checks We should be able to reuse quantities when possible (e.g. the full matrix for \"intensity\" scans or different synchrotron tunes, or parts for the calculation). Some ingredients break the reusability, the code should check automatically. Design ideas: Make a base \"coupling matrix\" class with the methods that are shared (e.g. compute_complex_tune_shifts(rescale_factors) ). Such a base class will depend on the beam distribution and on the chosen base functions but not on the description of the collective effects forces. Then we make children classes with specialized implementations for the different descriptions of the coherent forces: impedance sum from DELPHI, sine responses from eDELPHI, but also Dirac-delta response, specialized versions for fast-decaying wakes and resonators. We should be able to combine matrices. We could try to express all cases (or most cases) in general form like: $$ {\\Large M_{l,m,l',m'} = K_\\text{coh}\\sum_n A_n R_{l,m,n} \\tilde{R}_{l',m',n}} $$ The different child classes will have different ways of computing \\Large A_n \\Large A_n (which is the impedance in the case of Delphi), \\Large R_{l,m,n} \\Large R_{l,m,n} , \\Large \\tilde{R}_{l',m',n} \\Large \\tilde{R}_{l',m',n} . We would keep these quantities in memory to test different matrix truncations... Using python memorizers might help, but we would lose the possibility of working with numpy arrays, which could impact performance. Updates Working in in https://gitlab.cern.ch/IRIS/DELPHI/-/tree/merged_eDELPHI The Harmonic Response Matrix from eDELPHI has been ported and tested. Need to move generic functionalities to base class (done to some extent). Need to separate the part coming from detuning with longitudinal amplitude into a separate class (done) Implement matrix extension for convergence checks. n_l , n_m , n_l_pos could be properties At that point we should review the conventions (e.g. Qs or Q_s?, do we use f or omega, tunes... -> we should be consistent across the code). Then move to impedance, damper etc. Questions: - Maybe move de parameters in a dictionary and not have them as lose members of the object. Easier to make copies of objects. - We need a slim physics manual for notations and conventions. Plan (16/06/2021) Detuning extension (Gianni) \u2013 end of August (can be done in parallel or later than the second point) Make some separate tests on impedance sum - check best strategy time wise (Nicolas) \u2013 end of August If FactorizedMatrix is the way to go, generalize its implementation with functions of \u201cm\u201d for R and R_tilde (Nicolas/Gianni) Impedance Matrix implementation (Nicolas) Do it first in numpy/scipy Write it in the latex doc (Profile C++ vs Numba) Unit tests (Nicolas) Examples (Nicolas/Gianni/Sofia/Sebastien) Documentation (Nicolas/Gianni) Docstrings Readthedocs (doc on installation & use) (Attempt to parallelize on radial modes) (Nicolas) Convergence strategy (Nicolas/Gianni) PySSD Involved persons: Xavier Move to standard python-package structure (pip installable) Improve integration with DELPHI Milestones and timeline Timescale for objectives mentioned above: 2021 PyRADISE Involved persons: Xavier, Sondre Prepare startup examples. Milestones and timeline Timescale for objectives mentioned above: 2021 Impedance ToolBox Involved persons: Nicolas, Markus Develop a python package for the management of Accelerator Impedance Models. Use standard python-package structure (pip installable) The code should be interfaced to IW2D and TLWall Make sure that we cover specificities of all accelerators of interest (including injectors and possibly FCC) Milestones and timeline Timescale for objectives mentioned above: 2021 IW2D and TLWall Involved persons: Nicolas, Markus, Carlo Move to standard python-package structure (pip installable) Milestones and timeline Timescale for objectives mentioned above: 2021 RFTrack Involved persons: Andrea Support and development for identified use cases (positron sources, very low-energy machines, bunch compressors, electron cooling, RFQs, flash medical accelerator, Compton scattering) Implementation of CSR Expose individual components (interpolators, integrators) for combined usage with other tools (possibly via python) Milestones and timeline Timescale for objectives mentioned above: 2021 Beam cooling tools Interface Involved persons: Davide, Andrea Develop a common interface to use and compare different beam cooling codes (e.g. betacool and RFTrack) Further develop integration with other modeling tools (e.g. PyHEADTAIL, new tracking library) Milestones and timeline Timescale for objectives mentioned above: 2021-2022 Linear collider tools Placet / Placet2 Involved persons: Andrea Integrate in Placet 2 the missing features from Placet Missing CLIC-specific physics to be implemented (e.g. Power Extraction and Transfer Structures) Sliced beam model to be implemented Improve integration with other tools (PyHEADTAIL, XTrack), e.g. expose CSR modeling Milestones and timeline Timescale for objectives mentioned above: 2021-2022. In particular: Missing CLIC-specific physics should come in 2021 Sliced model should come in 2022 Guinea-Pig Involved persons: Andrea, Daniel Focus on C++ implementation if possible Extensions for FCC-ee being discussed Luminosity modeling tools LumiMod Involved persons: Guido, Gianni, Ilias Review and document the code Move to standard python-package structure (pip installable) Milestones and timeline Timescale for objectives mentioned above: 2021 Beam source modeling ONIX Involved persons: Anna, Jacques Adaptation to CERN\u2019s H- source Milestones and timeline Timescale for objectives mentioned above: 2022 FCC-ee developments Involved persons (from ABP): Riccardo, Xavier, Gianni, Tobias, Frank Schmidt, Daniel, Frank Zimmermann Synergy with code development project led by EPFL ABP involved on three main fronts: Development of optics tools Modeling of interaction regions (e.g. Beamstrahlung) Collimation studies","title":"Code-specific"},{"location":"codespecific_activities/#code-specific-objectives-and-activities","text":"","title":"Code-specific objectives and activities"},{"location":"codespecific_activities/#mad-x","text":"Involved persons: Tobias, Laurent Improve support for misaligned and overlapping elements Support cpymad for python workflows Improve save/reload capabilities (full mad-x state) Milestones and timeline Timescale for objectives mentioned above: 2021 Pending bug-fixes (required for ABP activities) Calculation of crabcavity R and T matrix not correct in MADX and PTC. Required to include crab cavities in the HL-LHC sequence! Some functionality duplication between crabcavity and RFmultipole. We might want to to port all the functionality in RFMultipole and call it from CrabCavity for backward compatibility. Related GitHub issue here Current master segfaults on a long hl-lhc script when compiled with 'make' Related GitHub issue here","title":"MAD-X"},{"location":"codespecific_activities/#mad-ng","text":"Involved persons: Laurent Focus will be on building know how and develop tools for non-linear normal forms analysis and advanced tracking maps (see recommendations of MAD-NG review ). The long-term goal is to match the capabilities and performance of PTC and possibly do better. Provide python interface for integration with other ABP and operation tools (LSA, pyheadtail, sixtracklib, pymask) Milestones and timeline See presentation at MAD-NG forum","title":"MAD-NG"},{"location":"codespecific_activities/#tracking-tools","text":"","title":"Tracking tools"},{"location":"codespecific_activities/#xtrack","text":"[NAME IS TEMPORARY] Involved people: Riccardo, Martin, Kostas, Frederik, Tobias, Gianni, Roderik Based on the sixtracklib experience, build a modular library (Xtrack) that performs tracking on CPUs and GPUs Requirements: Possibility of working in combination with other tools (PyHEADTAIL, PyPIC) for collective effects studies (instabilities, space-charge, e-cloud) Should be able to replace Sixtrack for conventional single-particle studies Run efficiently with HTCondor and BOINC (requires numerical reproducibility) Adopt design choices that facilitate future maintenance and development, as well as adaptation to new technologies (e.g. GPU standards) -> keep the code slim Allow the integration of advanced collimation capabilities (advanced aperture models, scattering, with FLUKA, tracking of fragments) Be extendable to model lepton rings. Design choices To keep a slim and flexible code, we choose to manage the memory and the simulation through python. We will use pyopencl to build a first version, then possibly extend to cupy and numba-cuda. Some modifications in PyPIC and PyHEADTAIL might be necessary. Description of the particles ensemble might be moved to a separate library ( Xpart ) and shared with PyHEADTAIL. Short-term tasks Build a first skeleton version. Collect a set of examples representative of all the use-cases, to make sure that the design covers all needs: Examples for DA and lifetime studies already available We need a set of examples illustrating che collimation us-cases (using the SixTrack) We need an example for the space charge studies (sixtracklib, pyheadtail, pypic) Milestones and timeline Skeleton code with main features Feb 2021 Collection of use-cases (e.g. space-charge, collimation) to be done in Q1-Q2 2021 By end of 2021 ABP-INC should be able to migrate their studies: requires all features available in sixtracklib 1.0, interface with PyPIC, dynamic elements (noise) Other features (aiming at matching SixTrack capabilities that we need to retain): 2022 For collimation features, the timescale will be defined only after reviewing all the use cases (might need to go beyond 2022, depending on allocated resources).","title":"Xtrack"},{"location":"codespecific_activities/#xpart","text":"[NAME IS TEMPORARY] Involved persons: Guido, Foteini, Kostas, Lotta Concentrate in a single library tools for the generation of particle ensembles. Capabilities are presently scattered over: SixDesk pysixdesk sixtrack (recent work by Riccardo, Tobias and co.) pysixtrack (Kostas' code) PyHEADTAIL (advanced longitudinal matching) This library should define a general and flexible particle description that can be imported and used in PyHEADTAIL, in the new tracking library and for other usages. Probably we can use PyHEADTAIL's Particles class as a starting point. Remember that masked access needs to be available for bunch slicing. Extraction of statistical properties (average position, r.m.s. sizes, optical functions, emittances) should be implemented (ported from PyHEADTAIL). Milestones and timeline Timescale for objectives mentioned above: 2021","title":"Xpart"},{"location":"codespecific_activities/#sixdesk","text":"Involved persons: Guido, Frederik Restructure the tool to ease maintenance and further development (re-use as much as possible the pysixdesk code). Integrate new developments (pymask, Xtrack) Improve usage flexibility. Explore the usage of containers ( singularity or docker ) on BOINC for more flexible simulations (e.g. including python) Move generation of particles distribution to a separate library usable for other applications (synergy with PyHEADTAIL). Milestones and timeline Timescale for objectives mentioned above: 2021-2022 Updates An example with some python helpers (prepared by Sofia) can be found here A possible functionality breakdown can be found here , together with some notes.","title":"SixDesk"},{"location":"codespecific_activities/#mask-pymask","text":"Involved persons: Guido, Sofia Kostoglou, NLD fellow (with Massimo) Extend to ions Strengthen automatic and interactive checks on error-routines, optics, knobs, etc. Validate beam 4 implementation. Plan transition to pymask for all applications. Milestones and timeline Timescale for objectives mentioned above: 2021-2022 Ion extension by Spring Validation of beam 4 and first set of checks by end-2021 Pending bug-fixes (required for ABP activities) Presently not working with machine imperfections! (Frederik is on it)","title":"mask / pymask"},{"location":"codespecific_activities/#tools-for-collective-effects","text":"","title":"Tools for collective effects"},{"location":"codespecific_activities/#combi","text":"Involved persons: Xavier, Sondre Port to python the top-level layer of COMBI that manages the interaction schedule, in order to profit from other libraries having a python interface (PyHEADTAIL, Xtrack) Build a \u201dproof-of-principle\u201d code to test and validate the main functionalities (e.g. asynchronous MPI calls) Then move to a well structured python library (reusing as much as possible modules from PyHEADTAIL) Expose to python features of COMBI that are not already available in other python packages (e.g. BTF, noise modeling). Milestones and timeline Timescale for objectives mentioned above: 2021-2022 \u201dproof-of-principle\u201d code with main functionalities in 2021","title":"COMBI"},{"location":"codespecific_activities/#pyheadtail","text":"Involved persons: Lotta Build a solid set of examples and tests covering the entire library Improve documentation, introduce a simple \u201cGetting started guide\u201d Finalize merge into master of the coupled-bunch branch Generation and description of particles ensembles should be moved to a separated library Xpart (shared with the Xtrack tracking library) Integrate with Xtrack library for advanced non-linear tracking. Introduce multithread CPU parallelization (numba, or cython), required for performance in COMBI-like simulations Review, document and update GPU features (need to stay compatible with Xtrack ). Integrate coasting beam capabilities (development from Nicolo'). Milestones and timeline Timescale for objectives mentioned above: 2021-2022 merge of coupled-bunch branch, example set, and documentation review to be done in 2021. ESRF would like to use the coupled bunch mode in python 3 (needs merged version). Tentatively this should be possible in June.","title":"PyHEADTAIL"},{"location":"codespecific_activities/#pypic-becomes-xfields","text":"Involved persons: Gianni Restructure the library to better integrate CPU-based FD solvers and GPU-based FFT solvers. Foresee combined usage with Xtrack for space-charge simulations. Move to standard python-package structure (pip installable). Milestones and timeline Timescale for objectives mentioned above: 2021 Design ideas and constraints Details here: https://github.com/giadarol/PyPIC/issues/1 Updates Development being done in this GitHub branch: https://github.com/giadarol/PyPIC/tree/reorganize Started sketching the structure Space-charge equations collected at: https://github.com/giadarol/PyPIC/tree/reorganize/doc","title":"PyPIC (becomes Xfields)"},{"location":"codespecific_activities/#pyecloud-pyparis","text":"Involved persons: Gianni, Lotta Move to standard python-package structure (pip installable). Milestones and timeline Timescale for objectives mentioned above: 2021","title":"PyECLOUD, PyPARIS"},{"location":"codespecific_activities/#delphi","text":"Involved persons: Nicolas, Gianni, Sebastien, Sofia Johannesson Restructure and simplify the code (more flexible interface, more pythonic, easier to bypass convergence tests, etc.), Move to standard python-package structure (pip installable) Integrate different matrix calculation from temporary eDELPHI package (to handle e-cloud, detuning impedances, short wakes) Improve integration with PySSD Milestones and timeline Timescale for objectives mentioned above: 2021 Design constraints and first ideas (2020-01-15) Requirements: We need the possibility of enlarging the matrix for convergence checks We should be able to reuse quantities when possible (e.g. the full matrix for \"intensity\" scans or different synchrotron tunes, or parts for the calculation). Some ingredients break the reusability, the code should check automatically. Design ideas: Make a base \"coupling matrix\" class with the methods that are shared (e.g. compute_complex_tune_shifts(rescale_factors) ). Such a base class will depend on the beam distribution and on the chosen base functions but not on the description of the collective effects forces. Then we make children classes with specialized implementations for the different descriptions of the coherent forces: impedance sum from DELPHI, sine responses from eDELPHI, but also Dirac-delta response, specialized versions for fast-decaying wakes and resonators. We should be able to combine matrices. We could try to express all cases (or most cases) in general form like: $$ {\\Large M_{l,m,l',m'} = K_\\text{coh}\\sum_n A_n R_{l,m,n} \\tilde{R}_{l',m',n}} $$ The different child classes will have different ways of computing \\Large A_n \\Large A_n (which is the impedance in the case of Delphi), \\Large R_{l,m,n} \\Large R_{l,m,n} , \\Large \\tilde{R}_{l',m',n} \\Large \\tilde{R}_{l',m',n} . We would keep these quantities in memory to test different matrix truncations... Using python memorizers might help, but we would lose the possibility of working with numpy arrays, which could impact performance. Updates Working in in https://gitlab.cern.ch/IRIS/DELPHI/-/tree/merged_eDELPHI The Harmonic Response Matrix from eDELPHI has been ported and tested. Need to move generic functionalities to base class (done to some extent). Need to separate the part coming from detuning with longitudinal amplitude into a separate class (done) Implement matrix extension for convergence checks. n_l , n_m , n_l_pos could be properties At that point we should review the conventions (e.g. Qs or Q_s?, do we use f or omega, tunes... -> we should be consistent across the code). Then move to impedance, damper etc. Questions: - Maybe move de parameters in a dictionary and not have them as lose members of the object. Easier to make copies of objects. - We need a slim physics manual for notations and conventions. Plan (16/06/2021) Detuning extension (Gianni) \u2013 end of August (can be done in parallel or later than the second point) Make some separate tests on impedance sum - check best strategy time wise (Nicolas) \u2013 end of August If FactorizedMatrix is the way to go, generalize its implementation with functions of \u201cm\u201d for R and R_tilde (Nicolas/Gianni) Impedance Matrix implementation (Nicolas) Do it first in numpy/scipy Write it in the latex doc (Profile C++ vs Numba) Unit tests (Nicolas) Examples (Nicolas/Gianni/Sofia/Sebastien) Documentation (Nicolas/Gianni) Docstrings Readthedocs (doc on installation & use) (Attempt to parallelize on radial modes) (Nicolas) Convergence strategy (Nicolas/Gianni)","title":"DELPHI"},{"location":"codespecific_activities/#pyssd","text":"Involved persons: Xavier Move to standard python-package structure (pip installable) Improve integration with DELPHI Milestones and timeline Timescale for objectives mentioned above: 2021","title":"PySSD"},{"location":"codespecific_activities/#pyradise","text":"Involved persons: Xavier, Sondre Prepare startup examples. Milestones and timeline Timescale for objectives mentioned above: 2021","title":"PyRADISE"},{"location":"codespecific_activities/#impedance-toolbox","text":"Involved persons: Nicolas, Markus Develop a python package for the management of Accelerator Impedance Models. Use standard python-package structure (pip installable) The code should be interfaced to IW2D and TLWall Make sure that we cover specificities of all accelerators of interest (including injectors and possibly FCC) Milestones and timeline Timescale for objectives mentioned above: 2021","title":"Impedance ToolBox"},{"location":"codespecific_activities/#iw2d-and-tlwall","text":"Involved persons: Nicolas, Markus, Carlo Move to standard python-package structure (pip installable) Milestones and timeline Timescale for objectives mentioned above: 2021","title":"IW2D and TLWall"},{"location":"codespecific_activities/#rftrack","text":"Involved persons: Andrea Support and development for identified use cases (positron sources, very low-energy machines, bunch compressors, electron cooling, RFQs, flash medical accelerator, Compton scattering) Implementation of CSR Expose individual components (interpolators, integrators) for combined usage with other tools (possibly via python) Milestones and timeline Timescale for objectives mentioned above: 2021","title":"RFTrack"},{"location":"codespecific_activities/#beam-cooling-tools","text":"","title":"Beam cooling tools"},{"location":"codespecific_activities/#interface","text":"Involved persons: Davide, Andrea Develop a common interface to use and compare different beam cooling codes (e.g. betacool and RFTrack) Further develop integration with other modeling tools (e.g. PyHEADTAIL, new tracking library) Milestones and timeline Timescale for objectives mentioned above: 2021-2022","title":"Interface"},{"location":"codespecific_activities/#linear-collider-tools","text":"","title":"Linear collider tools"},{"location":"codespecific_activities/#placet-placet2","text":"Involved persons: Andrea Integrate in Placet 2 the missing features from Placet Missing CLIC-specific physics to be implemented (e.g. Power Extraction and Transfer Structures) Sliced beam model to be implemented Improve integration with other tools (PyHEADTAIL, XTrack), e.g. expose CSR modeling Milestones and timeline Timescale for objectives mentioned above: 2021-2022. In particular: Missing CLIC-specific physics should come in 2021 Sliced model should come in 2022","title":"Placet / Placet2"},{"location":"codespecific_activities/#guinea-pig","text":"Involved persons: Andrea, Daniel Focus on C++ implementation if possible Extensions for FCC-ee being discussed","title":"Guinea-Pig"},{"location":"codespecific_activities/#luminosity-modeling-tools","text":"","title":"Luminosity modeling tools"},{"location":"codespecific_activities/#lumimod","text":"Involved persons: Guido, Gianni, Ilias Review and document the code Move to standard python-package structure (pip installable) Milestones and timeline Timescale for objectives mentioned above: 2021","title":"LumiMod"},{"location":"codespecific_activities/#beam-source-modeling","text":"","title":"Beam source modeling"},{"location":"codespecific_activities/#onix","text":"Involved persons: Anna, Jacques Adaptation to CERN\u2019s H- source Milestones and timeline Timescale for objectives mentioned above: 2022","title":"ONIX"},{"location":"codespecific_activities/#fcc-ee-developments","text":"Involved persons (from ABP): Riccardo, Xavier, Gianni, Tobias, Frank Schmidt, Daniel, Frank Zimmermann Synergy with code development project led by EPFL ABP involved on three main fronts: Development of optics tools Modeling of interaction regions (e.g. Beamstrahlung) Collimation studies","title":"FCC-ee developments"},{"location":"housekeeping/","text":"Housekeeping for ABP core tools Guidelines ABP core codes (listed here ) should fulfil guidelines provided here . Survey Situation and progress are tracked in this spreadsheet .","title":"Housekeeping"},{"location":"housekeeping/#housekeeping-for-abp-core-tools","text":"","title":"Housekeeping for ABP core tools"},{"location":"housekeeping/#guidelines","text":"ABP core codes (listed here ) should fulfil guidelines provided here .","title":"Guidelines"},{"location":"housekeeping/#survey","text":"Situation and progress are tracked in this spreadsheet .","title":"Survey"},{"location":"meetings/","text":"Meetings The agenda and Zoom links of ABP Computing Meetings can be find in the corresponding indico category . Notes from the meetings can be found here . e-group You can be included in the meetings invitation list by subscribing to the abp-cp-meetings e-group at this link .","title":"Meetings"},{"location":"meetings/#meetings","text":"The agenda and Zoom links of ABP Computing Meetings can be find in the corresponding indico category . Notes from the meetings can be found here .","title":"Meetings"},{"location":"meetings/#e-group","text":"You can be included in the meetings invitation list by subscribing to the abp-cp-meetings e-group at this link .","title":"e-group"},{"location":"notes_from_meetings/","text":"Notes from ABP Computing Meetings The agenda and Zoom links of ABP Computing Meetings can be find in the corresponding indico category Information and some points to be followed up from each meeting can be found in the following. 22 October 2021 Present: F. Asvesta, R. Bruce, X. Buffat, F. Carlier, L. Deniau, J. Farmer, D. Gamba, L. Giacomel, G. Iadarola, P. Kicsiny, A. Latina, K. Paraschou, A. Poyet, F. Van Der Veken. Notes from the meeting ... 17 September 2021 Present: F. Asvesta, R. Bruce, F. Carlier, R. De Maria, R. De Maria, L. Deniau, D. Gamba, L. Giacomel, P. Hermes, G. Iadarola, P. Kicsiny, J. Molson, N. Mounet, A. Poyet, M. Schwinzerl, G. Sterbini, F. Van Der Veken Notes from the meeting NXCALS survey has been launched: please take the time to share your experience and future needs. VEGA HPC cluster test: report being edited by IT. ABP participants contacted to provide feedback Very interesting talk on non-linear normal forms at last LNO meeting (L. Deniau). Subject for future ABP computing forum. Ongoing work on Xsuite: Synchrotron radiation advancing well (A. Latina, G. Iadarola) Interface with Geant4 running, setting up of HL-LHC loss map ongoing (A. Abramov) Strong-strong 6D bb being developed (P. Kicsiny, X Buffat) Large scale DA simulation (10k jobs) being compared against Sixtrack (S. Kostoglou, G. Sterbini) First long space-charge studies to launched (H. Bartosik, G. Iadarola) Generation of matched gaussian beam distributions with non-linear bucket (F. Asvesta), to be extended to more distributions of interest (pencil, halos\u2026) Next important milestones Implementation of reference management for knobs - deferred-expression like (R. De Maria) Lattice description, can we think of a further integration with MAD-X/cpymad which considers flexibility required for collective effects simulations (R. De Maria, G. Iadarola, T. Persson) Porting of particle scattering from sixtrack for collimation, K2 and Fluka coupling (F. Van Der Veken, P. Hermes, ?) Implementation of beam loss refinement, aperture interpolation (G. Iadarola, A. Abramov) Porting of symplectic interpolator from sixtracklib (K. Paraschou, G. Iadarola) Interface with MAD-NG would be a plus \uf0e0 needs MAD-NG/python interface (L. Deniau) Points to be followed up On the benchmark of synchrotron radiation in Xsuite people who might be interested are Axel, Andrey, Felix Carlier. Riccardo, Gianni, Felix and Tobias could start a discussion on rationalizing machine descriptions. Pascal, Bjorn and James are facing massive slowdown of AFS Axel discussed with Tobias and Leon to create tables with the EMIT module in MAD-X (presently one needs to parse the stdout). Lorenzo (as Xavier in the past) experiences large failure rate (>5%) for long HTCondor jobs. 3 September 2021 Present: F. Asvesta, R. De Maria, L. Deniau, P. Elson, J. Farmer, P. Hermes, G. Iadarola, P. Kicsiny, S. Kostoglou, A. Latina, A. Latina, L. Mether, N. Mounet, K. Paraschou, A. Poyet, M. Schwinzerl, G. Sterbini, F. Van Der Veken, A. Wegscheider Notes from the meeting From ATS-CTTB meeting (20 Aug): Presentation of Community Forum: Machine Learning (V. Kain): \"ML coffees' will evolve into the new Machine Learning Community Forum (conveners: A. Apollonio, V. Kain, F. Velotti). Positive experience from the CloudBank pilot project from IT to access computing resources (e.g. GPUs and even quantum computers) from external cloud providers (Google, Amazon) Remote Operations Gateway (T. Oulevey): New service developed by BE-CSS to access resources in the TN from outside CERN. It allows connecting to linux machines in the TN through a web browser (could be quite handy for machine follow-up). Confirmed end of support for Windows in Controls (M. Gourber-Pace) Replacement campaign is ongoing for faulty Siemens I/O cards CASTOR service is being discontinued: Users should have been already contacted to move their data Compute accelerator forum on 8 September. Presentation on Kokkos (C++ performance portability programming model) PRACE (Partnership for Advanced Computing in Europe) offers several online courses (often for free). Full list here . School on Efficient Scientific Computing in Bertinoro (Italy), 4 - 9 Ottobre 2021. ABP could support participation of 1-2 interested students. Pymask v1.1.0 released recently Should cover all use-cases of old mask files and offers more features We plan to soon shift to Pymask for long-term support. Please start using it and don't hesitate to give feedback. Developemnt of Xsuite for tracking simulations is advancing well. First production studies launched. Fell free to give it a try. Discussion on NXCALS/LSA (re)naming conventions, status here . Status of GUI development platforms: Java is and will be supported PyQT community growing fast especially in the injectors Points to be followed up Check status of \"pip install nxcals\". NXCALS survey has been launched: please take the time to share your experience and future needs. 2 July 2021 Present: R. Bruce, X. Buffat, R. De Maria, J. Farmer, D. Gamba, L. Giacomel, G. Iadarola, S. Kostoglou, L. Mether, J. Molson, K. Paraschou, M. Schwinzerl, F. Soubelet, G. Sterbini, T. Tobias, A. Wegscheider. Notes from the meeting UDEMY offer for CERN includes several course that could be of interest Xsuite is coming together: Integrated suite for single-particle and collective effects simulations Supports CPU and GPU Just introduced integration with PyHEADTAIL. By now tested for LHC with bb and machine imperfections and on SPS with frozen spacecharge Info at https://xsuite.readthedocs.io Feedback is welcome Points to be followed up There is an inconsistency in the element naming between twiss and survey in cpymad ( :n missing in survey). This is fixed in the mad-x master. Will become available in lxplus from the next release. 25 June 2021 Present: F. Asvesta, R. Bruce, R. De Maria, L. Deniau, J. Dilly, J. Farmer, D. Gamba, A. Gerbershagen, L. Giacomel, H. Graham, M. Hofer, G. Iadarola, P. Kicsiny, J. Molson, N. Mounet, K. Paraschou, F. Soubelet, G. Sterbini, T. Tobias, F. Van Der Veken. Notes from the meeting Continued review of tools to launch and manage jobs: Presentation by Felix on PyLHCsubmitter Presentation by Michael on DAGman 18 June 2021 Present: R. Bruce, R. De Maria, L. Deniau, J. Dilly, J. Farmer, D. Gamba, L. Giacomel, A. Gerbershagen, H. Graham, P. Hermes, C. Hernalsteens, E. Hoydalsvik, M. Hofer, G. Iadarola, A. Latina, J. Molson, T. Persson, A. Poyet, G. Russo, F. Soubelet, G. Sterbini, F. Van Der Veken, A. Wegscheider. Started discussion on tools to launch and manage jobs: Presentation by G. Sterbini General feedback: users prefer small tools for individual functionalities (templating, job submission) to be combined flexibly in python, instead of complex integrated tools that might lack flexibility and require a big investment to integrate our codes. Agility and adaptability to different simulation setups is key 11 June 2021 Present: J. Dilly, J. Farmer, A. Gerbershagen, P. Hermes, M. Hofer, G. Iadarola, A. Latina, L. Mether, N. Mounet, T. Persson, F. Soubelet. Notes from the meeting A Web Development Technical Exchange meeting organized by BE-CSS will takeplace on Thursday, June 24. The next computing meeting (18 June) will be devoted to tools for HTCondor submission and management. \u201cABP Computing day\u201d will take place towards the end of the year. The main goals will be: Discuss the progress on software development activities in ABP Present the strategy and objectives for 2022+ Tentative date: Thu 18 Nov 2021 (there might be a conflict with Evian 2021) Agenda to be drafted early enough to allow for proper preparation Proposals on possible topics and contributions are very welcome Points to be followed up Tobias has finished the implementation of the wire in MAD-X. Guido and team will test it. It would be useful to have a script in the MAD-X repository to generate cpymad from a development version. It could be limited to linux for the time being. Tests could be made against the R matrix computation from tracking (using finite differences) implemented by Kostas. Guido found problems runnnig pytimber on SWAN to access NXCALS: Ticket opened with CSS team 4 June 2021 Present: R. Bruce, F. Carlier, R. De Maria, P. Elson, J. Farmer, D. Gamba, A. Gerbenshagen, P. Hermes, G. Iadarola, A. Latina, L. Mether, J. Moldson, N. Mounet, K. Paraschou, F. Soubelet, G. Sterbini, M. Schwinzerl, F. Van Der Veken. Points to follow up Points related to BE-CSS services: Users had issues using pytimber with NXCALS. Something went wrong with their access request and they had troubles related to authentication. The main issue was that the exception raised did not refer at all to authentication and therfore it took very long time to diagnose the problem. PyJAPCScout is now being used widely in the injectors for commissioning and studies. The tool could be presented to a wider community in an AccPy meeting. It would be good to identify features of general interest and incorporate them in PyJAPC. There were issues installing NXCALS on the SWAN Stack 100. In principle NXCALS should be already installed. On 18 June there will be a meeting dedicated to HTCondor submission tools. It would be good to make a new release of sixtrack and update the sixtrack version on AFS. 7 May 2021 Present: X. Buffat, R. Bruce, F. Carlier, L. Deniau, J. Dilly, J. Farmer, P. Hermes, M. Hofer, D. Gamba, L. Giacomel, G. Iadarola, A. Latina, L. Mether, J. Molson, N. Mounet, K. Paraschou, T. Persson, M. Schwinzerl, F. Soubelet, G. Sterbini. Information BE-CSS contacted us to see if there is any interest from our side to use SWAN in the technical network: If such a service becomes available there would be definitely several users in ABP (especially if PyJAPC, PjLSA etc. would also be accessible through SWAN-TN). A question that was raised is wether making SWAN available in the TN would mean that the EOS filesystem would become also available in the TN (as SWAN presently runs on EOS). This would be very useful to avoid the overhead of copying data for offline analysis. The reorganization of the HTCondor e-groups has been finalized. Users from old e-groups (SLAP, ICE) have been redistributed. Old e-groups have been discontinued. Tobias and Laurent annonced the latest MAD-X release. Nicolas and Markus announced their new python package for handling impedance models . Points to be followed up AWAKE groups for batch and HPC should be rationalized. General discussion on HTCondor use and performance (to be followed up with IT): Different teams have developed tools to launch and manage HTCondor jobs (the most advanced are most likely from the OMC team and Guido/Axel). There is a general interest in using a supported python API for HTCondor. Could IT provide such a service? Users observe high failure rate for long jobs due to the fact that sometimes the SCHEDD stops working for a short time and the job hangs. Can any improvement be considered on the IT side? Users find the 2 GB of RAM associated with a single-core job small for many jobs. What is the overhead in terms of priority of requesting two cores just to have more RAM? Davide is working on the implementation of PyJAPC scout It would be useful to have a basic intro page 30 April 2021 Present: R.Bruce, X. Buffat, R. De Maria, F. Carlier, L. Denieau, J. Dilly, J. Farmer, S. Furuseth, A. Gerbershagen, P. Hermes, M. Hofer, G. Iadarola, A. Latina, J. Molson, N. Mounet, K. Paraschou, T. Persson, M. Rognlien, M. Schwinzerl, F. Soubelet, F. Van Der Veken. Information A very interesting workshop on \u201cCloudBank\u201d pilot project took place on Tuesday. Allows usage of computing resources from commercial providers (e.g. Google, Amazon) See indico page . HTCondor groups reorganization: Users from old e-groups (SLAP, ICE) were redistributed (thanks to Massimo for the help!). The old e-groups will be discontinued soon. Users can use the Haggis tool to check their accounting group (accessible only within the CERN network). MAD-NG vs PTC discrepancy for CLIC is understood. MAD-NG works on Apple M1. Work on non-linear normal forms is advancing. Points to be followed up It would be convenient to have the long-name export from MAD to sixtrack enabled by default. This should come with the next MAD release (next week). James would be interested in trying CVMFS for alleviating AFS/EOS issues with IO intensive startup of many jobs. A \"beam-physics\" CVMFS space was recently created (reqeuest by Cedric, TE-MPE) and could be used for this purpose. There should be the possibility of restricting access, e.g. to comply with software license conditions. Many parallel FCC-ee tracking efforts are starting. It is not always trivial to understand tools assumptions and conventions (e.g. with respect to tapering and sawtooth orbit). Some coordinated benchmarking effort would be beneficial. Andrea mentioned tha some benchmarking of MAD-X radiation features against MAD8 and Placet was done in the past. 23 April 2021 Present: A. Abramov, X. Buffat, J. Dilly, J. Farmer, L. Giacomel, D. Gamba, P. Hermes, G. Iadarola, A. Latina, J. Molson, N. Mounet, M. Rognlien, T. Persson, F. Soubelet, G. Sterbini, F. Van Der Veken Information CERN HPC team asked for test simulations that could be used to validate scalability on large external HPC clusters (details on mattermost ). Registrations to PyHEP2021 (July 2021) are open. HTCondor usage: so far no visible side effect of accounting groups remapping. James mentioned that bugfixes for Geant4 have been provided (related to issues mentioned in a previous meeting) and are being tested. Pull request for sixtrack coming soon (mostly collimation features). Andrea developed backtracking in RFTrack for injector-gun design based on required distribution defined downstream. Nicolas asked for passwordless access to NXCALS. It can be done only with kerberos. Markus suggested to look into the joblib library for simple parallel tasks. Points to be followed up MAD-X team looking into issue with IBS module. Pascal encountered issues with BOINC spool space. Being followed-up. Gianni/Frederik/Tobias to have a chat on status and evolution of DIST library. 16 April 2021 Present: R. Bruce, X. Buffat, D. Gamba, G. Iadarola, R. De Maria, J. Dilly, S. Joly, P. Elson, J. Farmer, A. Gerbenshagen, L. Giacomel, A. Latina, T. Persson, F. Soubelet, G. Sterbini, F. Van Der Veken. Information ABP HTCondor quota remapped on Tuesday 13 Apr: we now have a single computing group \u201cgroup_u_BE.ABP.NORMAL\u201d, which is organized in six e-groups associated to the sections batch-u-abp-cei (contains the old e-group: htcondor-u-ICE) batch-u-abp-hsl batch-u-abp-inc batch-u-abp-laf batch-u-abp-lno (contains old e-group: htcondor-u-SLAP) batch-u-abp-ndc (contains old e-group: LHC-COLL-LSF-users) Please use the new section e-groups to add new users. The old e-groups will be discontinued after moving the users to the respective section e-group. Discussion have started to prolong the exploitation of the cluster at INFN-CNAF (800 cores) beyond the end of the contract (Dec 2022). We could prolong its usage for three more years (paying the energy consumption) If some extra funding is available, a few more nodes could be added to compensate for expected \u201cageing\u201d. It is assumed that it will be used mostly for single-node jobs (up to 48 cores) due to end of support of the installed low-latency network (OK based on past experience). A survey is being run across the BE department to identify services provided by the IT department on which we rely for simulations and data analysis. The goal is to put in place a coordinated interface to IT and find synergies with other ATS departments. Some draft slides are discussed. Points that could be added: HDFS filesystem for data storage for spark-based analysis. Users would like a more complete and supported software stack (libraries, compilers, python, etc.) Openshift and webservices for documentation. Openstack, GitLab Licensed software: Mathematica, Matlab, CST, etc. Lxplus, Windows Terminal Services Information from Phil (CSS): A software stack based on LCG 100 is being prepared for usage in LXPLUS via CVMFS. A pip-installable NXCALS-pyspark bundle is being also considered. NXCALS can be accessed from outside CERN using an ssh tunnel. lxtunnel.cern.ch to be preferred to lxplus.cern.ch for this purpose (see recipe just updated by Joshua). Discussion on how to best reformat JAPC data for storage on parquet files: one possibility is to use the same serialization used at RDA level. 9 April 2021 Present: R. Bruce, X. Buffat, R. De Maria, L. Deniau, J. Farmer, A. Gerbenshagen, L. Giacomel, P. Hermes, G. Iadarola, A. Latina, N. Mounet, T. Persson, F. Soubelet, G. Sterbini, F. Van Der Veken. Information Phil from CSS will join the next meeting. Applications are open for the upcoming CERN thematic School on Computing on \"Scientific Software for Heterogeneous Architectures\u201d. A guide on mounting EOS on mac has been prepared by Ilias, Guido and Joshua. More info also on this mattermost discussion . An Accelerating Python user's meeting will take place t2021 Q1 on 29 April. Topics: PyTimber, PyJapc, accwidgets, asyncio usage A pilot program was launched by IT and IPT to access commercial cloud services (Google, Amazon). A workshop will be held on Apr 28 to have a first overview. Information on how to install EOS on Ubuntu can be found in this docker file prepared by Guido. Frederik deployed a fixed version of SixDesk/DB in the AFS repository (compiles correctly). Issues with slicing of solenoid in MAD-X was fixed by Helmut. Nicolas and Markus found very convenient to use joblib for simple parallelization in the new Impedance Toolbox. Points to be followed up New e-groups for access to HTCondor have been created. One per section, with associated admin e-group. SL and CP members have been added to admin e-group for their sections We will proceed to the remapping of the ABP quota quota next week. The operation should be transparent as the old e-groups were inserted inside the new ones. We should gradually empty the old groups and move users to the new ones Guido prepared an \"ABP docker container\" based on Ubuntu with the typical tools (python installation, mad-x) and the capability of mounting EOS. An installation of HTCondor will also be added. An issue was identified in the usage of beam-beam in sixrtack with ions (see GitHub issue ). A workaround has been inserted in pymask to circumvent the problem. Frederik is now working on fixing the error ruotines for the mask. Should be ready in a few weeks. A ticket has been opened by external users on the IBS calculation for MAD-X in some special cases. Tobias will look into it together with Fanouria. Andrea is studying multibunch effects for the FCC-ee injector linac. He improved the modeling of the wakefields. Perhaps we could look into possible synergies with PyHEADTAIL. 26 March 2021 Present: R. Bruce, X. Buffat, R. De Maria, L. Deniau, F. Calier, J. Dilly, J. Farmer, S. Furuseh, D. Gamba, L. Giacomel, P. Hermes, M. Hofer, G. Iadarola, A. Latina, J. Molson, N. Mounet, T. Persson, K. Paraschou, T. Pieloni, F. Soubelet, G. Sterbini, F. Van Der Veken. Information JetBrains (the company developing PyCharm) is looking for early adopters at CERN for their new IDE for data analysis and prototyping machine learning models. An interesting reading: \u201cGood enough practices in scientific computing\" The Zenodo service allows getting DOI code for data and code, for referencing in publications. Tatiana introduced the EPFL project on FCC-ee code development. Nicolas had very satisfactory results speeding up pythoon code with numba parallel. Points to be followed up There are discussions ongoing on the future of PyTIMBER (see: https://wikis.cern.ch/display/NXCALS/Future+of+PyTimber ). Riccardo joined the meeting.The idea is to converge to a single Python interface for NXCALS, which is easy to install, exposes the pyspark features, and has high-level methods like pytimber. CSS plans to discontinue the Java Backport API at the end of Run 3. Pascal encountered a blocking issue installing SixDesk/SixDB. Frederik will provide some help. It is proposed to enable the long names by default in the sixtrack input generation of MAD-X (requires Sixtrack V). Some updates have been introduced in the SixTrack collimation features (to be pushed). A problem has been identified in Geant 4 (a bug report has been opened). Non-staff members of the collimation team have issues accessing the source of Fluka. Nicolas asked whether anybody has experience with the diagonalization of very large matrices. Riccardo faced the problem in the past and will provide the solution that he adopted. 19 March 2021 Present: F. Antoniou, F. Asvesta, H. Bartosik, R. Bruce, F. Calier, R. De Maria, L. Deniau, P. Elson, J. Farmer, S. Furuseth, J. Dilly, P. Hermes, D. Gamba, L. Giacomel, M. Hofer, G. Iadarola, A. Latina, N. Mounet, K. Paraschou, T. Persson, T. Pieloni, F. Soubelet, G. Sterbini, F. Van Der Veken. Information The TE-MPE team would like to store software for beam physics in CVMFS. We will propose to create a general \"beam-physics\" project in CVMFS where we will able to store also ABP software if needed. Even if the AFS phaseout has been put on hold, IT is still checking with users whether their project spaces could be moved to EOS. In general this works for data storage, but can create problems when using the filesystem as a working space for simulations (e.g in HTCondor). For this kind of usage we have the option to keep using AFS. Next events: GPU Technology Conference 2021 (GTC21) , free to attend. Points related to CSS services This page summarizes issues encountered with python packages stored in PyPI, which depend on packages available only in the acc-py repository. Although a workaround was identified it would be good to find a more flexible solution. The ongoing development to adapt PyJAPC to the needs of the injectors commissioning (to discontinue the usage of matlabMonitor) is using this repository . More info can be found in the slides shared by Guido . The main needs are: Ensuring \u201csynchronization\u201d of a list of devices properties Having a single centralized callback Saving automatically the data in a \u201creasonable\u201d format. Relying on pandas for the data manipulation is a reasonable choice. The candidate format for the data saving is parquet at present, but we could probably get in contact with the NXCALS team to see if they have any further suggestion. Feather and HDF5 could also be investigated. Answers to some of the questions in Guido's slides were provided by Phil after the meeting Q: Is it possible to install Acc-Py on non TN machines? Is there a container/docker image? A: Yes, there is an installer for the base distribution available. Please see https://wikis.cern.ch/display/ACCPY/Acc-Py+base#Acc-Pybase-UsingAcc-Pybase . If there are any issues, please contact acc-python-support@cern.ch for the installer. There is a prototype docker image, but we have not officially released it yet; we would happily engage with anybody who wanted to use the docker image to better understand the use cases and requirements. Q: Is there a better way to avoid blocking the exit of a Python application which uses the JVM? A: The next version of cmmnbuild-dep-manager includes a workaround for Java libraries that aren\u2019t correctly clearing up their non-daemon threads. This will be released over the coming days as cmmnbuild-dep-manager 2.8.0. Q: What is the best way to build documentation for packages with Acc-Py A: Please see https://wikis.cern.ch/display/ACCPY/Documentation for some tips. We would be very happy to improve our documentation service\u2019s documentation \ud83d\ude0a\u2026 please get in touch if something isn\u2019t clear or could be better phrased! Note that this service is only available inside the GPN \u2013 we don\u2019t currently plan to make the documentation available outside of the CERN network. For externally hosted documentation readthedocs is a good solution. 12 March 2021 Present: X. Buffat, R. Bruce, F. Carlier, R. De Maria, J. Dilly, L. Deniau, J. Farmer, S. Furuseth, P. Hermes, M. Hofer, G. Iadarola, L. Giacomel, S. Kostoglou, A. Latina, J. Molson, N. Mounet, T. Persson, G. Sterbini. Information Phil from CSS will join the next meeting (19 March 2021) Talks on GPU applications at last Compute Accelerator Forum ( link ). Gianni is progressing with the development of the Xfield library. Sofia is continuing the development of pymask to cover simulations with ions. Pascal is looking into refurbishing the python tools used for collimation, to move to a pip-installable package. Something that might be useful: Guidelines for python packages are available on the ABP Computing website (including an example package with instructions). Git workflows can be used for the automatic deployment in PyPIC (see for example PyLHC ). Guido found an issue in the generation of the MAD-X beam-beam lenses in pymask (which does not affect the generation of the sixtrack models, unless matchings are done with beam-beam on). This has been fixed in the latest release. Laurent is working on non-linear normal forms. Andrea (and Raul) are porting CSR features form Placet 1 to Placet 2, improving the implementation in different ways. John and the AWAKE team solved an issue with their quasi-static PIC code. Riccardo added a little feature in cpymad to access the strengths generated by the correct module. Points to be followed up IT is moving Nag and Lahey compilers and libraries from afs to cvmfs. The compilers are indeed used to test sixtrack compilation. Joshua and Felix prepared an intro page on AccPy . Here they summarize also the issues that they faced with python packages hosted in the standard PyPI index, which depend AccPy hosted packages. Their workaround to make the package pip installable is described in the page o Definitely a general issue to be discussed with CSS. EPFL is working on code development for FCC-ee. Input on possible software requirements for future studies is welcome. MAD-X development (Tobias): Issue with equilibrium emittance calculation in the presence of tapering being investigated (for FCC-ee). Improving the slicing of RBEND (placing of slices for shared dipoles). Sondre finds extreme numerical convergence conditions (number of slices) in COMBI to resolve the tune shift. Roderick and team are working on identifying software tools for collimation studies for FCC-ee. In a few weeks we could have a first discussion on collimation requirements for the Xtrack library that is being developed. Discussion are starting in CSS on how to rationalize python tools to access NXCALS. 5 March 2021 Present: X. Buffat, R. Bruce, F. Carlier, L. Deniau, R. De Maria, J. W. Dilly, J. Farmer, M. Hofer, D. Gamba, P. Hermes, G. Iadarola, A. Latina, S. Joly, N. Mounet, M, Rognlien, G. Sterbini, T. Persson, F. Soubelet, F. Van Der Veken. Information New ABP Computing Sandbox space was created on GitLab (suggestion by Davide and Guido). Feel free to use it as incubator for new shared projects. Following discussion from last meeting: Guido prepared a simple guide on the use of docker containers (available here ) Nicolas prepared a small guide on how to install PyHEADTAIL on Mac using Anaconda (available here ) In SixTrack two bug-fixes related to collimation were introduced (see PR1 and PR2 ) Report from the last IT User Meeting (by Laurent). Agenda and material of the meeting can be found here . Particularly relevant for our scientific computing activitiies: AFS replacement is now stopped (since no available drop-in solution was found). The strategy will be reviewed at the end of Run-3 (Interesting review document published - https://cds.cern.ch/record/2750122 ). A working group is being setup to look into possible replacement of CentOS7 due to a change in support policy from RedHat. There is an upcoming OpenLab Technical workshop ( indico page ). Points to be followed up For MAD-X: The new MAD-X release is slowly converging. The build through GitHub actions started failing due to an upgrade of the compiler on the GitHub size (probably related to vectorization and data alignment). It might be related to the problems recently encountered by Riccardo (see issue ). For SixTrack: Sixtrack build with GCC 10 was failing due to multiple includes in DISTlib. A workaround has been introduced in sixtrack (see PR ), but proper fix needs to be introduced in DISTlib. Full crab cavity tilt required by Sofia is not really easy to implement, but could be handled through reference frame rotations in the user's side. Features to complement PyJAPC are being developed in the PyJapcScout repository on the ABP Computing sandbox. Josh and Felix are facing issue in the development of the OMC3 toolbox, having to combine packages from the standard PyPI and the acc-py package indices. They arranged a workaround and summarized the situation here -> to be discussed with Phil, to see whether better solutions can be identified. 26 February 2021 Seminar Our usual round table was replaced by a seminar by R. Brito Da Rocha from IT on \"Notebooks and Computing Workflows with IT services\". The slides and and the recording of his presentation are available on indico . 19 February 2021 Present: R. Bruce, X. Buffat, L. Deniau, P. Elson, J. Farmer, P. Hermes, G. Iadarola, S. Kostoglou, A. Latina, N. Mounet, T. Persson, G. Sterbini. Information P. Elson, indicated as link person between BE-CSS and BE-ABP, joined the meeting. He has vast background on python solutions for scientific computing and has a leading role in the AccPy community. He could join our meetings ~once per month, to discuss CSS services. Two guides in abpcomputing website have been updated to cover Ubuntu 20.04: Local HTCondor installation (thanks Joschua & co.) OpenAFS installation (thanks Guido, Riccardo etc.) We have two new Mattermost channels : Six* (for sixtrack, sixdesk, and other things like that) and pymask. The change of PyPI index caused by the installation of the acc-py-pip-config package can be undone with pip uninstall. Thanks Phil! Sofia is working to extend pymask to work with ions. There will be an inrmediat MAD-X course on 22-23 March Points to be followed up Guido gave a presentation at the ABP-INC meeting on how to reverse sequences using cpymad (generation of beam 4). cpymad input from python seems to be slower than calling a madx script file. To be investigated. The issue with SixTrack V identified by the collimation team was traced back to wrong units used for the proton mass in one place (to be ported in the master branch). There was also an issue compiling Sixtrack in the presence of the dist module. For now this can be bypassed disabling the module (not needed for these studies). Tobias will follow it up. Issue with crabcavity in MAD-X : it has been fixed in the MAD part. The problem with PTC still needs to be understood (not urgent). The fix for the MAD side will be deployed with the next release. Group subscription in PyJAPC: Davide will start working on it soon, using examples for the PS provided by Alex. Phil provided useful suggestions to ease the implementation. Nicolas asked for suggestions to get PyHEADTAIL running on Mac (issues with compiling the Cython part). One possibility is to usee Anaconda: after the meeting Nicolas provided a simple recipe on the PyHEADTAIL wiki . The alternative is to use docker -> some information on how to get started has been prepared by Guido ( available on CodiMD , to be ported to the abpcomputing website). 12 February 2021 Present: R. Bruce, R. De Maria, J. Dilly, D. Gamba, A. Gerbershagen, P. Hermes, G. Iadarola, S. Joly, J. Farmer, A. Latina, L. Medina, N. Mounet, K. Paraschou, T. Persson, G. Sterbini, F. Van Der Veken. Information Phil Elson will be the CSS contact person for ABP News on GPU infrastructure from IT (e.g. GPUs for CI). More info ( here ) Discussion on necessary prerequisites to migrate ABP tools for the injectors to PyJAPC, to be followed up in collaboration with BE-CSS Need for group subscriptions (as available in MATLAB interface). Davide working on the development of this functionality Need to identify a common way of saving PyJAPC data on file (Guido investigated the possibility of using parquet files) Some extra information on how to access NXCALS using pytimber is available at: http://abpcomputing.web.cern.ch/guides/pytimber_nxcals/ Issues encountered with with Pytimber in SWAN: there are plans ot remove pytimber from SWAN and leave the installation to the user. News from BE-EA: some members of their team are already using cpymad. Their interface to mad-x (ApplePy) is being interfaced with the control system (CESAR). Points to be followed up Issue with CrabCavity map in MAD-X should be easy to solve. Fixing the PTC side is more involved but not urgent. Issue in the error routines in the lhcmask is expected to be addressed in 2-3 weeks. The collimation team had problems in compiling sixtrack, which could be solved only by removing the DIST block. Tobias is aware of this and will follow it up. No particular urgency, as this is not blocking. The collimation team is investigating different behaviors of the scattering routines between Sixtrack 4 and Sixtrack 5. Andrea is trying to align spacecharge and beambeam development in MAD-X from F. Schmidt and collaborators to the latest version of the code. Herry Randal is investigating speed issues of MAD-X compared to older versions. The issue seems to be related to OpenMP. 5 February 2021 Present: F. Asvesta, D. Banerjee, R. Bruce, X. Buffat, L. Deniau, R. De Maria, J. Farmer, S. Furuseth, A. Gerbershagen, L. Giacomel, P. Hermes, G. Iadarola, S. Kostoglou, J.B. Lallement, A. Latina, N. Mounet, T. Persson, G. Sterbini, F. Van Der Veken. Information Opened Mattermost channel Proposal (driven by BE-CSS) to create a structured interface between the ATS and IT managements, being discussed within ATS. Software developers from the BE-EA team joined the meeting. They will stay in contact for discussions related to mad-x and layout database. Riccardo and team might use their expertise with BEACH to understand discrepancies with respect to MAD-X. Tensor Processing Unit (TPU) made available experimentally by IT for machine learning applications (contact IT person: Ricardo Brito Da Roca) CALS was discontinued on Feb 1, replaced by NXCALS. Guido is working on a simple package to create trees of jobs. Andrea implemented long-range wakes in RFTrack for studying multibunch effects in FLASH. Synergy with PyHEADTAIL for parallelizing interpolations. Xavier implemented new feature in COMBI to have linearizeed beam-beam (for tests). Points to be followed up MAD-X issues to be followed up: Calculation of crabcavity R and T matrix not correct in MADX and PTC. (Required to include crab cavities in the HL-LHC sequence!). Related GitHub issue here Current master segfaults on a long hl-lhc script when compiled with 'make'. Related GitHub issue here Tobias is reviewing the behavior of the solenoid. 29 January 2021 Present: G. Iadarola, X. Buffat, L. Deniau, R. De Maria, J. Farmer, S. Furuseth, D. Gamba, S. Kostoglou, A. Latina, N. Nounet, K. Paraschou, G. Sterbini, F. Van Der Veken, T. Persson, F. Soubelet. Informatio n** Ilias and Guido prepared a page for the configuration of NXCALS (pyspark) on lxplus. It can be found here . The Timber web application does not work from outside the CERN GPN network. To solve this, Riccardo and Davide have recipes to redirect the web traffic to the CERN network. Their bash scripts have been made available here . Example SixDesk study prepared by Sofia and Guido. First thoughts on how to extract components of general interest. Davide and Andrea are working on benchmarking of cooling modeling tools. A thorough exploration of the parameter space is revealing quite educative, leading to several improvements in RFTrack. Tracking library development: refactoring of GPU memory management is ongoing (Riccardo). Davide is refactoring the MATLAB interface to the controls infrastructure (PyJAPC is now used under the hood). John and the AWAKE team are comparing different PIC solvers in very challenging scenarios with many plasma buckets. New release of MAD-X coming soon to fix a bug related to particle losses in PTC-Track (Tobias). The CERN gitlab service now allows Continuos Integration tests on different GPUs (more info on Mattermost GPU). Work on restructuring and extension of DELPHI Vlasov solver is ongoing (Nicolas). GitHub actions allow build tests on several different platform (being used by MAD-X team). We will have a IT seminar on \"Notebooks and Computing Workflows with IT services\" on 26 Feb ( indico page ). Points to be followed up Reorganization of HTCondor e-groups (one e-group per section). Codes housekeeping page to be completed with missing info. Some of the tools from the OMC suite could be of general interest (e.g. harpy, machine imperfection management and correction). A first step could be to prepare a list of components that could be shared as individual packages, and start from the ones that are easier and more useful. An updated version of harpy should come within a few months. In MAD-X/PTC, found a bug related to particles having mass that is not a multiple of the electron or the proton mass. Piotr might be able to look into that. There is a problem with imperfection simulation in the mask. If anybody needs simulations with imperfections before this is solved should contact Frederik. COMBI team worried about \"unintended consequences\" of migrating repository to common space. Forking instead of transferring the project should be safe. 22 January 2021 (Kick-off meeting) Present: R. Bruce, R. De Maria, L. Deniau, G. Iadarola, J. Farmer, S. Furuseth, D. Gamba, L. Giacomel, A. Latina, J. Lettry, T. Persson, F. Van Der Veken. Points to be followed up Please review the list of objectives and activities and provide feedback by Fri 29 Jan (comments already received from Roderik and Jacques \u2192 added to the webpage) Feel free to use the abp-computing website to share any technical information/guideline of general interest (including those related to controls/operation tools). Anybody interested in attending the meetings can self-subscribe the invitation list using this link .","title":"Notes from meetings"},{"location":"notes_from_meetings/#notes-from-abp-computing-meetings","text":"The agenda and Zoom links of ABP Computing Meetings can be find in the corresponding indico category Information and some points to be followed up from each meeting can be found in the following.","title":"Notes from ABP Computing Meetings"},{"location":"notes_from_meetings/#22-october-2021","text":"Present: F. Asvesta, R. Bruce, X. Buffat, F. Carlier, L. Deniau, J. Farmer, D. Gamba, L. Giacomel, G. Iadarola, P. Kicsiny, A. Latina, K. Paraschou, A. Poyet, F. Van Der Veken.","title":"22 October 2021"},{"location":"notes_from_meetings/#notes-from-the-meeting","text":"...","title":"Notes from the meeting"},{"location":"notes_from_meetings/#17-september-2021","text":"Present: F. Asvesta, R. Bruce, F. Carlier, R. De Maria, R. De Maria, L. Deniau, D. Gamba, L. Giacomel, P. Hermes, G. Iadarola, P. Kicsiny, J. Molson, N. Mounet, A. Poyet, M. Schwinzerl, G. Sterbini, F. Van Der Veken","title":"17 September 2021"},{"location":"notes_from_meetings/#notes-from-the-meeting_1","text":"NXCALS survey has been launched: please take the time to share your experience and future needs. VEGA HPC cluster test: report being edited by IT. ABP participants contacted to provide feedback Very interesting talk on non-linear normal forms at last LNO meeting (L. Deniau). Subject for future ABP computing forum. Ongoing work on Xsuite: Synchrotron radiation advancing well (A. Latina, G. Iadarola) Interface with Geant4 running, setting up of HL-LHC loss map ongoing (A. Abramov) Strong-strong 6D bb being developed (P. Kicsiny, X Buffat) Large scale DA simulation (10k jobs) being compared against Sixtrack (S. Kostoglou, G. Sterbini) First long space-charge studies to launched (H. Bartosik, G. Iadarola) Generation of matched gaussian beam distributions with non-linear bucket (F. Asvesta), to be extended to more distributions of interest (pencil, halos\u2026) Next important milestones Implementation of reference management for knobs - deferred-expression like (R. De Maria) Lattice description, can we think of a further integration with MAD-X/cpymad which considers flexibility required for collective effects simulations (R. De Maria, G. Iadarola, T. Persson) Porting of particle scattering from sixtrack for collimation, K2 and Fluka coupling (F. Van Der Veken, P. Hermes, ?) Implementation of beam loss refinement, aperture interpolation (G. Iadarola, A. Abramov) Porting of symplectic interpolator from sixtracklib (K. Paraschou, G. Iadarola) Interface with MAD-NG would be a plus \uf0e0 needs MAD-NG/python interface (L. Deniau)","title":"Notes from the meeting"},{"location":"notes_from_meetings/#points-to-be-followed-up","text":"On the benchmark of synchrotron radiation in Xsuite people who might be interested are Axel, Andrey, Felix Carlier. Riccardo, Gianni, Felix and Tobias could start a discussion on rationalizing machine descriptions. Pascal, Bjorn and James are facing massive slowdown of AFS Axel discussed with Tobias and Leon to create tables with the EMIT module in MAD-X (presently one needs to parse the stdout). Lorenzo (as Xavier in the past) experiences large failure rate (>5%) for long HTCondor jobs.","title":"Points to be followed up"},{"location":"notes_from_meetings/#3-september-2021","text":"Present: F. Asvesta, R. De Maria, L. Deniau, P. Elson, J. Farmer, P. Hermes, G. Iadarola, P. Kicsiny, S. Kostoglou, A. Latina, A. Latina, L. Mether, N. Mounet, K. Paraschou, A. Poyet, M. Schwinzerl, G. Sterbini, F. Van Der Veken, A. Wegscheider","title":"3 September 2021"},{"location":"notes_from_meetings/#notes-from-the-meeting_2","text":"From ATS-CTTB meeting (20 Aug): Presentation of Community Forum: Machine Learning (V. Kain): \"ML coffees' will evolve into the new Machine Learning Community Forum (conveners: A. Apollonio, V. Kain, F. Velotti). Positive experience from the CloudBank pilot project from IT to access computing resources (e.g. GPUs and even quantum computers) from external cloud providers (Google, Amazon) Remote Operations Gateway (T. Oulevey): New service developed by BE-CSS to access resources in the TN from outside CERN. It allows connecting to linux machines in the TN through a web browser (could be quite handy for machine follow-up). Confirmed end of support for Windows in Controls (M. Gourber-Pace) Replacement campaign is ongoing for faulty Siemens I/O cards CASTOR service is being discontinued: Users should have been already contacted to move their data Compute accelerator forum on 8 September. Presentation on Kokkos (C++ performance portability programming model) PRACE (Partnership for Advanced Computing in Europe) offers several online courses (often for free). Full list here . School on Efficient Scientific Computing in Bertinoro (Italy), 4 - 9 Ottobre 2021. ABP could support participation of 1-2 interested students. Pymask v1.1.0 released recently Should cover all use-cases of old mask files and offers more features We plan to soon shift to Pymask for long-term support. Please start using it and don't hesitate to give feedback. Developemnt of Xsuite for tracking simulations is advancing well. First production studies launched. Fell free to give it a try. Discussion on NXCALS/LSA (re)naming conventions, status here . Status of GUI development platforms: Java is and will be supported PyQT community growing fast especially in the injectors","title":"Notes from the meeting"},{"location":"notes_from_meetings/#points-to-be-followed-up_1","text":"Check status of \"pip install nxcals\". NXCALS survey has been launched: please take the time to share your experience and future needs.","title":"Points to be followed up"},{"location":"notes_from_meetings/#2-july-2021","text":"Present: R. Bruce, X. Buffat, R. De Maria, J. Farmer, D. Gamba, L. Giacomel, G. Iadarola, S. Kostoglou, L. Mether, J. Molson, K. Paraschou, M. Schwinzerl, F. Soubelet, G. Sterbini, T. Tobias, A. Wegscheider.","title":"2 July 2021"},{"location":"notes_from_meetings/#notes-from-the-meeting_3","text":"UDEMY offer for CERN includes several course that could be of interest Xsuite is coming together: Integrated suite for single-particle and collective effects simulations Supports CPU and GPU Just introduced integration with PyHEADTAIL. By now tested for LHC with bb and machine imperfections and on SPS with frozen spacecharge Info at https://xsuite.readthedocs.io Feedback is welcome","title":"Notes from the meeting"},{"location":"notes_from_meetings/#points-to-be-followed-up_2","text":"There is an inconsistency in the element naming between twiss and survey in cpymad ( :n missing in survey). This is fixed in the mad-x master. Will become available in lxplus from the next release.","title":"Points to be followed up"},{"location":"notes_from_meetings/#25-june-2021","text":"Present: F. Asvesta, R. Bruce, R. De Maria, L. Deniau, J. Dilly, J. Farmer, D. Gamba, A. Gerbershagen, L. Giacomel, H. Graham, M. Hofer, G. Iadarola, P. Kicsiny, J. Molson, N. Mounet, K. Paraschou, F. Soubelet, G. Sterbini, T. Tobias, F. Van Der Veken.","title":"25 June 2021"},{"location":"notes_from_meetings/#notes-from-the-meeting_4","text":"Continued review of tools to launch and manage jobs: Presentation by Felix on PyLHCsubmitter Presentation by Michael on DAGman","title":"Notes from the meeting"},{"location":"notes_from_meetings/#18-june-2021","text":"Present: R. Bruce, R. De Maria, L. Deniau, J. Dilly, J. Farmer, D. Gamba, L. Giacomel, A. Gerbershagen, H. Graham, P. Hermes, C. Hernalsteens, E. Hoydalsvik, M. Hofer, G. Iadarola, A. Latina, J. Molson, T. Persson, A. Poyet, G. Russo, F. Soubelet, G. Sterbini, F. Van Der Veken, A. Wegscheider. Started discussion on tools to launch and manage jobs: Presentation by G. Sterbini General feedback: users prefer small tools for individual functionalities (templating, job submission) to be combined flexibly in python, instead of complex integrated tools that might lack flexibility and require a big investment to integrate our codes. Agility and adaptability to different simulation setups is key","title":"18 June 2021"},{"location":"notes_from_meetings/#11-june-2021","text":"Present: J. Dilly, J. Farmer, A. Gerbershagen, P. Hermes, M. Hofer, G. Iadarola, A. Latina, L. Mether, N. Mounet, T. Persson, F. Soubelet.","title":"11 June 2021"},{"location":"notes_from_meetings/#notes-from-the-meeting_5","text":"A Web Development Technical Exchange meeting organized by BE-CSS will takeplace on Thursday, June 24. The next computing meeting (18 June) will be devoted to tools for HTCondor submission and management. \u201cABP Computing day\u201d will take place towards the end of the year. The main goals will be: Discuss the progress on software development activities in ABP Present the strategy and objectives for 2022+ Tentative date: Thu 18 Nov 2021 (there might be a conflict with Evian 2021) Agenda to be drafted early enough to allow for proper preparation Proposals on possible topics and contributions are very welcome","title":"Notes from the meeting"},{"location":"notes_from_meetings/#points-to-be-followed-up_3","text":"Tobias has finished the implementation of the wire in MAD-X. Guido and team will test it. It would be useful to have a script in the MAD-X repository to generate cpymad from a development version. It could be limited to linux for the time being. Tests could be made against the R matrix computation from tracking (using finite differences) implemented by Kostas. Guido found problems runnnig pytimber on SWAN to access NXCALS: Ticket opened with CSS team","title":"Points to be followed up"},{"location":"notes_from_meetings/#4-june-2021","text":"Present: R. Bruce, F. Carlier, R. De Maria, P. Elson, J. Farmer, D. Gamba, A. Gerbenshagen, P. Hermes, G. Iadarola, A. Latina, L. Mether, J. Moldson, N. Mounet, K. Paraschou, F. Soubelet, G. Sterbini, M. Schwinzerl, F. Van Der Veken.","title":"4 June 2021"},{"location":"notes_from_meetings/#points-to-follow-up","text":"Points related to BE-CSS services: Users had issues using pytimber with NXCALS. Something went wrong with their access request and they had troubles related to authentication. The main issue was that the exception raised did not refer at all to authentication and therfore it took very long time to diagnose the problem. PyJAPCScout is now being used widely in the injectors for commissioning and studies. The tool could be presented to a wider community in an AccPy meeting. It would be good to identify features of general interest and incorporate them in PyJAPC. There were issues installing NXCALS on the SWAN Stack 100. In principle NXCALS should be already installed. On 18 June there will be a meeting dedicated to HTCondor submission tools. It would be good to make a new release of sixtrack and update the sixtrack version on AFS.","title":"Points to follow up"},{"location":"notes_from_meetings/#7-may-2021","text":"Present: X. Buffat, R. Bruce, F. Carlier, L. Deniau, J. Dilly, J. Farmer, P. Hermes, M. Hofer, D. Gamba, L. Giacomel, G. Iadarola, A. Latina, L. Mether, J. Molson, N. Mounet, K. Paraschou, T. Persson, M. Schwinzerl, F. Soubelet, G. Sterbini.","title":"7 May 2021"},{"location":"notes_from_meetings/#information","text":"BE-CSS contacted us to see if there is any interest from our side to use SWAN in the technical network: If such a service becomes available there would be definitely several users in ABP (especially if PyJAPC, PjLSA etc. would also be accessible through SWAN-TN). A question that was raised is wether making SWAN available in the TN would mean that the EOS filesystem would become also available in the TN (as SWAN presently runs on EOS). This would be very useful to avoid the overhead of copying data for offline analysis. The reorganization of the HTCondor e-groups has been finalized. Users from old e-groups (SLAP, ICE) have been redistributed. Old e-groups have been discontinued. Tobias and Laurent annonced the latest MAD-X release. Nicolas and Markus announced their new python package for handling impedance models .","title":"Information"},{"location":"notes_from_meetings/#points-to-be-followed-up_4","text":"AWAKE groups for batch and HPC should be rationalized. General discussion on HTCondor use and performance (to be followed up with IT): Different teams have developed tools to launch and manage HTCondor jobs (the most advanced are most likely from the OMC team and Guido/Axel). There is a general interest in using a supported python API for HTCondor. Could IT provide such a service? Users observe high failure rate for long jobs due to the fact that sometimes the SCHEDD stops working for a short time and the job hangs. Can any improvement be considered on the IT side? Users find the 2 GB of RAM associated with a single-core job small for many jobs. What is the overhead in terms of priority of requesting two cores just to have more RAM? Davide is working on the implementation of PyJAPC scout It would be useful to have a basic intro page","title":"Points to be followed up"},{"location":"notes_from_meetings/#30-april-2021","text":"Present: R.Bruce, X. Buffat, R. De Maria, F. Carlier, L. Denieau, J. Dilly, J. Farmer, S. Furuseth, A. Gerbershagen, P. Hermes, M. Hofer, G. Iadarola, A. Latina, J. Molson, N. Mounet, K. Paraschou, T. Persson, M. Rognlien, M. Schwinzerl, F. Soubelet, F. Van Der Veken.","title":"30 April 2021"},{"location":"notes_from_meetings/#information_1","text":"A very interesting workshop on \u201cCloudBank\u201d pilot project took place on Tuesday. Allows usage of computing resources from commercial providers (e.g. Google, Amazon) See indico page . HTCondor groups reorganization: Users from old e-groups (SLAP, ICE) were redistributed (thanks to Massimo for the help!). The old e-groups will be discontinued soon. Users can use the Haggis tool to check their accounting group (accessible only within the CERN network). MAD-NG vs PTC discrepancy for CLIC is understood. MAD-NG works on Apple M1. Work on non-linear normal forms is advancing.","title":"Information"},{"location":"notes_from_meetings/#points-to-be-followed-up_5","text":"It would be convenient to have the long-name export from MAD to sixtrack enabled by default. This should come with the next MAD release (next week). James would be interested in trying CVMFS for alleviating AFS/EOS issues with IO intensive startup of many jobs. A \"beam-physics\" CVMFS space was recently created (reqeuest by Cedric, TE-MPE) and could be used for this purpose. There should be the possibility of restricting access, e.g. to comply with software license conditions. Many parallel FCC-ee tracking efforts are starting. It is not always trivial to understand tools assumptions and conventions (e.g. with respect to tapering and sawtooth orbit). Some coordinated benchmarking effort would be beneficial. Andrea mentioned tha some benchmarking of MAD-X radiation features against MAD8 and Placet was done in the past.","title":"Points to be followed up"},{"location":"notes_from_meetings/#23-april-2021","text":"Present: A. Abramov, X. Buffat, J. Dilly, J. Farmer, L. Giacomel, D. Gamba, P. Hermes, G. Iadarola, A. Latina, J. Molson, N. Mounet, M. Rognlien, T. Persson, F. Soubelet, G. Sterbini, F. Van Der Veken","title":"23 April 2021"},{"location":"notes_from_meetings/#information_2","text":"CERN HPC team asked for test simulations that could be used to validate scalability on large external HPC clusters (details on mattermost ). Registrations to PyHEP2021 (July 2021) are open. HTCondor usage: so far no visible side effect of accounting groups remapping. James mentioned that bugfixes for Geant4 have been provided (related to issues mentioned in a previous meeting) and are being tested. Pull request for sixtrack coming soon (mostly collimation features). Andrea developed backtracking in RFTrack for injector-gun design based on required distribution defined downstream. Nicolas asked for passwordless access to NXCALS. It can be done only with kerberos. Markus suggested to look into the joblib library for simple parallel tasks.","title":"Information"},{"location":"notes_from_meetings/#points-to-be-followed-up_6","text":"MAD-X team looking into issue with IBS module. Pascal encountered issues with BOINC spool space. Being followed-up. Gianni/Frederik/Tobias to have a chat on status and evolution of DIST library.","title":"Points to be followed up"},{"location":"notes_from_meetings/#16-april-2021","text":"Present: R. Bruce, X. Buffat, D. Gamba, G. Iadarola, R. De Maria, J. Dilly, S. Joly, P. Elson, J. Farmer, A. Gerbenshagen, L. Giacomel, A. Latina, T. Persson, F. Soubelet, G. Sterbini, F. Van Der Veken.","title":"16 April 2021"},{"location":"notes_from_meetings/#information_3","text":"ABP HTCondor quota remapped on Tuesday 13 Apr: we now have a single computing group \u201cgroup_u_BE.ABP.NORMAL\u201d, which is organized in six e-groups associated to the sections batch-u-abp-cei (contains the old e-group: htcondor-u-ICE) batch-u-abp-hsl batch-u-abp-inc batch-u-abp-laf batch-u-abp-lno (contains old e-group: htcondor-u-SLAP) batch-u-abp-ndc (contains old e-group: LHC-COLL-LSF-users) Please use the new section e-groups to add new users. The old e-groups will be discontinued after moving the users to the respective section e-group. Discussion have started to prolong the exploitation of the cluster at INFN-CNAF (800 cores) beyond the end of the contract (Dec 2022). We could prolong its usage for three more years (paying the energy consumption) If some extra funding is available, a few more nodes could be added to compensate for expected \u201cageing\u201d. It is assumed that it will be used mostly for single-node jobs (up to 48 cores) due to end of support of the installed low-latency network (OK based on past experience). A survey is being run across the BE department to identify services provided by the IT department on which we rely for simulations and data analysis. The goal is to put in place a coordinated interface to IT and find synergies with other ATS departments. Some draft slides are discussed. Points that could be added: HDFS filesystem for data storage for spark-based analysis. Users would like a more complete and supported software stack (libraries, compilers, python, etc.) Openshift and webservices for documentation. Openstack, GitLab Licensed software: Mathematica, Matlab, CST, etc. Lxplus, Windows Terminal Services Information from Phil (CSS): A software stack based on LCG 100 is being prepared for usage in LXPLUS via CVMFS. A pip-installable NXCALS-pyspark bundle is being also considered. NXCALS can be accessed from outside CERN using an ssh tunnel. lxtunnel.cern.ch to be preferred to lxplus.cern.ch for this purpose (see recipe just updated by Joshua). Discussion on how to best reformat JAPC data for storage on parquet files: one possibility is to use the same serialization used at RDA level.","title":"Information"},{"location":"notes_from_meetings/#9-april-2021","text":"Present: R. Bruce, X. Buffat, R. De Maria, L. Deniau, J. Farmer, A. Gerbenshagen, L. Giacomel, P. Hermes, G. Iadarola, A. Latina, N. Mounet, T. Persson, F. Soubelet, G. Sterbini, F. Van Der Veken.","title":"9 April 2021"},{"location":"notes_from_meetings/#information_4","text":"Phil from CSS will join the next meeting. Applications are open for the upcoming CERN thematic School on Computing on \"Scientific Software for Heterogeneous Architectures\u201d. A guide on mounting EOS on mac has been prepared by Ilias, Guido and Joshua. More info also on this mattermost discussion . An Accelerating Python user's meeting will take place t2021 Q1 on 29 April. Topics: PyTimber, PyJapc, accwidgets, asyncio usage A pilot program was launched by IT and IPT to access commercial cloud services (Google, Amazon). A workshop will be held on Apr 28 to have a first overview. Information on how to install EOS on Ubuntu can be found in this docker file prepared by Guido. Frederik deployed a fixed version of SixDesk/DB in the AFS repository (compiles correctly). Issues with slicing of solenoid in MAD-X was fixed by Helmut. Nicolas and Markus found very convenient to use joblib for simple parallelization in the new Impedance Toolbox.","title":"Information"},{"location":"notes_from_meetings/#points-to-be-followed-up_7","text":"New e-groups for access to HTCondor have been created. One per section, with associated admin e-group. SL and CP members have been added to admin e-group for their sections We will proceed to the remapping of the ABP quota quota next week. The operation should be transparent as the old e-groups were inserted inside the new ones. We should gradually empty the old groups and move users to the new ones Guido prepared an \"ABP docker container\" based on Ubuntu with the typical tools (python installation, mad-x) and the capability of mounting EOS. An installation of HTCondor will also be added. An issue was identified in the usage of beam-beam in sixrtack with ions (see GitHub issue ). A workaround has been inserted in pymask to circumvent the problem. Frederik is now working on fixing the error ruotines for the mask. Should be ready in a few weeks. A ticket has been opened by external users on the IBS calculation for MAD-X in some special cases. Tobias will look into it together with Fanouria. Andrea is studying multibunch effects for the FCC-ee injector linac. He improved the modeling of the wakefields. Perhaps we could look into possible synergies with PyHEADTAIL.","title":"Points to be followed up"},{"location":"notes_from_meetings/#26-march-2021","text":"Present: R. Bruce, X. Buffat, R. De Maria, L. Deniau, F. Calier, J. Dilly, J. Farmer, S. Furuseh, D. Gamba, L. Giacomel, P. Hermes, M. Hofer, G. Iadarola, A. Latina, J. Molson, N. Mounet, T. Persson, K. Paraschou, T. Pieloni, F. Soubelet, G. Sterbini, F. Van Der Veken.","title":"26 March 2021"},{"location":"notes_from_meetings/#information_5","text":"JetBrains (the company developing PyCharm) is looking for early adopters at CERN for their new IDE for data analysis and prototyping machine learning models. An interesting reading: \u201cGood enough practices in scientific computing\" The Zenodo service allows getting DOI code for data and code, for referencing in publications. Tatiana introduced the EPFL project on FCC-ee code development. Nicolas had very satisfactory results speeding up pythoon code with numba parallel.","title":"Information"},{"location":"notes_from_meetings/#points-to-be-followed-up_8","text":"There are discussions ongoing on the future of PyTIMBER (see: https://wikis.cern.ch/display/NXCALS/Future+of+PyTimber ). Riccardo joined the meeting.The idea is to converge to a single Python interface for NXCALS, which is easy to install, exposes the pyspark features, and has high-level methods like pytimber. CSS plans to discontinue the Java Backport API at the end of Run 3. Pascal encountered a blocking issue installing SixDesk/SixDB. Frederik will provide some help. It is proposed to enable the long names by default in the sixtrack input generation of MAD-X (requires Sixtrack V). Some updates have been introduced in the SixTrack collimation features (to be pushed). A problem has been identified in Geant 4 (a bug report has been opened). Non-staff members of the collimation team have issues accessing the source of Fluka. Nicolas asked whether anybody has experience with the diagonalization of very large matrices. Riccardo faced the problem in the past and will provide the solution that he adopted.","title":"Points to be followed up"},{"location":"notes_from_meetings/#19-march-2021","text":"Present: F. Antoniou, F. Asvesta, H. Bartosik, R. Bruce, F. Calier, R. De Maria, L. Deniau, P. Elson, J. Farmer, S. Furuseth, J. Dilly, P. Hermes, D. Gamba, L. Giacomel, M. Hofer, G. Iadarola, A. Latina, N. Mounet, K. Paraschou, T. Persson, T. Pieloni, F. Soubelet, G. Sterbini, F. Van Der Veken.","title":"19 March 2021"},{"location":"notes_from_meetings/#information_6","text":"The TE-MPE team would like to store software for beam physics in CVMFS. We will propose to create a general \"beam-physics\" project in CVMFS where we will able to store also ABP software if needed. Even if the AFS phaseout has been put on hold, IT is still checking with users whether their project spaces could be moved to EOS. In general this works for data storage, but can create problems when using the filesystem as a working space for simulations (e.g in HTCondor). For this kind of usage we have the option to keep using AFS. Next events: GPU Technology Conference 2021 (GTC21) , free to attend.","title":"Information"},{"location":"notes_from_meetings/#points-related-to-css-services","text":"This page summarizes issues encountered with python packages stored in PyPI, which depend on packages available only in the acc-py repository. Although a workaround was identified it would be good to find a more flexible solution. The ongoing development to adapt PyJAPC to the needs of the injectors commissioning (to discontinue the usage of matlabMonitor) is using this repository . More info can be found in the slides shared by Guido . The main needs are: Ensuring \u201csynchronization\u201d of a list of devices properties Having a single centralized callback Saving automatically the data in a \u201creasonable\u201d format. Relying on pandas for the data manipulation is a reasonable choice. The candidate format for the data saving is parquet at present, but we could probably get in contact with the NXCALS team to see if they have any further suggestion. Feather and HDF5 could also be investigated. Answers to some of the questions in Guido's slides were provided by Phil after the meeting Q: Is it possible to install Acc-Py on non TN machines? Is there a container/docker image? A: Yes, there is an installer for the base distribution available. Please see https://wikis.cern.ch/display/ACCPY/Acc-Py+base#Acc-Pybase-UsingAcc-Pybase . If there are any issues, please contact acc-python-support@cern.ch for the installer. There is a prototype docker image, but we have not officially released it yet; we would happily engage with anybody who wanted to use the docker image to better understand the use cases and requirements. Q: Is there a better way to avoid blocking the exit of a Python application which uses the JVM? A: The next version of cmmnbuild-dep-manager includes a workaround for Java libraries that aren\u2019t correctly clearing up their non-daemon threads. This will be released over the coming days as cmmnbuild-dep-manager 2.8.0. Q: What is the best way to build documentation for packages with Acc-Py A: Please see https://wikis.cern.ch/display/ACCPY/Documentation for some tips. We would be very happy to improve our documentation service\u2019s documentation \ud83d\ude0a\u2026 please get in touch if something isn\u2019t clear or could be better phrased! Note that this service is only available inside the GPN \u2013 we don\u2019t currently plan to make the documentation available outside of the CERN network. For externally hosted documentation readthedocs is a good solution.","title":"Points related to CSS services"},{"location":"notes_from_meetings/#12-march-2021","text":"Present: X. Buffat, R. Bruce, F. Carlier, R. De Maria, J. Dilly, L. Deniau, J. Farmer, S. Furuseth, P. Hermes, M. Hofer, G. Iadarola, L. Giacomel, S. Kostoglou, A. Latina, J. Molson, N. Mounet, T. Persson, G. Sterbini.","title":"12 March 2021"},{"location":"notes_from_meetings/#information_7","text":"Phil from CSS will join the next meeting (19 March 2021) Talks on GPU applications at last Compute Accelerator Forum ( link ). Gianni is progressing with the development of the Xfield library. Sofia is continuing the development of pymask to cover simulations with ions. Pascal is looking into refurbishing the python tools used for collimation, to move to a pip-installable package. Something that might be useful: Guidelines for python packages are available on the ABP Computing website (including an example package with instructions). Git workflows can be used for the automatic deployment in PyPIC (see for example PyLHC ). Guido found an issue in the generation of the MAD-X beam-beam lenses in pymask (which does not affect the generation of the sixtrack models, unless matchings are done with beam-beam on). This has been fixed in the latest release. Laurent is working on non-linear normal forms. Andrea (and Raul) are porting CSR features form Placet 1 to Placet 2, improving the implementation in different ways. John and the AWAKE team solved an issue with their quasi-static PIC code. Riccardo added a little feature in cpymad to access the strengths generated by the correct module.","title":"Information"},{"location":"notes_from_meetings/#points-to-be-followed-up_9","text":"IT is moving Nag and Lahey compilers and libraries from afs to cvmfs. The compilers are indeed used to test sixtrack compilation. Joshua and Felix prepared an intro page on AccPy . Here they summarize also the issues that they faced with python packages hosted in the standard PyPI index, which depend AccPy hosted packages. Their workaround to make the package pip installable is described in the page o Definitely a general issue to be discussed with CSS. EPFL is working on code development for FCC-ee. Input on possible software requirements for future studies is welcome. MAD-X development (Tobias): Issue with equilibrium emittance calculation in the presence of tapering being investigated (for FCC-ee). Improving the slicing of RBEND (placing of slices for shared dipoles). Sondre finds extreme numerical convergence conditions (number of slices) in COMBI to resolve the tune shift. Roderick and team are working on identifying software tools for collimation studies for FCC-ee. In a few weeks we could have a first discussion on collimation requirements for the Xtrack library that is being developed. Discussion are starting in CSS on how to rationalize python tools to access NXCALS.","title":"Points to be followed up"},{"location":"notes_from_meetings/#5-march-2021","text":"Present: X. Buffat, R. Bruce, F. Carlier, L. Deniau, R. De Maria, J. W. Dilly, J. Farmer, M. Hofer, D. Gamba, P. Hermes, G. Iadarola, A. Latina, S. Joly, N. Mounet, M, Rognlien, G. Sterbini, T. Persson, F. Soubelet, F. Van Der Veken.","title":"5 March 2021"},{"location":"notes_from_meetings/#information_8","text":"New ABP Computing Sandbox space was created on GitLab (suggestion by Davide and Guido). Feel free to use it as incubator for new shared projects. Following discussion from last meeting: Guido prepared a simple guide on the use of docker containers (available here ) Nicolas prepared a small guide on how to install PyHEADTAIL on Mac using Anaconda (available here ) In SixTrack two bug-fixes related to collimation were introduced (see PR1 and PR2 ) Report from the last IT User Meeting (by Laurent). Agenda and material of the meeting can be found here . Particularly relevant for our scientific computing activitiies: AFS replacement is now stopped (since no available drop-in solution was found). The strategy will be reviewed at the end of Run-3 (Interesting review document published - https://cds.cern.ch/record/2750122 ). A working group is being setup to look into possible replacement of CentOS7 due to a change in support policy from RedHat. There is an upcoming OpenLab Technical workshop ( indico page ).","title":"Information"},{"location":"notes_from_meetings/#points-to-be-followed-up_10","text":"For MAD-X: The new MAD-X release is slowly converging. The build through GitHub actions started failing due to an upgrade of the compiler on the GitHub size (probably related to vectorization and data alignment). It might be related to the problems recently encountered by Riccardo (see issue ). For SixTrack: Sixtrack build with GCC 10 was failing due to multiple includes in DISTlib. A workaround has been introduced in sixtrack (see PR ), but proper fix needs to be introduced in DISTlib. Full crab cavity tilt required by Sofia is not really easy to implement, but could be handled through reference frame rotations in the user's side. Features to complement PyJAPC are being developed in the PyJapcScout repository on the ABP Computing sandbox. Josh and Felix are facing issue in the development of the OMC3 toolbox, having to combine packages from the standard PyPI and the acc-py package indices. They arranged a workaround and summarized the situation here -> to be discussed with Phil, to see whether better solutions can be identified.","title":"Points to be followed up"},{"location":"notes_from_meetings/#26-february-2021","text":"","title":"26 February 2021"},{"location":"notes_from_meetings/#seminar","text":"Our usual round table was replaced by a seminar by R. Brito Da Rocha from IT on \"Notebooks and Computing Workflows with IT services\". The slides and and the recording of his presentation are available on indico .","title":"Seminar"},{"location":"notes_from_meetings/#19-february-2021","text":"Present: R. Bruce, X. Buffat, L. Deniau, P. Elson, J. Farmer, P. Hermes, G. Iadarola, S. Kostoglou, A. Latina, N. Mounet, T. Persson, G. Sterbini.","title":"19 February 2021"},{"location":"notes_from_meetings/#information_9","text":"P. Elson, indicated as link person between BE-CSS and BE-ABP, joined the meeting. He has vast background on python solutions for scientific computing and has a leading role in the AccPy community. He could join our meetings ~once per month, to discuss CSS services. Two guides in abpcomputing website have been updated to cover Ubuntu 20.04: Local HTCondor installation (thanks Joschua & co.) OpenAFS installation (thanks Guido, Riccardo etc.) We have two new Mattermost channels : Six* (for sixtrack, sixdesk, and other things like that) and pymask. The change of PyPI index caused by the installation of the acc-py-pip-config package can be undone with pip uninstall. Thanks Phil! Sofia is working to extend pymask to work with ions. There will be an inrmediat MAD-X course on 22-23 March","title":"Information"},{"location":"notes_from_meetings/#points-to-be-followed-up_11","text":"Guido gave a presentation at the ABP-INC meeting on how to reverse sequences using cpymad (generation of beam 4). cpymad input from python seems to be slower than calling a madx script file. To be investigated. The issue with SixTrack V identified by the collimation team was traced back to wrong units used for the proton mass in one place (to be ported in the master branch). There was also an issue compiling Sixtrack in the presence of the dist module. For now this can be bypassed disabling the module (not needed for these studies). Tobias will follow it up. Issue with crabcavity in MAD-X : it has been fixed in the MAD part. The problem with PTC still needs to be understood (not urgent). The fix for the MAD side will be deployed with the next release. Group subscription in PyJAPC: Davide will start working on it soon, using examples for the PS provided by Alex. Phil provided useful suggestions to ease the implementation. Nicolas asked for suggestions to get PyHEADTAIL running on Mac (issues with compiling the Cython part). One possibility is to usee Anaconda: after the meeting Nicolas provided a simple recipe on the PyHEADTAIL wiki . The alternative is to use docker -> some information on how to get started has been prepared by Guido ( available on CodiMD , to be ported to the abpcomputing website).","title":"Points to be followed up"},{"location":"notes_from_meetings/#12-february-2021","text":"Present: R. Bruce, R. De Maria, J. Dilly, D. Gamba, A. Gerbershagen, P. Hermes, G. Iadarola, S. Joly, J. Farmer, A. Latina, L. Medina, N. Mounet, K. Paraschou, T. Persson, G. Sterbini, F. Van Der Veken.","title":"12 February  2021"},{"location":"notes_from_meetings/#information_10","text":"Phil Elson will be the CSS contact person for ABP News on GPU infrastructure from IT (e.g. GPUs for CI). More info ( here ) Discussion on necessary prerequisites to migrate ABP tools for the injectors to PyJAPC, to be followed up in collaboration with BE-CSS Need for group subscriptions (as available in MATLAB interface). Davide working on the development of this functionality Need to identify a common way of saving PyJAPC data on file (Guido investigated the possibility of using parquet files) Some extra information on how to access NXCALS using pytimber is available at: http://abpcomputing.web.cern.ch/guides/pytimber_nxcals/ Issues encountered with with Pytimber in SWAN: there are plans ot remove pytimber from SWAN and leave the installation to the user. News from BE-EA: some members of their team are already using cpymad. Their interface to mad-x (ApplePy) is being interfaced with the control system (CESAR).","title":"Information"},{"location":"notes_from_meetings/#points-to-be-followed-up_12","text":"Issue with CrabCavity map in MAD-X should be easy to solve. Fixing the PTC side is more involved but not urgent. Issue in the error routines in the lhcmask is expected to be addressed in 2-3 weeks. The collimation team had problems in compiling sixtrack, which could be solved only by removing the DIST block. Tobias is aware of this and will follow it up. No particular urgency, as this is not blocking. The collimation team is investigating different behaviors of the scattering routines between Sixtrack 4 and Sixtrack 5. Andrea is trying to align spacecharge and beambeam development in MAD-X from F. Schmidt and collaborators to the latest version of the code. Herry Randal is investigating speed issues of MAD-X compared to older versions. The issue seems to be related to OpenMP.","title":"Points to be followed up"},{"location":"notes_from_meetings/#5-february-2021","text":"Present: F. Asvesta, D. Banerjee, R. Bruce, X. Buffat, L. Deniau, R. De Maria, J. Farmer, S. Furuseth, A. Gerbershagen, L. Giacomel, P. Hermes, G. Iadarola, S. Kostoglou, J.B. Lallement, A. Latina, N. Mounet, T. Persson, G. Sterbini, F. Van Der Veken.","title":"5 February  2021"},{"location":"notes_from_meetings/#information_11","text":"Opened Mattermost channel Proposal (driven by BE-CSS) to create a structured interface between the ATS and IT managements, being discussed within ATS. Software developers from the BE-EA team joined the meeting. They will stay in contact for discussions related to mad-x and layout database. Riccardo and team might use their expertise with BEACH to understand discrepancies with respect to MAD-X. Tensor Processing Unit (TPU) made available experimentally by IT for machine learning applications (contact IT person: Ricardo Brito Da Roca) CALS was discontinued on Feb 1, replaced by NXCALS. Guido is working on a simple package to create trees of jobs. Andrea implemented long-range wakes in RFTrack for studying multibunch effects in FLASH. Synergy with PyHEADTAIL for parallelizing interpolations. Xavier implemented new feature in COMBI to have linearizeed beam-beam (for tests).","title":"Information"},{"location":"notes_from_meetings/#points-to-be-followed-up_13","text":"MAD-X issues to be followed up: Calculation of crabcavity R and T matrix not correct in MADX and PTC. (Required to include crab cavities in the HL-LHC sequence!). Related GitHub issue here Current master segfaults on a long hl-lhc script when compiled with 'make'. Related GitHub issue here Tobias is reviewing the behavior of the solenoid.","title":"Points to be followed up"},{"location":"notes_from_meetings/#29-january-2021","text":"Present: G. Iadarola, X. Buffat, L. Deniau, R. De Maria, J. Farmer, S. Furuseth, D. Gamba, S. Kostoglou, A. Latina, N. Nounet, K. Paraschou, G. Sterbini, F. Van Der Veken, T. Persson, F. Soubelet.","title":"29 January 2021"},{"location":"notes_from_meetings/#information_12","text":"Ilias and Guido prepared a page for the configuration of NXCALS (pyspark) on lxplus. It can be found here . The Timber web application does not work from outside the CERN GPN network. To solve this, Riccardo and Davide have recipes to redirect the web traffic to the CERN network. Their bash scripts have been made available here . Example SixDesk study prepared by Sofia and Guido. First thoughts on how to extract components of general interest. Davide and Andrea are working on benchmarking of cooling modeling tools. A thorough exploration of the parameter space is revealing quite educative, leading to several improvements in RFTrack. Tracking library development: refactoring of GPU memory management is ongoing (Riccardo). Davide is refactoring the MATLAB interface to the controls infrastructure (PyJAPC is now used under the hood). John and the AWAKE team are comparing different PIC solvers in very challenging scenarios with many plasma buckets. New release of MAD-X coming soon to fix a bug related to particle losses in PTC-Track (Tobias). The CERN gitlab service now allows Continuos Integration tests on different GPUs (more info on Mattermost GPU). Work on restructuring and extension of DELPHI Vlasov solver is ongoing (Nicolas). GitHub actions allow build tests on several different platform (being used by MAD-X team). We will have a IT seminar on \"Notebooks and Computing Workflows with IT services\" on 26 Feb ( indico page ).","title":"Information**"},{"location":"notes_from_meetings/#points-to-be-followed-up_14","text":"Reorganization of HTCondor e-groups (one e-group per section). Codes housekeeping page to be completed with missing info. Some of the tools from the OMC suite could be of general interest (e.g. harpy, machine imperfection management and correction). A first step could be to prepare a list of components that could be shared as individual packages, and start from the ones that are easier and more useful. An updated version of harpy should come within a few months. In MAD-X/PTC, found a bug related to particles having mass that is not a multiple of the electron or the proton mass. Piotr might be able to look into that. There is a problem with imperfection simulation in the mask. If anybody needs simulations with imperfections before this is solved should contact Frederik. COMBI team worried about \"unintended consequences\" of migrating repository to common space. Forking instead of transferring the project should be safe.","title":"Points to be followed up"},{"location":"notes_from_meetings/#22-january-2021","text":"(Kick-off meeting) Present: R. Bruce, R. De Maria, L. Deniau, G. Iadarola, J. Farmer, S. Furuseth, D. Gamba, L. Giacomel, A. Latina, J. Lettry, T. Persson, F. Van Der Veken.","title":"22 January 2021"},{"location":"notes_from_meetings/#points-to-be-followed-up_15","text":"Please review the list of objectives and activities and provide feedback by Fri 29 Jan (comments already received from Roderik and Jacques \u2192 added to the webpage) Feel free to use the abp-computing website to share any technical information/guideline of general interest (including those related to controls/operation tools). Anybody interested in attending the meetings can self-subscribe the invitation list using this link .","title":"Points to be followed up"},{"location":"webtodo/","text":"A todo list for this website We need a basic git guide,eg https://product.hubspot.com/blog/git-and-github-tutorial-for-beginners","title":"A todo list for this website"},{"location":"webtodo/#a-todo-list-for-this-website","text":"We need a basic git guide,eg https://product.hubspot.com/blog/git-and-github-tutorial-for-beginners","title":"A todo list for this website"},{"location":"codes/softwarelist_core/","text":"Core codes developed by ABP Optics design MAD-X MAD-NG cpymad Single-particle tracking Xsuite SixTrack SixDesk Sixtracklib Mask/pymask DistLib ( repo ) Frequency analysis harpy NAFFLIB Collective effects macroparticle tools PyHEADTAIL COMBI PyECLOUD PyPIC RFTrack Collective effect Vlasov solvers and tools DELPHI PySSD PyRADISE BimBim Impedance computation tools ImpedanceWake2D Tools for linear colliders and recirculating machines (ER-Linacs, Combiner Rings) Guinea Pig Placet Placet 2 MapClass Tools for luminosity modeling LumiMod CTE MBS Tools for hadron linacs PATH [TO BE FILLED!]","title":"Core codes developed by ABP"},{"location":"codes/softwarelist_core/#core-codes-developed-by-abp","text":"","title":"Core codes developed by ABP"},{"location":"codes/softwarelist_core/#optics-design","text":"MAD-X MAD-NG cpymad","title":"Optics design"},{"location":"codes/softwarelist_core/#single-particle-tracking","text":"Xsuite SixTrack SixDesk Sixtracklib Mask/pymask DistLib ( repo )","title":"Single-particle tracking"},{"location":"codes/softwarelist_core/#frequency-analysis","text":"harpy NAFFLIB","title":"Frequency analysis"},{"location":"codes/softwarelist_core/#collective-effects-macroparticle-tools","text":"PyHEADTAIL COMBI PyECLOUD PyPIC RFTrack","title":"Collective effects macroparticle tools"},{"location":"codes/softwarelist_core/#collective-effect-vlasov-solvers-and-tools","text":"DELPHI PySSD PyRADISE BimBim","title":"Collective effect Vlasov solvers and tools"},{"location":"codes/softwarelist_core/#impedance-computation-tools","text":"ImpedanceWake2D","title":"Impedance computation tools"},{"location":"codes/softwarelist_core/#tools-for-linear-colliders-and-recirculating-machines-er-linacs-combiner-rings","text":"Guinea Pig Placet Placet 2 MapClass","title":"Tools for linear colliders and recirculating machines (ER-Linacs, Combiner Rings)"},{"location":"codes/softwarelist_core/#tools-for-luminosity-modeling","text":"LumiMod CTE MBS","title":"Tools for luminosity modeling"},{"location":"codes/softwarelist_core/#tools-for-hadron-linacs","text":"PATH [TO BE FILLED!]","title":"Tools for hadron linacs"},{"location":"codes/softwarelist_external/","text":"Beam physics software tools - external ABCI ACE3P CST particle studio GdFiDl HFSS MOSES Ninja Onix PyORBIT IBSimu pytimber","title":"Beam physics software tools - external"},{"location":"codes/softwarelist_external/#beam-physics-software-tools-external","text":"ABCI ACE3P CST particle studio GdFiDl HFSS MOSES Ninja Onix PyORBIT IBSimu pytimber","title":"Beam physics software tools - external"},{"location":"codes/softwarelist_other/","text":"New developments, R&D and legacy tools TLWall PyRADISE PHOTON pyoptics TRAIN SIRE BimBim Fastion Footprint viewer LHC Online Model PageStore SUSSIX","title":"New developments, R&D and legacy tools"},{"location":"codes/softwarelist_other/#new-developments-rd-and-legacy-tools","text":"TLWall PyRADISE PHOTON pyoptics TRAIN SIRE BimBim Fastion Footprint viewer LHC Online Model PageStore SUSSIX","title":"New developments, R&amp;D and legacy tools"},{"location":"codes/codes_pages/ABCI/","text":"ABCI Short description ABCI (Azimuthal Beam Cavity Interaction) is a code used for impedance and wakefield calculations created and maintained by Yong Ho Chin (KEK, Japan). It is a time domain solver of electromagnetic fields when a bunched beam goes through an axi-symmetric structure (on or off axis). An arbitrary charge distribution can be defined by the user (default=Gaussian). Web resources Home page and source code: http://abci.kek.jp/abci.htm Technical information Operating systems: Linux and Windows (works at least on XP and 7) Other information Developed by: Y. H. Chin License: freeware Contact persons at CERN: Benoit Salvant Being actively developed and supported: Yes","title":"ABCI"},{"location":"codes/codes_pages/ABCI/#abci","text":"","title":"ABCI"},{"location":"codes/codes_pages/ABCI/#short-description","text":"ABCI (Azimuthal Beam Cavity Interaction) is a code used for impedance and wakefield calculations created and maintained by Yong Ho Chin (KEK, Japan). It is a time domain solver of electromagnetic fields when a bunched beam goes through an axi-symmetric structure (on or off axis). An arbitrary charge distribution can be defined by the user (default=Gaussian).","title":"Short description"},{"location":"codes/codes_pages/ABCI/#web-resources","text":"Home page and source code: http://abci.kek.jp/abci.htm","title":"Web resources"},{"location":"codes/codes_pages/ABCI/#technical-information","text":"Operating systems: Linux and Windows (works at least on XP and 7)","title":"Technical information"},{"location":"codes/codes_pages/ABCI/#other-information","text":"Developed by: Y. H. Chin License: freeware Contact persons at CERN: Benoit Salvant Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/ACE3P/","text":"ACE3P Short description ACE3P is a complete package for electrodynamics simulations in accelerator components, including eigenmode (Omega3P), S-parameters (S3P), Time domain wakefields etc. (T3P), Particle in cell injectors etc. (Pic3P), Tracking charged particles in eigenmode fields for multipacting and dark current (Track3P), and multi-physics for detuning due to thermal expansion from wall losses and mechanical forces (Temp3P). The solvers run on the external computer clusters at NERSC . For post-processing, the package uses ParaView , which can be installed through the package manager on most Linux systems, in addition to the \"acdtool\" post-processor. For mesh generation, the package uses Trellis or Cubit - please contact the contact persons for more information. Trellis can be installed on most Linux systems as a .rpm (Fedora, Scientific Linux and other RedHat based systems) or .deb (Debian, Ubuntu, etc.), and can also be installed on most Windows and Mac systems. Web resources ACE3P Homepage at SLAC Technical information Programming Languages used for implementation: C++ (mainly) Parallelization strategy: MPI Operating systems: Solvers: Linux ACDTOOL pre- and post-processing: Linux Paraview: Any modern desktop OS Trellis/CUBIT: Any modern desktop OS Other prerequisites: You need a NERSC account in order to run the solvers. Other information Developed by: The SLAC ACD (advanced computations) group License: Unclear, source code private to SLAC Contact persons: https://phonebook.cern.ch/phonebook/#search/?query=Kyrre+Ness+Sjobaek+BE-ABP-HSS (ABP) and Nikolay Schwerg (in general at CERN) Being actively developed and supported: Yes Main.KyrreSjobak - 2016-11-25","title":"ACE3P"},{"location":"codes/codes_pages/ACE3P/#ace3p","text":"","title":"ACE3P"},{"location":"codes/codes_pages/ACE3P/#short-description","text":"ACE3P is a complete package for electrodynamics simulations in accelerator components, including eigenmode (Omega3P), S-parameters (S3P), Time domain wakefields etc. (T3P), Particle in cell injectors etc. (Pic3P), Tracking charged particles in eigenmode fields for multipacting and dark current (Track3P), and multi-physics for detuning due to thermal expansion from wall losses and mechanical forces (Temp3P). The solvers run on the external computer clusters at NERSC . For post-processing, the package uses ParaView , which can be installed through the package manager on most Linux systems, in addition to the \"acdtool\" post-processor. For mesh generation, the package uses Trellis or Cubit - please contact the contact persons for more information. Trellis can be installed on most Linux systems as a .rpm (Fedora, Scientific Linux and other RedHat based systems) or .deb (Debian, Ubuntu, etc.), and can also be installed on most Windows and Mac systems.","title":"Short description"},{"location":"codes/codes_pages/ACE3P/#web-resources","text":"ACE3P Homepage at SLAC","title":"Web resources"},{"location":"codes/codes_pages/ACE3P/#technical-information","text":"Programming Languages used for implementation: C++ (mainly) Parallelization strategy: MPI Operating systems: Solvers: Linux ACDTOOL pre- and post-processing: Linux Paraview: Any modern desktop OS Trellis/CUBIT: Any modern desktop OS Other prerequisites: You need a NERSC account in order to run the solvers.","title":"Technical information"},{"location":"codes/codes_pages/ACE3P/#other-information","text":"Developed by: The SLAC ACD (advanced computations) group License: Unclear, source code private to SLAC Contact persons: https://phonebook.cern.ch/phonebook/#search/?query=Kyrre+Ness+Sjobaek+BE-ABP-HSS (ABP) and Nikolay Schwerg (in general at CERN) Being actively developed and supported: Yes Main.KyrreSjobak - 2016-11-25","title":"Other information"},{"location":"codes/codes_pages/BimBim/","text":"BimBim (Beam-Beam and IMpedance) Short description Semi-analytical derivation of the coherent modes of oscillation based on the derivation and diagonalisation of the coherent one turn map for multiple bunches of one or two beams including the effect of the lattice (Q, Q' and Q''), the transverse feedback, head-on beam-beam interaction with a crossing angle, long-range beam-beam interactions and the transverse dipolar and quadrupolar impedance. Web resources Sources CWG presentation Technical information Programming Languages used for implementation: Python 2.6 or higher Operating systems: tested exclusivey on Linux (Ubuntu 12.04 and SLC 5) Other prerequisites: Libraries: numpy, scipy Other informations Developed by : CERN License : CERN Copyright Contact persons : Xavier Buffat","title":"BimBim (Beam-Beam and IMpedance)"},{"location":"codes/codes_pages/BimBim/#bimbim-beam-beam-and-impedance","text":"","title":"BimBim (Beam-Beam and IMpedance)"},{"location":"codes/codes_pages/BimBim/#short-description","text":"Semi-analytical derivation of the coherent modes of oscillation based on the derivation and diagonalisation of the coherent one turn map for multiple bunches of one or two beams including the effect of the lattice (Q, Q' and Q''), the transverse feedback, head-on beam-beam interaction with a crossing angle, long-range beam-beam interactions and the transverse dipolar and quadrupolar impedance.","title":"Short description"},{"location":"codes/codes_pages/BimBim/#web-resources","text":"Sources CWG presentation","title":"Web resources"},{"location":"codes/codes_pages/BimBim/#technical-information","text":"Programming Languages used for implementation: Python 2.6 or higher Operating systems: tested exclusivey on Linux (Ubuntu 12.04 and SLC 5) Other prerequisites: Libraries: numpy, scipy","title":"Technical information"},{"location":"codes/codes_pages/BimBim/#other-informations","text":"Developed by : CERN License : CERN Copyright Contact persons : Xavier Buffat","title":"Other informations"},{"location":"codes/codes_pages/COMBI/","text":"COMBI (COherent Multibunch Beam-beam Interactions) Short description COMBI stands for COherent Multibunch Beam-beam Interaction. The code simulates interactions of two counter rotating beams. Several \u201cactions\u201d can be included in the simulation: linear transport, head-on and long-range beam-beam interactions, noise sources, collimators, impedances, linear detuning, transverse feedback, synctrotron radiation. Web resources Sources Technical information Programming Languages used for implementation: FORTRAN, C and C++ Operating systems: tested exclusivey on Linux (Ubuntu 12.04,18.04, SLC 5 and CENTOS7) Other prerequisites: Libraries: fftw, GSL Other informations Developed by : CERN License : CERN Copyright Contact persons : Xavier Buffat","title":"COMBI (COherent Multibunch Beam-beam Interactions)"},{"location":"codes/codes_pages/COMBI/#combi-coherent-multibunch-beam-beam-interactions","text":"","title":"COMBI (COherent Multibunch Beam-beam Interactions)"},{"location":"codes/codes_pages/COMBI/#short-description","text":"COMBI stands for COherent Multibunch Beam-beam Interaction. The code simulates interactions of two counter rotating beams. Several \u201cactions\u201d can be included in the simulation: linear transport, head-on and long-range beam-beam interactions, noise sources, collimators, impedances, linear detuning, transverse feedback, synctrotron radiation.","title":"Short description"},{"location":"codes/codes_pages/COMBI/#web-resources","text":"Sources","title":"Web resources"},{"location":"codes/codes_pages/COMBI/#technical-information","text":"Programming Languages used for implementation: FORTRAN, C and C++ Operating systems: tested exclusivey on Linux (Ubuntu 12.04,18.04, SLC 5 and CENTOS7) Other prerequisites: Libraries: fftw, GSL","title":"Technical information"},{"location":"codes/codes_pages/COMBI/#other-informations","text":"Developed by : CERN License : CERN Copyright Contact persons : Xavier Buffat","title":"Other informations"},{"location":"codes/codes_pages/CSTParticleStudio/","text":"CST Particle Studio Short description 3D electromagnetic simulations with and without exciting source beam (wakefield solver, eigenmode solver, time domain solver, frequency domain solver) Web resources Installer: installation through CMF or from \\\\cern.ch\\DFS\\Applications\\CST Product webpage: https://www.cst.com/ Technical information Parallelization strategy: MPI (requires license) and multithreading (\"for free\") Operating systems: Windows and Linux (optimized so far primarily for Windows) Other prerequisites: requires valid viewer, solver and acceleration licenses can run on standard PC for small scale study, but requires dedicated servers for large scale. Other information License: commercial license Contact persons: Benoit Salvant, Monika Balk (at CST), and support line via tickets Being actively developed and supported: Yes, by the company","title":"CST Particle Studio"},{"location":"codes/codes_pages/CSTParticleStudio/#cst-particle-studio","text":"","title":"CST Particle Studio"},{"location":"codes/codes_pages/CSTParticleStudio/#short-description","text":"3D electromagnetic simulations with and without exciting source beam (wakefield solver, eigenmode solver, time domain solver, frequency domain solver)","title":"Short description"},{"location":"codes/codes_pages/CSTParticleStudio/#web-resources","text":"Installer: installation through CMF or from \\\\cern.ch\\DFS\\Applications\\CST Product webpage: https://www.cst.com/","title":"Web resources"},{"location":"codes/codes_pages/CSTParticleStudio/#technical-information","text":"Parallelization strategy: MPI (requires license) and multithreading (\"for free\") Operating systems: Windows and Linux (optimized so far primarily for Windows) Other prerequisites: requires valid viewer, solver and acceleration licenses can run on standard PC for small scale study, but requires dedicated servers for large scale.","title":"Technical information"},{"location":"codes/codes_pages/CSTParticleStudio/#other-information","text":"License: commercial license Contact persons: Benoit Salvant, Monika Balk (at CST), and support line via tickets Being actively developed and supported: Yes, by the company","title":"Other information"},{"location":"codes/codes_pages/DELPHI/","text":"DELPHI Short description DELPHI (Discrete Expansion over Laguerre Polynomials and HeadtaIl modes) allows to evaluate the transverse beam stability wrt. the machine impedance. It is a semi-analytical Vlasov solver which allows to perform fast scans overs parameters such as chromaticity, bunch intensity, damper gain... Web resources Source code: GitLab Code presentation: PDF presentation by N.Mounet Technical information Programming Languages used for implementation: C++, Python Operating systems: Linux (SLC5, Ubuntu 12.04+) Other prerequisites: Python 2.7 NumPy, SciPy (installation of Anaconda2 recommended) Other information Developed by: N.Mounet, N.Biancacci, D.Amorim License: CERN Copyright Contact persons: N.Mounet Being actively developed and supported: Yes","title":"DELPHI"},{"location":"codes/codes_pages/DELPHI/#delphi","text":"","title":"DELPHI"},{"location":"codes/codes_pages/DELPHI/#short-description","text":"DELPHI (Discrete Expansion over Laguerre Polynomials and HeadtaIl modes) allows to evaluate the transverse beam stability wrt. the machine impedance. It is a semi-analytical Vlasov solver which allows to perform fast scans overs parameters such as chromaticity, bunch intensity, damper gain...","title":"Short description"},{"location":"codes/codes_pages/DELPHI/#web-resources","text":"Source code: GitLab Code presentation: PDF presentation by N.Mounet","title":"Web resources"},{"location":"codes/codes_pages/DELPHI/#technical-information","text":"Programming Languages used for implementation: C++, Python Operating systems: Linux (SLC5, Ubuntu 12.04+) Other prerequisites: Python 2.7 NumPy, SciPy (installation of Anaconda2 recommended)","title":"Technical information"},{"location":"codes/codes_pages/DELPHI/#other-information","text":"Developed by: N.Mounet, N.Biancacci, D.Amorim License: CERN Copyright Contact persons: N.Mounet Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/Fastion/","text":"FASTION Short description FASTION is a 2D macro-particle simulation code for modelling the fast beam-ion instability in electron machines. Web resources Source code: https://github.com/lmether/FASTION Technical information Programming Languages used for implementation: Mainly C, some routines implemented in FORTRAN. Parallelization strategy: None Operating systems: Linux (experience on Ubuntu 12.04 or more recent, and SLC 5 or more recent) Other prerequisites: FFTW Other information Developed by: Giovanni Rumolo et al. License: GPLv3 Contact persons: Lotta Mether, Giovanni Rumolo Being actively developed and supported: Yes","title":"FASTION"},{"location":"codes/codes_pages/Fastion/#fastion","text":"","title":"FASTION"},{"location":"codes/codes_pages/Fastion/#short-description","text":"FASTION is a 2D macro-particle simulation code for modelling the fast beam-ion instability in electron machines.","title":"Short description"},{"location":"codes/codes_pages/Fastion/#web-resources","text":"Source code: https://github.com/lmether/FASTION","title":"Web resources"},{"location":"codes/codes_pages/Fastion/#technical-information","text":"Programming Languages used for implementation: Mainly C, some routines implemented in FORTRAN. Parallelization strategy: None Operating systems: Linux (experience on Ubuntu 12.04 or more recent, and SLC 5 or more recent) Other prerequisites: FFTW","title":"Technical information"},{"location":"codes/codes_pages/Fastion/#other-information","text":"Developed by: Giovanni Rumolo et al. License: GPLv3 Contact persons: Lotta Mether, Giovanni Rumolo Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/FootprintViewer/","text":"Online Footprint Viewer Short description The knowledge of the machine optics and beam parameters are extracted from the LSA and logging database to construct a (thin lens) MAD-X model of the machine including the lattice non-linearities (e.g. Landau octupoles) and beam-beam interactions. The tune footprints of PACMAN bunches are computed in parallel on a remote server, based on tracking with MAD-X. The footprints are then displayed online in the LHC control room. Web resources Developer's wiki Technical information Programming Languages used for implementation: MAD-X interfaced with Java (JMAD) Operating systems: SLC 5 Other informations Developed by : CERN License : CERN Copyright Contact persons : Xavier Buffat","title":"Online Footprint Viewer"},{"location":"codes/codes_pages/FootprintViewer/#online-footprint-viewer","text":"","title":"Online Footprint Viewer"},{"location":"codes/codes_pages/FootprintViewer/#short-description","text":"The knowledge of the machine optics and beam parameters are extracted from the LSA and logging database to construct a (thin lens) MAD-X model of the machine including the lattice non-linearities (e.g. Landau octupoles) and beam-beam interactions. The tune footprints of PACMAN bunches are computed in parallel on a remote server, based on tracking with MAD-X. The footprints are then displayed online in the LHC control room.","title":"Short description"},{"location":"codes/codes_pages/FootprintViewer/#web-resources","text":"Developer's wiki","title":"Web resources"},{"location":"codes/codes_pages/FootprintViewer/#technical-information","text":"Programming Languages used for implementation: MAD-X interfaced with Java (JMAD) Operating systems: SLC 5","title":"Technical information"},{"location":"codes/codes_pages/FootprintViewer/#other-informations","text":"Developed by : CERN License : CERN Copyright Contact persons : Xavier Buffat","title":"Other informations"},{"location":"codes/codes_pages/GdFiDl/","text":"GdFiDl Short description Commercial 3D electromagnetic solver for wakefield and impedance. Web resources [Link web resources available for your software. For example:] Web page: http://www.gdfidl.de/ Technical information Parallelization strategy: runs on dedicated servers Operating systems: UNIX-like systems Other prerequisites: Other information Developed by: Warner Bruns License: site-wide license purchased by CERN Contact persons: Benoit, Salvant, Alexej Grudiev (BE-RF) Being actively developed and supported: Yes","title":"GdFiDl"},{"location":"codes/codes_pages/GdFiDl/#gdfidl","text":"","title":"GdFiDl"},{"location":"codes/codes_pages/GdFiDl/#short-description","text":"Commercial 3D electromagnetic solver for wakefield and impedance.","title":"Short description"},{"location":"codes/codes_pages/GdFiDl/#web-resources","text":"[Link web resources available for your software. For example:] Web page: http://www.gdfidl.de/","title":"Web resources"},{"location":"codes/codes_pages/GdFiDl/#technical-information","text":"Parallelization strategy: runs on dedicated servers Operating systems: UNIX-like systems Other prerequisites:","title":"Technical information"},{"location":"codes/codes_pages/GdFiDl/#other-information","text":"Developed by: Warner Bruns License: site-wide license purchased by CERN Contact persons: Benoit, Salvant, Alexej Grudiev (BE-RF) Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/Guinea-Pig/","text":"GUINEA-PIG Short description GUINEA-PIG++ (Generator of Unwanted Interactions for Numerical Experiment Analysis - Program Interfaced to GEANT) simulates the beam-beam interaction in electron-positron linear colliders. It uses a strong-strong model of the colliding beams and includes the emission of beamstrahlung, production of incoherent and coherent pairs as well as hadronic background. Web resources Source code: https://gitlab.cern.ch/clic-software/guinea-pig Documentation The appendix of Daniel's PhD thesis: http://flash.desy.de/sites2009/site_vuvfel/content/e403/e1644/e1314/e1316/infoboxContent1932/tesla1997-08.pdf Technical information [Provide the following information] Programming Languages used for implementation: C / C++ Parallelization strategy: no parallelism Operating systems: Linux, MacOSX, Cygwin Other prerequisites: fftw Other information Developed by: Daniel Schulte et al. License: CERN Licence Contact persons: Daniel Schulte Being actively developed and supported: Yes","title":"GUINEA-PIG"},{"location":"codes/codes_pages/Guinea-Pig/#guinea-pig","text":"","title":"GUINEA-PIG"},{"location":"codes/codes_pages/Guinea-Pig/#short-description","text":"GUINEA-PIG++ (Generator of Unwanted Interactions for Numerical Experiment Analysis - Program Interfaced to GEANT) simulates the beam-beam interaction in electron-positron linear colliders. It uses a strong-strong model of the colliding beams and includes the emission of beamstrahlung, production of incoherent and coherent pairs as well as hadronic background.","title":"Short description"},{"location":"codes/codes_pages/Guinea-Pig/#web-resources","text":"Source code: https://gitlab.cern.ch/clic-software/guinea-pig Documentation The appendix of Daniel's PhD thesis: http://flash.desy.de/sites2009/site_vuvfel/content/e403/e1644/e1314/e1316/infoboxContent1932/tesla1997-08.pdf Technical information [Provide the following information] Programming Languages used for implementation: C / C++ Parallelization strategy: no parallelism Operating systems: Linux, MacOSX, Cygwin Other prerequisites: fftw","title":"Web resources"},{"location":"codes/codes_pages/Guinea-Pig/#other-information","text":"Developed by: Daniel Schulte et al. License: CERN Licence Contact persons: Daniel Schulte Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/HFSS/","text":"Ansys HFSS Short description ANSYS Electronics Desktop (formerly known as HFSS) solves Maxwell's equations in RF / microwave structures with the absence of a particle beam. It can solve problems of four types: Modal, Terminal, Transient, and Eigenmode. An example of a typical task for HFSS is finding electromagnetic modes and their properties in a closed RF cavity (Eigenmode solver). Another example is finding S-parameters of an RF system (S11, S21, etc) at a frequency specified by the user (Modal solver). HFSS is often compared to CST Microwave Studio. The two codes use two fundamentally different ways to solve Maxwell's equations, hence an agreement between them usually means reliability of the results. Web resources [Link web resources available for your software. For example:] Installer: available on CMF Wiki pages: http://www.ansys.com/Products/Electronics/ANSYS-HFSS Technical information [Provide the following information] Programming Languages used for implementation: commercial software Parallelization strategy: Operating systems: windows and Linux Other prerequisites: requires license (bought by CERN) Other information Developed by: License: limited number of licenses Contact persons: Sergey Arsenyev (ABP-LAT) Being actively developed and supported: Yes","title":"Ansys HFSS"},{"location":"codes/codes_pages/HFSS/#ansys-hfss","text":"","title":"Ansys HFSS"},{"location":"codes/codes_pages/HFSS/#short-description","text":"ANSYS Electronics Desktop (formerly known as HFSS) solves Maxwell's equations in RF / microwave structures with the absence of a particle beam. It can solve problems of four types: Modal, Terminal, Transient, and Eigenmode. An example of a typical task for HFSS is finding electromagnetic modes and their properties in a closed RF cavity (Eigenmode solver). Another example is finding S-parameters of an RF system (S11, S21, etc) at a frequency specified by the user (Modal solver). HFSS is often compared to CST Microwave Studio. The two codes use two fundamentally different ways to solve Maxwell's equations, hence an agreement between them usually means reliability of the results.","title":"Short description"},{"location":"codes/codes_pages/HFSS/#web-resources","text":"[Link web resources available for your software. For example:] Installer: available on CMF Wiki pages: http://www.ansys.com/Products/Electronics/ANSYS-HFSS","title":"Web resources"},{"location":"codes/codes_pages/HFSS/#technical-information","text":"[Provide the following information] Programming Languages used for implementation: commercial software Parallelization strategy: Operating systems: windows and Linux Other prerequisites: requires license (bought by CERN)","title":"Technical information"},{"location":"codes/codes_pages/HFSS/#other-information","text":"Developed by: License: limited number of licenses Contact persons: Sergey Arsenyev (ABP-LAT) Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/IBSimu/","text":"IBSimu Short description Ion Beam Simulator or IBSimu is an ion optical computer simulation package for ion optics, plasma extraction and space charge dominated ion beam transport using Vlasov iteration. Web resources Source code: http://ibsimu.sourceforge.net/download.html Wiki pages: http://ibsimu.sourceforge.net/ Technical information Programming Languages used for implementation: C++ Parallelization strategy: none Operating systems: Linux and Windows Other prerequisites: Third party libraries, please see http://ibsimu.sourceforge.net/installation.html Other information Developed by: Taneli Kalvas (University of Jyv\u00e4skyl\u00e4, Finland) License: GNU General Public Licence Contact persons: Jacques Lettry Being actively developed and supported: Yes","title":"IBSimu"},{"location":"codes/codes_pages/IBSimu/#ibsimu","text":"","title":"IBSimu"},{"location":"codes/codes_pages/IBSimu/#short-description","text":"Ion Beam Simulator or IBSimu is an ion optical computer simulation package for ion optics, plasma extraction and space charge dominated ion beam transport using Vlasov iteration.","title":"Short description"},{"location":"codes/codes_pages/IBSimu/#web-resources","text":"Source code: http://ibsimu.sourceforge.net/download.html Wiki pages: http://ibsimu.sourceforge.net/","title":"Web resources"},{"location":"codes/codes_pages/IBSimu/#technical-information","text":"Programming Languages used for implementation: C++ Parallelization strategy: none Operating systems: Linux and Windows Other prerequisites: Third party libraries, please see http://ibsimu.sourceforge.net/installation.html","title":"Technical information"},{"location":"codes/codes_pages/IBSimu/#other-information","text":"Developed by: Taneli Kalvas (University of Jyv\u00e4skyl\u00e4, Finland) License: GNU General Public Licence Contact persons: Jacques Lettry Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/ImpedanceWake2D/","text":"ImpedanceWake2D Short description ImpedanceWake2D (IW2D) compute the resistive wall impedance and/or the wake functions for multilayered flat, cylindrical or elliptical structures. It assumes an infinitely long structure (2D). Web resources Source code: GitLab repository Technical information Programming Languages used for implementation: C++, Python Operating systems: Linux: SLC5, Ubuntu 12.04+ Other prerequisites: GMP, MPFR, ALGLIB (provided in the repository) g++ and gcc compilers Python 2.7 NumPy, SciPy (installation of Anaconda2 recommended) Other information Developed by: N.Mounet, N.Biancacci, D.Amorim License: CERN Copyright Contact persons: N.Mounet Being actively developed and supported: Yes","title":"ImpedanceWake2D"},{"location":"codes/codes_pages/ImpedanceWake2D/#impedancewake2d","text":"","title":"ImpedanceWake2D"},{"location":"codes/codes_pages/ImpedanceWake2D/#short-description","text":"ImpedanceWake2D (IW2D) compute the resistive wall impedance and/or the wake functions for multilayered flat, cylindrical or elliptical structures. It assumes an infinitely long structure (2D).","title":"Short description"},{"location":"codes/codes_pages/ImpedanceWake2D/#web-resources","text":"Source code: GitLab repository","title":"Web resources"},{"location":"codes/codes_pages/ImpedanceWake2D/#technical-information","text":"Programming Languages used for implementation: C++, Python Operating systems: Linux: SLC5, Ubuntu 12.04+ Other prerequisites: GMP, MPFR, ALGLIB (provided in the repository) g++ and gcc compilers Python 2.7 NumPy, SciPy (installation of Anaconda2 recommended)","title":"Technical information"},{"location":"codes/codes_pages/ImpedanceWake2D/#other-information","text":"Developed by: N.Mounet, N.Biancacci, D.Amorim License: CERN Copyright Contact persons: N.Mounet Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/LHCOnlineModel/","text":"LHCOnlineModel Short description The beam based model, aka online model, extends the (ideal) LHC accelerator model by including measured parameters of the machine. Orbit Optics (beta beating) Applied corrections (tune, orbit corrector magnets) Powering errors Magnetic field deviations (new implementation of WISE as Java app) Alignment errors Such model is more accurate and it provides higher predictive power. It allows calculating, for example, Feed-down effects due to imperfect orbit Modification of global parameters due to beta function beating or alignment errors The project aims at providing tools that allow Easy access to more accurate and up-to-date beam parameters. The principal example is the aperture meter application that displays available aperture along the machine taking into account the measured orbit and beta beating. Simulating more accurately the beam behavior upon a knob or parameter change. Easy comparison between the model and the available measurements. Facilitating data analysis throughout all different machine configurations hopefully lets understand the sources of the discrepancies and leads to even more accurate machine model. Web resources Web page: http://lhcmodel.web.cern.ch/lhcmodel/ Source codes: https://svnweb.cern.ch/cern/wsvn/lhcmodel https://svnweb.cern.ch/cern/wsvn/acc-co/trunk/accsoft/om/ https://svnweb.cern.ch/cern/wsvn/acc-co/trunk/lhc/lhc-model-extractor/ https://svnweb.cern.ch/cern/wsvn/acc-co/trunk/lhc/lhc-model-wise-errorestimators/ https://svnweb.cern.ch/cern/wsvn/acc-co/trunk/lhc/lhc-model-wise-ui/ https://svnweb.cern.ch/cern/wsvn/acc-co/trunk/lhc/lhc-model-fidel-extractor/ Wiki pages: https://wikis.cern.ch/display/OnlineModel/Home Technical information [Provide the following information] Programming Languages used for implementation: Java Python Parallelization strategy: None Operating systems: Linux, inside cern only, and in some cases on technical network only Other prerequisites: Other information Developed by: [Piotr Skowronski] License: Default CERN Contact persons: Piotr Skowronski, Tobias Persson Being actively developed and supported: Yes","title":"LHCOnlineModel"},{"location":"codes/codes_pages/LHCOnlineModel/#lhconlinemodel","text":"","title":"LHCOnlineModel"},{"location":"codes/codes_pages/LHCOnlineModel/#short-description","text":"The beam based model, aka online model, extends the (ideal) LHC accelerator model by including measured parameters of the machine. Orbit Optics (beta beating) Applied corrections (tune, orbit corrector magnets) Powering errors Magnetic field deviations (new implementation of WISE as Java app) Alignment errors Such model is more accurate and it provides higher predictive power. It allows calculating, for example, Feed-down effects due to imperfect orbit Modification of global parameters due to beta function beating or alignment errors The project aims at providing tools that allow Easy access to more accurate and up-to-date beam parameters. The principal example is the aperture meter application that displays available aperture along the machine taking into account the measured orbit and beta beating. Simulating more accurately the beam behavior upon a knob or parameter change. Easy comparison between the model and the available measurements. Facilitating data analysis throughout all different machine configurations hopefully lets understand the sources of the discrepancies and leads to even more accurate machine model.","title":"Short description"},{"location":"codes/codes_pages/LHCOnlineModel/#web-resources","text":"Web page: http://lhcmodel.web.cern.ch/lhcmodel/ Source codes: https://svnweb.cern.ch/cern/wsvn/lhcmodel https://svnweb.cern.ch/cern/wsvn/acc-co/trunk/accsoft/om/ https://svnweb.cern.ch/cern/wsvn/acc-co/trunk/lhc/lhc-model-extractor/ https://svnweb.cern.ch/cern/wsvn/acc-co/trunk/lhc/lhc-model-wise-errorestimators/ https://svnweb.cern.ch/cern/wsvn/acc-co/trunk/lhc/lhc-model-wise-ui/ https://svnweb.cern.ch/cern/wsvn/acc-co/trunk/lhc/lhc-model-fidel-extractor/ Wiki pages: https://wikis.cern.ch/display/OnlineModel/Home","title":"Web resources"},{"location":"codes/codes_pages/LHCOnlineModel/#technical-information","text":"[Provide the following information] Programming Languages used for implementation: Java Python Parallelization strategy: None Operating systems: Linux, inside cern only, and in some cases on technical network only Other prerequisites:","title":"Technical information"},{"location":"codes/codes_pages/LHCOnlineModel/#other-information","text":"Developed by: [Piotr Skowronski] License: Default CERN Contact persons: Piotr Skowronski, Tobias Persson Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/MOSES/","text":"MOSES Short description MOSES (MOde-coupling Single bunch instabilities in an Electron Storage ring) is a program, which computes complex coherent betatron tune shifts as a function of the bunch current for a Gaussian beam interacting with a resonator impedance. Web resources Source code and home page: http://abci.kek.jp/moses.htm Technical information Operating systems: Windows (works on 7) and Linux (not tested) Other prerequisites: e.g. libraries needed to install/use your software e.g. libraries needed to install/use your software Special hardware needs: e.g. GPU, resources for multithreading/multiprocessing. Special hardware needs: e.g. GPU, resources for multithreading/multiprocessing. Other information Developed by: [Name of developers] License: [Licence policy] Contact persons: [Contact persons for the code] Being actively developed and supported: [Yes/No]","title":"MOSES"},{"location":"codes/codes_pages/MOSES/#moses","text":"","title":"MOSES"},{"location":"codes/codes_pages/MOSES/#short-description","text":"MOSES (MOde-coupling Single bunch instabilities in an Electron Storage ring) is a program, which computes complex coherent betatron tune shifts as a function of the bunch current for a Gaussian beam interacting with a resonator impedance.","title":"Short description"},{"location":"codes/codes_pages/MOSES/#web-resources","text":"Source code and home page: http://abci.kek.jp/moses.htm","title":"Web resources"},{"location":"codes/codes_pages/MOSES/#technical-information","text":"Operating systems: Windows (works on 7) and Linux (not tested) Other prerequisites: e.g. libraries needed to install/use your software e.g. libraries needed to install/use your software Special hardware needs: e.g. GPU, resources for multithreading/multiprocessing. Special hardware needs: e.g. GPU, resources for multithreading/multiprocessing.","title":"Technical information"},{"location":"codes/codes_pages/MOSES/#other-information","text":"Developed by: [Name of developers] License: [Licence policy] Contact persons: [Contact persons for the code] Being actively developed and supported: [Yes/No]","title":"Other information"},{"location":"codes/codes_pages/MadX/","text":"MAD-X Short description MAD-X is an application used world-wide with a long history going back to the 80's in the field of high energy beam physics (i.e. MAD8, MAD9, MADX). It is an all-in-one application with its own scripting language used to design, simulate and optimize particle accelerators: lattice description, machine survey, single particles 6D tracking, optics modeling, beam simulation & analysis, machine optimisation, errors handling, orbit correction, aperture margin and emittance equilibrium. Web resources The MAD-X website provides access to information, documentation (PDF), releases (binaries), source code (tarball), e-groups, versioning system (SVN), issues tracking system (Trac), and more... The user manual can be downloaded here . Technical information Programming Languages used for implementation: Fortran 77, Fortran 90, C, C++ for a total of about 180000 lines of codes. MAD-X scripting language is garbage collected (based on GC Boehm). Build system entirely done with make. Tested every night on server equiped with VMs for supported OS and architectures. Parallelization strategy: Particle tracking (track command) is parallelized with OpenMP if compiled with the proper flags. This is not the default as it does not bring much speed gain (old-style Fortran 77). Compilers vectorize strings, arrays, vectors and matrices computations, and perform loop unrolling plus many other common optimizations done by modern compilers on modern CPU/FPU. Operating systems: Supported on MAC OSX (10.8 or above), Windows (7 or above), Linux (static binary). 32 bit and 64 bit architectures are avalaible. Other prerequisites: No libraries. Scatter plots use gnuplot. Other information Delivery: 2-3 releases per year. Developed by: Main developpers were H. Grote (MAD8, MADX) and C. Iselin (MAD8, MAD9) in the 90's. Many people contributed to the project since then, see the contributors section on the website. License: Open source software under CERN Copyrights. Contact persons: mad at cern dot ch (see the website for the persons members of the MAD team). Being actively developed and supported: Yes.","title":"MAD-X"},{"location":"codes/codes_pages/MadX/#mad-x","text":"","title":"MAD-X"},{"location":"codes/codes_pages/MadX/#short-description","text":"MAD-X is an application used world-wide with a long history going back to the 80's in the field of high energy beam physics (i.e. MAD8, MAD9, MADX). It is an all-in-one application with its own scripting language used to design, simulate and optimize particle accelerators: lattice description, machine survey, single particles 6D tracking, optics modeling, beam simulation & analysis, machine optimisation, errors handling, orbit correction, aperture margin and emittance equilibrium.","title":"Short description"},{"location":"codes/codes_pages/MadX/#web-resources","text":"The MAD-X website provides access to information, documentation (PDF), releases (binaries), source code (tarball), e-groups, versioning system (SVN), issues tracking system (Trac), and more... The user manual can be downloaded here .","title":"Web resources"},{"location":"codes/codes_pages/MadX/#technical-information","text":"Programming Languages used for implementation: Fortran 77, Fortran 90, C, C++ for a total of about 180000 lines of codes. MAD-X scripting language is garbage collected (based on GC Boehm). Build system entirely done with make. Tested every night on server equiped with VMs for supported OS and architectures. Parallelization strategy: Particle tracking (track command) is parallelized with OpenMP if compiled with the proper flags. This is not the default as it does not bring much speed gain (old-style Fortran 77). Compilers vectorize strings, arrays, vectors and matrices computations, and perform loop unrolling plus many other common optimizations done by modern compilers on modern CPU/FPU. Operating systems: Supported on MAC OSX (10.8 or above), Windows (7 or above), Linux (static binary). 32 bit and 64 bit architectures are avalaible. Other prerequisites: No libraries. Scatter plots use gnuplot.","title":"Technical information"},{"location":"codes/codes_pages/MadX/#other-information","text":"Delivery: 2-3 releases per year. Developed by: Main developpers were H. Grote (MAD8, MADX) and C. Iselin (MAD8, MAD9) in the 90's. Many people contributed to the project since then, see the contributors section on the website. License: Open source software under CERN Copyrights. Contact persons: mad at cern dot ch (see the website for the persons members of the MAD team). Being actively developed and supported: Yes.","title":"Other information"},{"location":"codes/codes_pages/MapClass/","text":"MapClass Short description MapClass and its successor MapClass2 are two codes written in Python and C++ conceived to optimize the non-linear aberrations in beam lines. First and second order momenta of the beam at the end of the given beam line are used as figure of merits. MapClass takes the transfer map from PTC while MapClass2 is equipped with a simple integrator to produce the transfer map up to the desired order. Web resources Source code: https://github.com/pylhc/MapClass2 MapClass documentation: https://cds.cern.ch/record/944769/files/ab-note-2006-017.pdf MapClass2 documentation: https://cds.cern.ch/record/1491228 GPU parallelization: http://www.sciencedirect.com/science/article/pii/S1877050916306573 Technical information Programming Languages used for implementation: Python C++ Parallelization strategy: GPU in C++ Operating systems: Linux Other prerequisites: Boost libraries for communication between C++ and python GPU optionally Other information Developed by: Rogelio Tomas , Eduardo Marin Lacoma, David Martinez, Alice Rosam, Hector Garcia Morales and Andrea Popescu. License: Open source Contact persons: Rogelio Tomas Being actively developed and supported: Yes","title":"MapClass"},{"location":"codes/codes_pages/MapClass/#mapclass","text":"","title":"MapClass"},{"location":"codes/codes_pages/MapClass/#short-description","text":"MapClass and its successor MapClass2 are two codes written in Python and C++ conceived to optimize the non-linear aberrations in beam lines. First and second order momenta of the beam at the end of the given beam line are used as figure of merits. MapClass takes the transfer map from PTC while MapClass2 is equipped with a simple integrator to produce the transfer map up to the desired order.","title":"Short description"},{"location":"codes/codes_pages/MapClass/#web-resources","text":"Source code: https://github.com/pylhc/MapClass2 MapClass documentation: https://cds.cern.ch/record/944769/files/ab-note-2006-017.pdf MapClass2 documentation: https://cds.cern.ch/record/1491228 GPU parallelization: http://www.sciencedirect.com/science/article/pii/S1877050916306573","title":"Web resources"},{"location":"codes/codes_pages/MapClass/#technical-information","text":"Programming Languages used for implementation: Python C++ Parallelization strategy: GPU in C++ Operating systems: Linux Other prerequisites: Boost libraries for communication between C++ and python GPU optionally","title":"Technical information"},{"location":"codes/codes_pages/MapClass/#other-information","text":"Developed by: Rogelio Tomas , Eduardo Marin Lacoma, David Martinez, Alice Rosam, Hector Garcia Morales and Andrea Popescu. License: Open source Contact persons: Rogelio Tomas Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/Ninja/","text":"NINJA Short description NINJA is a 2.5 D Implicit Particle-In-Cell Monte Carlo Collision code for the simulation of inductively coupled plasmas. The model includes a self-consistent description of the coupling between the electromagnetic field generated by the radio-frequency antenna and the plasma response, composed of the particles\u2019 motion and the collisions between them. Web resources [Link web resources available for your software. For example:] Source code: [link1] Wiki pages: [link2] Publication in preparation Technical information Programming Languages used for implementation: Fortran 90 Parallelization strategy: MPI, domain decomposition Operating systems: Linux Other prerequisites: Third party libraries: NITSOL, BLAS, LAPACK, SPARSKIT Other information Developed by: Stefano Mattei, in collaboration with KEIO University (Japan) License: [Licence policy] Contact persons: Stefano Mattei, Jacques Lettry Being actively developed and supported: Yes","title":"NINJA"},{"location":"codes/codes_pages/Ninja/#ninja","text":"","title":"NINJA"},{"location":"codes/codes_pages/Ninja/#short-description","text":"NINJA is a 2.5 D Implicit Particle-In-Cell Monte Carlo Collision code for the simulation of inductively coupled plasmas. The model includes a self-consistent description of the coupling between the electromagnetic field generated by the radio-frequency antenna and the plasma response, composed of the particles\u2019 motion and the collisions between them.","title":"Short description"},{"location":"codes/codes_pages/Ninja/#web-resources","text":"[Link web resources available for your software. For example:] Source code: [link1] Wiki pages: [link2] Publication in preparation","title":"Web resources"},{"location":"codes/codes_pages/Ninja/#technical-information","text":"Programming Languages used for implementation: Fortran 90 Parallelization strategy: MPI, domain decomposition Operating systems: Linux Other prerequisites: Third party libraries: NITSOL, BLAS, LAPACK, SPARSKIT","title":"Technical information"},{"location":"codes/codes_pages/Ninja/#other-information","text":"Developed by: Stefano Mattei, in collaboration with KEIO University (Japan) License: [Licence policy] Contact persons: Stefano Mattei, Jacques Lettry Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/ONIX/","text":"ONIX Short description ONIX (Orsay Negative Ion eXtraction) is a 3D particle-in-cell Monte Carlo Collision electrostatic code to simulate the particle transport in electronegative plasmas, in the vicinity of the extraction electrode of an ion source. Web resources References: http://iopscience.iop.org/article/10.1088/1367-2630/18/8/085011 http://scitation.aip.org/content/aip/journal/jap/111/11/10.1063/1.4727969 Technical information Programming Languages used for implementation: Fortran Parallelization strategy: MPI, domain decomposition Operating systems: Linux Other information Developed by: Serhiy Mochalskyy (IPP Garching, Germany) License: Contact persons: Jacques Lettry Being actively developed and supported: Yes","title":"ONIX"},{"location":"codes/codes_pages/ONIX/#onix","text":"","title":"ONIX"},{"location":"codes/codes_pages/ONIX/#short-description","text":"ONIX (Orsay Negative Ion eXtraction) is a 3D particle-in-cell Monte Carlo Collision electrostatic code to simulate the particle transport in electronegative plasmas, in the vicinity of the extraction electrode of an ion source.","title":"Short description"},{"location":"codes/codes_pages/ONIX/#web-resources","text":"References: http://iopscience.iop.org/article/10.1088/1367-2630/18/8/085011 http://scitation.aip.org/content/aip/journal/jap/111/11/10.1063/1.4727969","title":"Web resources"},{"location":"codes/codes_pages/ONIX/#technical-information","text":"Programming Languages used for implementation: Fortran Parallelization strategy: MPI, domain decomposition Operating systems: Linux","title":"Technical information"},{"location":"codes/codes_pages/ONIX/#other-information","text":"Developed by: Serhiy Mochalskyy (IPP Garching, Germany) License: Contact persons: Jacques Lettry Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/PATH/","text":"SoftwareName Short description [Insert a short description of your software.] Web resources [Link web resources available for your software. For example:] Source code: [link1] Wiki pages: [link2] [If you want you can insert the documentation directly in this wiki, feel free to create as many pages as you need ;-)] Technical information [Provide the following information] Programming Languages used for implementation: Language 1 Language 1 Language 2 Language 2 Parallelization strategy: e.g. MPI/multithreading.. e.g. MPI/multithreading.. Operating systems: Which operating systems have been tested and are supported Which operating systems have been tested and are supported Other prerequisites: e.g. libraries needed to install/use your software e.g. libraries needed to install/use your software Special hardware needs: e.g. GPU, resources for multithreading/multiprocessing. Special hardware needs: e.g. GPU, resources for multithreading/multiprocessing. Other information Developed by: [Name of developers] License: [Licence policy] Contact persons: [Contact persons for the code] Being actively developed and supported: [Yes/No]","title":"SoftwareName"},{"location":"codes/codes_pages/PATH/#softwarename","text":"","title":"SoftwareName"},{"location":"codes/codes_pages/PATH/#short-description","text":"[Insert a short description of your software.]","title":"Short description"},{"location":"codes/codes_pages/PATH/#web-resources","text":"[Link web resources available for your software. For example:] Source code: [link1] Wiki pages: [link2] [If you want you can insert the documentation directly in this wiki, feel free to create as many pages as you need ;-)]","title":"Web resources"},{"location":"codes/codes_pages/PATH/#technical-information","text":"[Provide the following information] Programming Languages used for implementation: Language 1 Language 1 Language 2 Language 2 Parallelization strategy: e.g. MPI/multithreading.. e.g. MPI/multithreading.. Operating systems: Which operating systems have been tested and are supported Which operating systems have been tested and are supported Other prerequisites: e.g. libraries needed to install/use your software e.g. libraries needed to install/use your software Special hardware needs: e.g. GPU, resources for multithreading/multiprocessing. Special hardware needs: e.g. GPU, resources for multithreading/multiprocessing.","title":"Technical information"},{"location":"codes/codes_pages/PATH/#other-information","text":"Developed by: [Name of developers] License: [Licence policy] Contact persons: [Contact persons for the code] Being actively developed and supported: [Yes/No]","title":"Other information"},{"location":"codes/codes_pages/PHOTON/","text":"SoftwareName PHOTON Short description The PHOTON program performs a Monte-Carlo simulation of photon flux on the vacuum-chamber wall around a storage ring or transport line. The simulation includes synchrotron radiation in dipoles and quadrupoles, the effects of closed orbit distortions, beam size and divergence, etc. The photon energies are determined following the algorithm proposed by Roy. The program requires 4 input files: (1) a look-up table for the SR spectrum, (2) a MAD twiss tape file which describes the beam optics - this can of course include optics imperfections - , (3) a file with magnet apertures, (4) a parameter input file. The latter contains, e.g., the beam emittances, energy, particle type, and various simulation flags. Web resources Home page: http://wwwslap.cern.ch/collective/electron-cloud/Programs/Photon/photon.html Source code: https://wwwslap.cern.ch/collective/electron-cloud/Programs/Photon/photon.tar References: https://care-hhh.web.cern.ch/care-hhh/Simulation-Codes/References%20pages/Ref_PHOTON.htm Technical information Programming Languages used for implementation: FORTRAN77 Parallelization strategy: none Operating systems: linux Other prerequisites: needs a MAD output file Other information Developed by: Frank Zimmermann frank.zimmermann@cernNOSPAMPLEASE.ch License: CERN Contact persons: Frank Zimmermann Being actively developed and supported: No","title":"SoftwareName"},{"location":"codes/codes_pages/PHOTON/#softwarename","text":"PHOTON","title":"SoftwareName"},{"location":"codes/codes_pages/PHOTON/#short-description","text":"The PHOTON program performs a Monte-Carlo simulation of photon flux on the vacuum-chamber wall around a storage ring or transport line. The simulation includes synchrotron radiation in dipoles and quadrupoles, the effects of closed orbit distortions, beam size and divergence, etc. The photon energies are determined following the algorithm proposed by Roy. The program requires 4 input files: (1) a look-up table for the SR spectrum, (2) a MAD twiss tape file which describes the beam optics - this can of course include optics imperfections - , (3) a file with magnet apertures, (4) a parameter input file. The latter contains, e.g., the beam emittances, energy, particle type, and various simulation flags.","title":"Short description"},{"location":"codes/codes_pages/PHOTON/#web-resources","text":"Home page: http://wwwslap.cern.ch/collective/electron-cloud/Programs/Photon/photon.html Source code: https://wwwslap.cern.ch/collective/electron-cloud/Programs/Photon/photon.tar References: https://care-hhh.web.cern.ch/care-hhh/Simulation-Codes/References%20pages/Ref_PHOTON.htm","title":"Web resources"},{"location":"codes/codes_pages/PHOTON/#technical-information","text":"Programming Languages used for implementation: FORTRAN77 Parallelization strategy: none Operating systems: linux Other prerequisites: needs a MAD output file","title":"Technical information"},{"location":"codes/codes_pages/PHOTON/#other-information","text":"Developed by: Frank Zimmermann frank.zimmermann@cernNOSPAMPLEASE.ch License: CERN Contact persons: Frank Zimmermann Being actively developed and supported: No","title":"Other information"},{"location":"codes/codes_pages/PageStore/","text":"PageStore Short description Library to store massive timeseries data on commodity file system. Web resources Source code: PageStore GitHub Pages Technical information Programming Languages used for implementation: Python Operating systems: Multiple OS Other prerequisites: numpy Other information Developed by: Riccardo De Maria License: No Explicit License Contact persons: Riccardo De Maria Being actively developed and supported: Yes","title":"PageStore"},{"location":"codes/codes_pages/PageStore/#pagestore","text":"","title":"PageStore"},{"location":"codes/codes_pages/PageStore/#short-description","text":"Library to store massive timeseries data on commodity file system.","title":"Short description"},{"location":"codes/codes_pages/PageStore/#web-resources","text":"Source code: PageStore GitHub Pages","title":"Web resources"},{"location":"codes/codes_pages/PageStore/#technical-information","text":"Programming Languages used for implementation: Python Operating systems: Multiple OS Other prerequisites: numpy","title":"Technical information"},{"location":"codes/codes_pages/PageStore/#other-information","text":"Developed by: Riccardo De Maria License: No Explicit License Contact persons: Riccardo De Maria Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/Placet/","text":"PLACET Short description PLACET (Program for Linear Accelerator Correction and Efficiency Tests) is a code that simulates the dynamics of a beam in the main accelerating or decelerating part of a linac in the presence of wakefields. It allows to investigate single- and multi-bunch effects, and to simulate normal cavities with relatively low group velocities as well as the special power extraction and transfer structures specific to CLIC. A number of correction schemes are implemented, to test what emittance growth must be expected for given prealignment errors. Web resources Source code: https://gitlab.cern.ch/clic-software/placet Project page: https://clicsw.web.cern.ch/clicsw Technical information Programming Languages used for implementation: C / C++ Parallelization strategy: openMP, optionally MPI Operating systems: Linux, MacOS, Cygwin Other prerequisites: fftw, gsl, tcl/tk user interface: Tcl/Tk, Octave, Python Run PLACET on HTCondor Guide to submit PLACET jobs to HTCondor available here Other information Developed by: Daniel Schulte, Andrea Latina, et al. License: CERN Licence Contact persons: Andrea Latina Being actively developed and supported: [Yes]","title":"PLACET"},{"location":"codes/codes_pages/Placet/#placet","text":"","title":"PLACET"},{"location":"codes/codes_pages/Placet/#short-description","text":"PLACET (Program for Linear Accelerator Correction and Efficiency Tests) is a code that simulates the dynamics of a beam in the main accelerating or decelerating part of a linac in the presence of wakefields. It allows to investigate single- and multi-bunch effects, and to simulate normal cavities with relatively low group velocities as well as the special power extraction and transfer structures specific to CLIC. A number of correction schemes are implemented, to test what emittance growth must be expected for given prealignment errors.","title":"Short description"},{"location":"codes/codes_pages/Placet/#web-resources","text":"Source code: https://gitlab.cern.ch/clic-software/placet Project page: https://clicsw.web.cern.ch/clicsw","title":"Web resources"},{"location":"codes/codes_pages/Placet/#technical-information","text":"Programming Languages used for implementation: C / C++ Parallelization strategy: openMP, optionally MPI Operating systems: Linux, MacOS, Cygwin Other prerequisites: fftw, gsl, tcl/tk user interface: Tcl/Tk, Octave, Python Run PLACET on HTCondor Guide to submit PLACET jobs to HTCondor available here","title":"Technical information"},{"location":"codes/codes_pages/Placet/#other-information","text":"Developed by: Daniel Schulte, Andrea Latina, et al. License: CERN Licence Contact persons: Andrea Latina Being actively developed and supported: [Yes]","title":"Other information"},{"location":"codes/codes_pages/PyECLOUD/","text":"PyECLOUD Short description PyECLOUD is a 2D macro-particle code for the simulation of electron cloud effects in particle accelerators. It can be used for two purposes: in stand-alone mode for the simulation of the e-cloud buildup at a certain section of an accelerator (it this case the beam is rigid and feels no effect from from the cloud); in combination with the PyHEADTAIL code for the simulation of the e-cloud effects on the beam dynamics. Web resources Git repository: https://github.com/PyCOMPLETE/PyECLOUD Getting started guides: Installation , Usage Examples: Available in Getting-started guides. Documentation: : https://github.com/PyCOMPLETE/PyECLOUD/wiki Technical information Programming Languages used for implementation: Mainly Python. Computationally intensive routines are implemented in FORTRAN (and linked via f2py) or C (and linked via cython). Parallelization strategy: PyECLOUD-PyHEADTAIL simulations can be paralleled using the PyPARIS layer. Operating systems: tested exclusively on Linux. Other prerequisites: Python 3.6+ Libraries: numpy, scipy, cython, h5py Other information Developed by: Giovanni Iadarola License: CERN Copyright Contact persons: Giovanni Iadarola","title":"PyECLOUD"},{"location":"codes/codes_pages/PyECLOUD/#pyecloud","text":"","title":"PyECLOUD"},{"location":"codes/codes_pages/PyECLOUD/#short-description","text":"PyECLOUD is a 2D macro-particle code for the simulation of electron cloud effects in particle accelerators. It can be used for two purposes: in stand-alone mode for the simulation of the e-cloud buildup at a certain section of an accelerator (it this case the beam is rigid and feels no effect from from the cloud); in combination with the PyHEADTAIL code for the simulation of the e-cloud effects on the beam dynamics.","title":"Short description"},{"location":"codes/codes_pages/PyECLOUD/#web-resources","text":"Git repository: https://github.com/PyCOMPLETE/PyECLOUD Getting started guides: Installation , Usage Examples: Available in Getting-started guides. Documentation: : https://github.com/PyCOMPLETE/PyECLOUD/wiki","title":"Web resources"},{"location":"codes/codes_pages/PyECLOUD/#technical-information","text":"Programming Languages used for implementation: Mainly Python. Computationally intensive routines are implemented in FORTRAN (and linked via f2py) or C (and linked via cython). Parallelization strategy: PyECLOUD-PyHEADTAIL simulations can be paralleled using the PyPARIS layer. Operating systems: tested exclusively on Linux. Other prerequisites: Python 3.6+ Libraries: numpy, scipy, cython, h5py","title":"Technical information"},{"location":"codes/codes_pages/PyECLOUD/#other-information","text":"Developed by: Giovanni Iadarola License: CERN Copyright Contact persons: Giovanni Iadarola","title":"Other information"},{"location":"codes/codes_pages/PyHEADTAIL/","text":"PyHEADTAIL Short description Python macroparticle simulation code library for modeling collective effects beam dynamics in circular accelerators. Web resources Source code: https://github.com/PyCOMPLETE/PyHEADTAIL Wiki pages: https://github.com/PyCOMPLETE/PyHEADTAIL/wiki Technical information Programming Languages used for implementation: Python, C, C++, Cython, CUDA Parallelization strategy: MPI, OpenMP, GPGPU Operating systems: Linux (experience on SLC5+, Ubuntu 12+, OpenSUSE 12+) Other prerequisites: Python 2.7+ (minor compatibility issues with Python 3 expected) Third party libraries: Cython, h5py, NumPy, SciPy Special hardware needs: NVidia GPU for CUDA parts Other information Developed by: Kevin Li et al. License: CERN Copyright Contact persons: Kevin Li Being actively developed and supported: Yes","title":"PyHEADTAIL"},{"location":"codes/codes_pages/PyHEADTAIL/#pyheadtail","text":"","title":"PyHEADTAIL"},{"location":"codes/codes_pages/PyHEADTAIL/#short-description","text":"Python macroparticle simulation code library for modeling collective effects beam dynamics in circular accelerators.","title":"Short description"},{"location":"codes/codes_pages/PyHEADTAIL/#web-resources","text":"Source code: https://github.com/PyCOMPLETE/PyHEADTAIL Wiki pages: https://github.com/PyCOMPLETE/PyHEADTAIL/wiki","title":"Web resources"},{"location":"codes/codes_pages/PyHEADTAIL/#technical-information","text":"Programming Languages used for implementation: Python, C, C++, Cython, CUDA Parallelization strategy: MPI, OpenMP, GPGPU Operating systems: Linux (experience on SLC5+, Ubuntu 12+, OpenSUSE 12+) Other prerequisites: Python 2.7+ (minor compatibility issues with Python 3 expected) Third party libraries: Cython, h5py, NumPy, SciPy Special hardware needs: NVidia GPU for CUDA parts","title":"Technical information"},{"location":"codes/codes_pages/PyHEADTAIL/#other-information","text":"Developed by: Kevin Li et al. License: CERN Copyright Contact persons: Kevin Li Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/PyORBIT/","text":"PyORBIT Short description PyORBIT is a Python/C++ implementation of the ORBIT (Objective Ring Beam Injection and Tracking) code. PyORBIT software is an open environment for simulations of diverse physical processes related to particle accelerators. The original ORBIT has the Super Code driver shell which is replaced by Python in PyORBIT. At this moment only few capabilities of the original ORBIT are implemented. Particle tracking can be done in two ways, either with the built-in Teapot tracker module or using a special version of the PTC library. Web resources Source code: https://sourceforge.net/projects/py-orbit/ Wiki pages: https://sourceforge.net/p/py-orbit/wiki/?source=navbar Technical information Programming Languages used for implementation: Top layer in Python All computationally intensive routines are implemented in C++ Parallelization strategy: Based on MPI, communication between cores at each space charge interaction node. Operating systems: Tested on Linux and MAC OSX (Windows), at CERN used on lxplus Other prerequisites: Python 2.6 PTC Other information Developed by: A. Shishlo, J. Holmes, S. Cousineau, SNS Oakridge, contributions from CERN License: MIT License Contact person at CERN: Hannes Bartosik Being actively developed and supported: Yes","title":"PyORBIT"},{"location":"codes/codes_pages/PyORBIT/#pyorbit","text":"","title":"PyORBIT"},{"location":"codes/codes_pages/PyORBIT/#short-description","text":"PyORBIT is a Python/C++ implementation of the ORBIT (Objective Ring Beam Injection and Tracking) code. PyORBIT software is an open environment for simulations of diverse physical processes related to particle accelerators. The original ORBIT has the Super Code driver shell which is replaced by Python in PyORBIT. At this moment only few capabilities of the original ORBIT are implemented. Particle tracking can be done in two ways, either with the built-in Teapot tracker module or using a special version of the PTC library.","title":"Short description"},{"location":"codes/codes_pages/PyORBIT/#web-resources","text":"Source code: https://sourceforge.net/projects/py-orbit/ Wiki pages: https://sourceforge.net/p/py-orbit/wiki/?source=navbar","title":"Web resources"},{"location":"codes/codes_pages/PyORBIT/#technical-information","text":"Programming Languages used for implementation: Top layer in Python All computationally intensive routines are implemented in C++ Parallelization strategy: Based on MPI, communication between cores at each space charge interaction node. Operating systems: Tested on Linux and MAC OSX (Windows), at CERN used on lxplus Other prerequisites: Python 2.6 PTC","title":"Technical information"},{"location":"codes/codes_pages/PyORBIT/#other-information","text":"Developed by: A. Shishlo, J. Holmes, S. Cousineau, SNS Oakridge, contributions from CERN License: MIT License Contact person at CERN: Hannes Bartosik Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/PyOptics/","text":"PyOptics Short description Python library to read common optics data files (TFS tables, madx input, SDDS binary files, Yasp) and perform optics analysis (plots, queries, tunes, harmonic driving terms, footprint). Web resources PyOptics GitHub pages Technical information Programming Languages used for implementation: Python Operating systems: Multiple OS Other prerequisites: numpy, scipy, matplotlib Other information Developed by: Riccardo De Maria License: No Explicit License at the moment Contact persons: Riccardo De Maria Being actively developed and supported: Yes","title":"PyOptics"},{"location":"codes/codes_pages/PyOptics/#pyoptics","text":"","title":"PyOptics"},{"location":"codes/codes_pages/PyOptics/#short-description","text":"Python library to read common optics data files (TFS tables, madx input, SDDS binary files, Yasp) and perform optics analysis (plots, queries, tunes, harmonic driving terms, footprint).","title":"Short description"},{"location":"codes/codes_pages/PyOptics/#web-resources","text":"PyOptics GitHub pages","title":"Web resources"},{"location":"codes/codes_pages/PyOptics/#technical-information","text":"Programming Languages used for implementation: Python Operating systems: Multiple OS Other prerequisites: numpy, scipy, matplotlib","title":"Technical information"},{"location":"codes/codes_pages/PyOptics/#other-information","text":"Developed by: Riccardo De Maria License: No Explicit License at the moment Contact persons: Riccardo De Maria Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/PyPIC/","text":"PyPIC Short description PyPIC is a python library featuring different 2D Particle in Cell Solvers. It includes advanced features like the accurate modelling of curved boundary using the Shortley-Weller approximation, and improved accuracy based on nested grids. PyPIC it is used to simulate electron cloud and space charge effects within PyECLOUD and PyHEADTAIL . Web resources Source code: https://github.com/PyCOMPLETE/PyPIC Additional information: https://github.com/PyCOMPLETE/PyPIC/wiki Technical information Programming Languages used for implementation: Mainly Python Computationally intensive routines are implemented in FORTRAN (and linked via f2py) or C/C++ (and linked via cython). Parallelization strategy: None Operating systems: Tested exclusivey on Linux (experience on Ubuntu 12.04 or more recent, and SLC 5 or more recent) Other prerequisites: Python 2.7+ (never tested on Python 3) Libraries: numpy, cython, f2py. Optionally: pyfftw, KLU interface). Other information Developed by: Giovanni Iadarola , Kevin Li, Eleonora Belli, Lotta Mether License: CERN Copyright Contact persons: Giovanni Iadarola Being actively developed and supported: Yes","title":"PyPIC"},{"location":"codes/codes_pages/PyPIC/#pypic","text":"","title":"PyPIC"},{"location":"codes/codes_pages/PyPIC/#short-description","text":"PyPIC is a python library featuring different 2D Particle in Cell Solvers. It includes advanced features like the accurate modelling of curved boundary using the Shortley-Weller approximation, and improved accuracy based on nested grids. PyPIC it is used to simulate electron cloud and space charge effects within PyECLOUD and PyHEADTAIL .","title":"Short description"},{"location":"codes/codes_pages/PyPIC/#web-resources","text":"Source code: https://github.com/PyCOMPLETE/PyPIC Additional information: https://github.com/PyCOMPLETE/PyPIC/wiki","title":"Web resources"},{"location":"codes/codes_pages/PyPIC/#technical-information","text":"Programming Languages used for implementation: Mainly Python Computationally intensive routines are implemented in FORTRAN (and linked via f2py) or C/C++ (and linked via cython). Parallelization strategy: None Operating systems: Tested exclusivey on Linux (experience on Ubuntu 12.04 or more recent, and SLC 5 or more recent) Other prerequisites: Python 2.7+ (never tested on Python 3) Libraries: numpy, cython, f2py. Optionally: pyfftw, KLU interface).","title":"Technical information"},{"location":"codes/codes_pages/PyPIC/#other-information","text":"Developed by: Giovanni Iadarola , Kevin Li, Eleonora Belli, Lotta Mether License: CERN Copyright Contact persons: Giovanni Iadarola Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/PyRADISE/","text":"PyRADISE Short description PyRADISE is a Python code for studying RAdial DIffusion and Stability Evolution. It numerically solves a PDE (diffusion equation) using a FVM scheme. Then the Evolving stability is evaluated by using PySSD to numerically solve the dipersion integral based on the bunch distribution function from the PDE solver and amplitude detuning. Web resources Git repository: https://gitlab.cern.ch/IRIS/pyradise Getting started guides: Examples: Documentation: : Technical information Programming Languages used for implementation: Python Parallelization strategy: None Operating systems: Tested on Linux (Ubuntu, CENTOS7) Other prerequisites: Numpy, scipy and PySSD Other information Developed by: S.V. Furuseth License: CERN Copyright Contact persons: S.V. Furuseth , X. Buffat","title":"PyRADISE"},{"location":"codes/codes_pages/PyRADISE/#pyradise","text":"","title":"PyRADISE"},{"location":"codes/codes_pages/PyRADISE/#short-description","text":"PyRADISE is a Python code for studying RAdial DIffusion and Stability Evolution. It numerically solves a PDE (diffusion equation) using a FVM scheme. Then the Evolving stability is evaluated by using PySSD to numerically solve the dipersion integral based on the bunch distribution function from the PDE solver and amplitude detuning.","title":"Short description"},{"location":"codes/codes_pages/PyRADISE/#web-resources","text":"Git repository: https://gitlab.cern.ch/IRIS/pyradise Getting started guides: Examples: Documentation: :","title":"Web resources"},{"location":"codes/codes_pages/PyRADISE/#technical-information","text":"Programming Languages used for implementation: Python Parallelization strategy: None Operating systems: Tested on Linux (Ubuntu, CENTOS7) Other prerequisites: Numpy, scipy and PySSD","title":"Technical information"},{"location":"codes/codes_pages/PyRADISE/#other-information","text":"Developed by: S.V. Furuseth License: CERN Copyright Contact persons: S.V. Furuseth , X. Buffat","title":"Other information"},{"location":"codes/codes_pages/PySSD/","text":"PySSD (Python Solver for Stability Diagrams) Short description Numerical solver for stability diagrams based on the beam distribution and the amplitude detuning from analytical formulas or interfaced with tracking codes, respectively SixTrack and MadX . Web resources Sources CWG presentation Technical information Programming Languages used for implementation: Python 2.6 or higher Operating systems: tested exclusivey on Linux (Ubuntu 12.04 and SLC 5) Other prerequisites: Libraries: numpy, scipy Other informations Developed by : CERN License : CERN Copyright Contact persons : Xavier Buffat","title":"PySSD (Python Solver for Stability Diagrams)"},{"location":"codes/codes_pages/PySSD/#pyssd-python-solver-for-stability-diagrams","text":"","title":"PySSD (Python Solver for Stability Diagrams)"},{"location":"codes/codes_pages/PySSD/#short-description","text":"Numerical solver for stability diagrams based on the beam distribution and the amplitude detuning from analytical formulas or interfaced with tracking codes, respectively SixTrack and MadX .","title":"Short description"},{"location":"codes/codes_pages/PySSD/#web-resources","text":"Sources CWG presentation","title":"Web resources"},{"location":"codes/codes_pages/PySSD/#technical-information","text":"Programming Languages used for implementation: Python 2.6 or higher Operating systems: tested exclusivey on Linux (Ubuntu 12.04 and SLC 5) Other prerequisites: Libraries: numpy, scipy","title":"Technical information"},{"location":"codes/codes_pages/PySSD/#other-informations","text":"Developed by : CERN License : CERN Copyright Contact persons : Xavier Buffat","title":"Other informations"},{"location":"codes/codes_pages/PyTimber/","text":"PyTimber Short description Python Wrapper to CERN Logging Java API (CALS) to extract machine data (LHC, SPS, ...) from the CERN Logging service. Web resources Documentation : PyTimber Wiki (CERN internal link) Installation: PyTimber Installation Guide (CERN internal link) Source code: PyTimber GitLab Pages Technical information [Provide the following information] Programming Languages used for implementation: Python Operating systems: Multiple OS Other prerequisites: jdk 1.8, jpype, numpy, (optional matplotlib) Other information Developed by: P. Sowinski, P. Elson, R. De Maria, T. Levens, C. Hernalsteens, M. Betz. License: No Explicit License Contact persons: P. Sowinski Being actively developed and supported: Yes","title":"PyTimber"},{"location":"codes/codes_pages/PyTimber/#pytimber","text":"","title":"PyTimber"},{"location":"codes/codes_pages/PyTimber/#short-description","text":"Python Wrapper to CERN Logging Java API (CALS) to extract machine data (LHC, SPS, ...) from the CERN Logging service.","title":"Short description"},{"location":"codes/codes_pages/PyTimber/#web-resources","text":"Documentation : PyTimber Wiki (CERN internal link) Installation: PyTimber Installation Guide (CERN internal link) Source code: PyTimber GitLab Pages","title":"Web resources"},{"location":"codes/codes_pages/PyTimber/#technical-information","text":"[Provide the following information] Programming Languages used for implementation: Python Operating systems: Multiple OS Other prerequisites: jdk 1.8, jpype, numpy, (optional matplotlib)","title":"Technical information"},{"location":"codes/codes_pages/PyTimber/#other-information","text":"Developed by: P. Sowinski, P. Elson, R. De Maria, T. Levens, C. Hernalsteens, M. Betz. License: No Explicit License Contact persons: P. Sowinski Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/RF-Track/","text":"RF-Track Short description RF-Track is a novel tracking code developed for the optimization of low-energy linacs in presence of space-charge effects. RF-Track can transport beams of particles with arbitrary mass and charge even mixed together, solving fully relativistic equations of motion. It implements direct space-charge effects in a physically consistent manner, using parallel algorithms. It can simulate bunched beams as well as continuous beams, and transport through conventional elements as well as through field maps of oscillating electromagnetic fields. RF-Track is written in optimized and parallel C++, and it uses the scripting languages Octave and Python as user interfaces. RF-Track has been tested successfully in several cases: TULIP, a backward-traveling-wave linac for medical applications; 750 MHz CERN's radio-frequency quadrupole; the transfer line for low energy anti-protons of the ELENA ring; the CLIC positron injector, and the AWAKE injector linac. Web resources Source code: not yet published Short Presentation: https://indico.cern.ch/event/514848/contributions/2037361/attachments/1250710/1844703/ABP_Information_2016_Mar_31.pdf Technical information Programming Languages used for implementation: C++, SWIG Parallelization strategy: C++11 threads, multi-core parallelism Operating systems: Linux, MacOS Other prerequisites: GSL, fftw User interface: octave, python Other information Developed by: Andrea Latina License: CERN License Contact persons: Andrea Latina Being actively developed and supported: Yes","title":"RF-Track"},{"location":"codes/codes_pages/RF-Track/#rf-track","text":"","title":"RF-Track"},{"location":"codes/codes_pages/RF-Track/#short-description","text":"RF-Track is a novel tracking code developed for the optimization of low-energy linacs in presence of space-charge effects. RF-Track can transport beams of particles with arbitrary mass and charge even mixed together, solving fully relativistic equations of motion. It implements direct space-charge effects in a physically consistent manner, using parallel algorithms. It can simulate bunched beams as well as continuous beams, and transport through conventional elements as well as through field maps of oscillating electromagnetic fields. RF-Track is written in optimized and parallel C++, and it uses the scripting languages Octave and Python as user interfaces. RF-Track has been tested successfully in several cases: TULIP, a backward-traveling-wave linac for medical applications; 750 MHz CERN's radio-frequency quadrupole; the transfer line for low energy anti-protons of the ELENA ring; the CLIC positron injector, and the AWAKE injector linac.","title":"Short description"},{"location":"codes/codes_pages/RF-Track/#web-resources","text":"Source code: not yet published Short Presentation: https://indico.cern.ch/event/514848/contributions/2037361/attachments/1250710/1844703/ABP_Information_2016_Mar_31.pdf","title":"Web resources"},{"location":"codes/codes_pages/RF-Track/#technical-information","text":"Programming Languages used for implementation: C++, SWIG Parallelization strategy: C++11 threads, multi-core parallelism Operating systems: Linux, MacOS Other prerequisites: GSL, fftw User interface: octave, python","title":"Technical information"},{"location":"codes/codes_pages/RF-Track/#other-information","text":"Developed by: Andrea Latina License: CERN License Contact persons: Andrea Latina Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/SIRE/","text":"SIRE (Software for IBS and Radiation Effects) Short description The SIRE was inspired by MOCAC (MOnte CArlo Code). After specifying the beam distribution and the optics along a lattice, SIRE iteratively computes intrabeam collisions between pairs of macro-particles. If requested it also evaluates the effects of synchrotron radiation damping and quantum excitation. The beam distribution is updated and the rms beam emittances are recomputed, giving finally as output the emittance evolution in time. Web resources Source code: can be downoloaded here Documentation: in preparation... Technical information Programming Languages used for implementation: C Parallelization strategy: None Operating systems: Linux, Windows Other prerequisites: None Other information Developed by: M. Martini, A. Vivoli License: CERN Copyright Contact persons: F. Antoniou, S. Papadopoulou, Y. Papaphilippou Being actively developed and supported: Yes","title":"SIRE (Software for IBS and Radiation Effects)"},{"location":"codes/codes_pages/SIRE/#sire-software-for-ibs-and-radiation-effects","text":"","title":"SIRE (Software for IBS and Radiation Effects)"},{"location":"codes/codes_pages/SIRE/#short-description","text":"The SIRE was inspired by MOCAC (MOnte CArlo Code). After specifying the beam distribution and the optics along a lattice, SIRE iteratively computes intrabeam collisions between pairs of macro-particles. If requested it also evaluates the effects of synchrotron radiation damping and quantum excitation. The beam distribution is updated and the rms beam emittances are recomputed, giving finally as output the emittance evolution in time.","title":"Short description"},{"location":"codes/codes_pages/SIRE/#web-resources","text":"Source code: can be downoloaded here Documentation: in preparation...","title":"Web resources"},{"location":"codes/codes_pages/SIRE/#technical-information","text":"Programming Languages used for implementation: C Parallelization strategy: None Operating systems: Linux, Windows Other prerequisites: None","title":"Technical information"},{"location":"codes/codes_pages/SIRE/#other-information","text":"Developed by: M. Martini, A. Vivoli License: CERN Copyright Contact persons: F. Antoniou, S. Papadopoulou, Y. Papaphilippou Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/SUSSIX/","text":"sussix Interpolated FFT for up 9999 cases in bpm.XXXX files or 32 SixTrack binary files [Provide the following information] Programming Languages used for implementation: Fortran Parallelization strategy: none Operating systems: LINUX Other prerequisites: none Developed by: [R. Bartolini and F. Schmidt] License: none Contact persons: F. Schmidt Being actively developed and supported: [Yes]","title":"sussix"},{"location":"codes/codes_pages/SUSSIX/#sussix","text":"","title":"sussix"},{"location":"codes/codes_pages/SUSSIX/#interpolated-fft-for-up-9999-cases-in-bpmxxxx-files-or-32-sixtrack-binary-files","text":"","title":"Interpolated FFT for up 9999 cases in bpm.XXXX files or 32 SixTrack binary files"},{"location":"codes/codes_pages/SixTrack/","text":"SixTrack Short description SixTrack is a single particle 6D symplectic tracking code optimized for long term tracking in high energy rings. It is mainly used for the LHC for dynamic aperture studies, tune optimization, collimation studies. Web resources The main website contains links to source code, manual and documentation. Technical information Programming Languages used for implementation: Fortran (f90) C Parallelization strategy: Vectorization of loops Operating systems: Linux, Windows, Mac Other information Developed by: Frank Schmidt, Eric McIntosh et al. License: LGPLv2 Contact persons: riccardo.de.maria@cern.ch Being actively developed and supported: Yes","title":"SixTrack"},{"location":"codes/codes_pages/SixTrack/#sixtrack","text":"","title":"SixTrack"},{"location":"codes/codes_pages/SixTrack/#short-description","text":"SixTrack is a single particle 6D symplectic tracking code optimized for long term tracking in high energy rings. It is mainly used for the LHC for dynamic aperture studies, tune optimization, collimation studies.","title":"Short description"},{"location":"codes/codes_pages/SixTrack/#web-resources","text":"The main website contains links to source code, manual and documentation.","title":"Web resources"},{"location":"codes/codes_pages/SixTrack/#technical-information","text":"Programming Languages used for implementation: Fortran (f90) C Parallelization strategy: Vectorization of loops Operating systems: Linux, Windows, Mac","title":"Technical information"},{"location":"codes/codes_pages/SixTrack/#other-information","text":"Developed by: Frank Schmidt, Eric McIntosh et al. License: LGPLv2 Contact persons: riccardo.de.maria@cern.ch Being actively developed and supported: Yes","title":"Other information"},{"location":"codes/codes_pages/TRAIN/","text":"TRAIN Short description In part of the straight sections of the LHC the two beams share a common beam tube. Therefore the bunches cross each other not only at the interaction point, but as well at many places on either side, with a typical transverse separation of 10 times the transverse beam size. These \"parasitic\" encounters lead to orbit distortions and tune shifts, in addition to higher order effects. Since the string of bunches from the injection machine contains gaps, not all possible 3564 \"buckets\" around the machine are filled, but only about 3000. This in turn causes some bunches to not always encounter bunches in the opposite beam at one or several parasitic collision points (so-called \"pacman\" bunches), or even at the head-on interaction point (\"super-pacman\" bunches). With this special program self-consistent orbits in the LHC have been calculated for the first time with the full beam-beam collision scheme resulting from various injection scenarios. The offsets at the interaction points, and the tune shifts are shown to be small enough to be easily controlled. Web resources Source code: http://gitlab.cern.ch/agorzaws/train Old webpage: http://lhc-beam-beam.web.cern.ch/lhc-beam-beam/train_welcome.html ABP CWG presentation : https://indico.cern.ch/event/631880/contributions/2557453/attachments/1450659/2236799/2017-04-27_TRAIN-expanded.pdf TRAIN wiki page Source code (PyTRAIN): https://gitlab.cern.ch/mihostet/pytrain PyTRAIN presentation : https://indico.cern.ch/event/658908/contributions/2686544/attachments/1507510/2350662/emitscan_train_mad.pdf Technical information [Provide the following information] Programming Languages used for implementation: Fortran Parallelization strategy: None Operating systems: SLC6 Other prerequisites: None Other information Developed by: F.C. Iselin, E. Keil, H. Grote, W. Herr, A. Gorzawski License: None Contact persons: A. Gorzawski, T. Pieloni, X. Buffat Being actively developed and supported: No","title":"TRAIN"},{"location":"codes/codes_pages/TRAIN/#train","text":"","title":"TRAIN"},{"location":"codes/codes_pages/TRAIN/#short-description","text":"In part of the straight sections of the LHC the two beams share a common beam tube. Therefore the bunches cross each other not only at the interaction point, but as well at many places on either side, with a typical transverse separation of 10 times the transverse beam size. These \"parasitic\" encounters lead to orbit distortions and tune shifts, in addition to higher order effects. Since the string of bunches from the injection machine contains gaps, not all possible 3564 \"buckets\" around the machine are filled, but only about 3000. This in turn causes some bunches to not always encounter bunches in the opposite beam at one or several parasitic collision points (so-called \"pacman\" bunches), or even at the head-on interaction point (\"super-pacman\" bunches). With this special program self-consistent orbits in the LHC have been calculated for the first time with the full beam-beam collision scheme resulting from various injection scenarios. The offsets at the interaction points, and the tune shifts are shown to be small enough to be easily controlled.","title":"Short description"},{"location":"codes/codes_pages/TRAIN/#web-resources","text":"Source code: http://gitlab.cern.ch/agorzaws/train Old webpage: http://lhc-beam-beam.web.cern.ch/lhc-beam-beam/train_welcome.html ABP CWG presentation : https://indico.cern.ch/event/631880/contributions/2557453/attachments/1450659/2236799/2017-04-27_TRAIN-expanded.pdf TRAIN wiki page Source code (PyTRAIN): https://gitlab.cern.ch/mihostet/pytrain PyTRAIN presentation : https://indico.cern.ch/event/658908/contributions/2686544/attachments/1507510/2350662/emitscan_train_mad.pdf","title":"Web resources"},{"location":"codes/codes_pages/TRAIN/#technical-information","text":"[Provide the following information] Programming Languages used for implementation: Fortran Parallelization strategy: None Operating systems: SLC6 Other prerequisites: None","title":"Technical information"},{"location":"codes/codes_pages/TRAIN/#other-information","text":"Developed by: F.C. Iselin, E. Keil, H. Grote, W. Herr, A. Gorzawski License: None Contact persons: A. Gorzawski, T. Pieloni, X. Buffat Being actively developed and supported: No","title":"Other information"},{"location":"computing_resources/abpstorage/","text":"ABP data storage EOS storage ABP has a general storage space for measurements and simulation data in \\eos\\project\\a\\abpdata Please contact abp.computing[AT]cern.ch for further information.","title":"ABP data storage"},{"location":"computing_resources/abpstorage/#abp-data-storage","text":"","title":"ABP data storage"},{"location":"computing_resources/abpstorage/#eos-storage","text":"ABP has a general storage space for measurements and simulation data in \\eos\\project\\a\\abpdata Please contact abp.computing[AT]cern.ch for further information.","title":"EOS storage"},{"location":"computing_resources/cernbatch/","text":"HTCondor batch system Computing jobs that run on individual nodes with up to 32 CPU cores per node can be submitted to the CERN batch service (lxbatch). The jobs are submitted and managed using the HTCcondor platform. Access All users having access to the CERN linux service and the AFS filesystem (which can be self-enabled at https://resources.web.cern.ch ) can submit jobs to HTCondor, but have by default rather low priority. ABP users working on computationally intensive tasks can be granted higher priority, by being added to one of the following e-groups (based on their section): Section name e-group name BE-ABP-CEI batch-u-abp-cei BE-ABP-HSL batch-u-abp-hsl BE-ABP-INC batch-u-abp-inc BE-ABP-LAF batch-u-abp-laf BE-ABP-LNO batch-u-abp-lno BE-ABP-NDC batch-u-abp-ndc The section leaders and the ABP-CP members have admin rights to add users to the e-group of their section. All these e-groups are mapped to a single computing group called \u201cgroup_u_BE.ABP.NORMAL\u201d. Usage Detailed documentation managed by the IT department can be found here: https://batchdocs.web.cern.ch/index.html A quick start guide can be found here: https://batchdocs.web.cern.ch/local/quick.html GUIs for monitoring the clusters can be found here: https://batch-carbon.cern.ch/grafana/dashboard/db/user-batch-jobs https://batch-carbon.cern.ch/grafana/dashboard/db/cluster-batch-jobs https://monit-grafana.cern.ch/d/000000865/experiment-batch-details https://monit-grafana.cern.ch/d/000000868/schedds GPUs Graphics Processing Units are available in the system. To use GPUs please follow the instructions available here: https://batchdocs.web.cern.ch/tutorial/exercise10.html An example submit file is: executable = job_name/job_name.sh arguments = $(ClusterId) $(ProcId) output = job_name/htcondor.out error = job_name/htcondor.err log = job_name/htcondor.log transfer_input_files = job_name requirements = regexp(\"V100\", Target.CUDADeviceName) request_GPUs = 1 request_CPUs = 1 +MaxRunTime = 86400 queue The line requirements = regexp(\"V100\", Target.CUDADeviceName) will find nodes that only have a V100 GPU. Nodes with T4 GPUs also exist. Nodes with large memory Some nodes are equipped with a larger number of cores and memory, namely (31 Oct 2019): - a few nodes with 24 physical cores and 1Tb of memory - 6 nodes with 48 cores (hyperthreaded) and 512Gb of memory These can be used via HTCondor by adding the appropriate lines to the submit file, e.g.: RequestCpus = 24 +BigMemJob = True Members of the accounting group group_u_BE.ABP.NORMAL should have access to run on these nodes. Access can be granted to other users in the group. Specific applications Example on how HTCondor is used to manage PyECLOUD, PyHEADTAIL and PLACET simulations can be found here: https://indico.cern.ch/event/637703/contributions/2583365/ https://indico.cern.ch/event/580885/contributions/2355043/ https://twiki.cern.ch/twiki/pub/ABPComputing/Placet/Run_placet_on_HTCondor.pdf For administrators The shares of the different groups can be monitored on https://haggis.cern.ch/ (available only inside the CERN network). Search for \"be\" to see our shares. FAQs Scheduler Not Replying From time to time it happens that the scheduler does not reply. In general, it is a temporary problem; if this is not the case, open IT ticket . At the same time, you may try changing the scheduler you are assigned by default. This can be accomplished by one of two ways: setting the two environment variables: _condor_SCHEDD_HOST and _condor_CREDD_HOST . E.g.: tcsh setenv _condor_SCHEDD_HOST bigbird02.cern.ch setenv _condor_CREDD_HOST bigbird02.cern.ch bash export _condor_SCHEDD_HOST = \"bigbird02.cern.ch\" export _condor_CREDD_HOST = \"bigbird02.cern.ch\" In the output of a simple call to condor_q you can find the scheduler name. If you don't set these variables, the reported scheduler name is the one assigned to you by default; otherwise, you should find the one that you have set via the previous variables. Please keep in mind that these statements, if typed on terminal, will apply only to that session. For instance, in case you log out or the lxplus session expires, you have to re-set those two variables if you want them also in the new session. So, please remember the scheduler that you have requested, otherwise you won't be able to retrieve the results form HTCondor . Calling condor commands with -name parameter. You can use another than your defined schedular by addressing it directly in your commands, e.g. condor_q -name bigbird15.cern.ch This should work for any of the condor -commands (e.g. condor_q , condor_submit , etc.). The scheduler is then used only for this command. Jobs Being Taken Very Slowly It may happen that you see your jobs queueing for too long. This might be simply due to overload of the batch system (please check the batch GUI ); more rarely, it can be also a problem with priorities. Indeed, it may happen that your jobs are assigned (by mistake) an accounting group with very low priority. Hence, you can check if your jobs are assigned the wrong accounting group via (an example output is shown): $ condor_q owner $LOGNAME -long | grep '^AccountingGroup' | sort | uniq -c 9 AccountingGroup = \"group_u_ATLAS.u_zp.nkarast\" 1496 AccountingGroup = \"group_u_BE.UNIX.u_pz.nkarast\" You can force the use of the high priority accounting group modifying your .sub script as: +AccountingGroup = \"group_u_BE.ABP.NORMAL\" Advanced features Spool Option The -spool option can be used at condor_submit level, e.g. condor_submit -spool htcondor.sub In this case, all the output files ( transfer_output_files ) and the error , log and output files are not generated once the jobs finishes, but only when requested by the user, after the job is over. Retrieval can be done via the following command: condor_transfer_data $LOGNAME -const 'JobStatus == 4' In the above example, the files from all completed job in each cluster will be retrieved. In this case, the jobs may not automatically disappear from condor_q . Job removal takes place after 10 days the job has finished. Submitting Jobs to HTCondor from a local Machine The recommended way of using HTCondor is to submit jobs by logging to lxplus.cern.ch. It is also possible to configure your own computer to manage HTCondor jobs, as described in this guide .","title":"HTCondor batch system"},{"location":"computing_resources/cernbatch/#htcondor-batch-system","text":"Computing jobs that run on individual nodes with up to 32 CPU cores per node can be submitted to the CERN batch service (lxbatch). The jobs are submitted and managed using the HTCcondor platform.","title":"HTCondor batch system"},{"location":"computing_resources/cernbatch/#access","text":"All users having access to the CERN linux service and the AFS filesystem (which can be self-enabled at https://resources.web.cern.ch ) can submit jobs to HTCondor, but have by default rather low priority. ABP users working on computationally intensive tasks can be granted higher priority, by being added to one of the following e-groups (based on their section): Section name e-group name BE-ABP-CEI batch-u-abp-cei BE-ABP-HSL batch-u-abp-hsl BE-ABP-INC batch-u-abp-inc BE-ABP-LAF batch-u-abp-laf BE-ABP-LNO batch-u-abp-lno BE-ABP-NDC batch-u-abp-ndc The section leaders and the ABP-CP members have admin rights to add users to the e-group of their section. All these e-groups are mapped to a single computing group called \u201cgroup_u_BE.ABP.NORMAL\u201d.","title":"Access"},{"location":"computing_resources/cernbatch/#usage","text":"Detailed documentation managed by the IT department can be found here: https://batchdocs.web.cern.ch/index.html A quick start guide can be found here: https://batchdocs.web.cern.ch/local/quick.html GUIs for monitoring the clusters can be found here: https://batch-carbon.cern.ch/grafana/dashboard/db/user-batch-jobs https://batch-carbon.cern.ch/grafana/dashboard/db/cluster-batch-jobs https://monit-grafana.cern.ch/d/000000865/experiment-batch-details https://monit-grafana.cern.ch/d/000000868/schedds","title":"Usage"},{"location":"computing_resources/cernbatch/#gpus","text":"Graphics Processing Units are available in the system. To use GPUs please follow the instructions available here: https://batchdocs.web.cern.ch/tutorial/exercise10.html An example submit file is: executable = job_name/job_name.sh arguments = $(ClusterId) $(ProcId) output = job_name/htcondor.out error = job_name/htcondor.err log = job_name/htcondor.log transfer_input_files = job_name requirements = regexp(\"V100\", Target.CUDADeviceName) request_GPUs = 1 request_CPUs = 1 +MaxRunTime = 86400 queue The line requirements = regexp(\"V100\", Target.CUDADeviceName) will find nodes that only have a V100 GPU. Nodes with T4 GPUs also exist.","title":"GPUs"},{"location":"computing_resources/cernbatch/#nodes-with-large-memory","text":"Some nodes are equipped with a larger number of cores and memory, namely (31 Oct 2019): - a few nodes with 24 physical cores and 1Tb of memory - 6 nodes with 48 cores (hyperthreaded) and 512Gb of memory These can be used via HTCondor by adding the appropriate lines to the submit file, e.g.: RequestCpus = 24 +BigMemJob = True Members of the accounting group group_u_BE.ABP.NORMAL should have access to run on these nodes. Access can be granted to other users in the group.","title":"Nodes with large memory"},{"location":"computing_resources/cernbatch/#specific-applications","text":"Example on how HTCondor is used to manage PyECLOUD, PyHEADTAIL and PLACET simulations can be found here: https://indico.cern.ch/event/637703/contributions/2583365/ https://indico.cern.ch/event/580885/contributions/2355043/ https://twiki.cern.ch/twiki/pub/ABPComputing/Placet/Run_placet_on_HTCondor.pdf","title":"Specific applications"},{"location":"computing_resources/cernbatch/#for-administrators","text":"The shares of the different groups can be monitored on https://haggis.cern.ch/ (available only inside the CERN network). Search for \"be\" to see our shares.","title":"For administrators"},{"location":"computing_resources/cernbatch/#faqs","text":"","title":"FAQs"},{"location":"computing_resources/cernbatch/#scheduler-not-replying","text":"From time to time it happens that the scheduler does not reply. In general, it is a temporary problem; if this is not the case, open IT ticket . At the same time, you may try changing the scheduler you are assigned by default. This can be accomplished by one of two ways: setting the two environment variables: _condor_SCHEDD_HOST and _condor_CREDD_HOST . E.g.: tcsh setenv _condor_SCHEDD_HOST bigbird02.cern.ch setenv _condor_CREDD_HOST bigbird02.cern.ch bash export _condor_SCHEDD_HOST = \"bigbird02.cern.ch\" export _condor_CREDD_HOST = \"bigbird02.cern.ch\" In the output of a simple call to condor_q you can find the scheduler name. If you don't set these variables, the reported scheduler name is the one assigned to you by default; otherwise, you should find the one that you have set via the previous variables. Please keep in mind that these statements, if typed on terminal, will apply only to that session. For instance, in case you log out or the lxplus session expires, you have to re-set those two variables if you want them also in the new session. So, please remember the scheduler that you have requested, otherwise you won't be able to retrieve the results form HTCondor . Calling condor commands with -name parameter. You can use another than your defined schedular by addressing it directly in your commands, e.g. condor_q -name bigbird15.cern.ch This should work for any of the condor -commands (e.g. condor_q , condor_submit , etc.). The scheduler is then used only for this command.","title":"Scheduler Not Replying"},{"location":"computing_resources/cernbatch/#jobs-being-taken-very-slowly","text":"It may happen that you see your jobs queueing for too long. This might be simply due to overload of the batch system (please check the batch GUI ); more rarely, it can be also a problem with priorities. Indeed, it may happen that your jobs are assigned (by mistake) an accounting group with very low priority. Hence, you can check if your jobs are assigned the wrong accounting group via (an example output is shown): $ condor_q owner $LOGNAME -long | grep '^AccountingGroup' | sort | uniq -c 9 AccountingGroup = \"group_u_ATLAS.u_zp.nkarast\" 1496 AccountingGroup = \"group_u_BE.UNIX.u_pz.nkarast\" You can force the use of the high priority accounting group modifying your .sub script as: +AccountingGroup = \"group_u_BE.ABP.NORMAL\"","title":"Jobs Being Taken Very Slowly"},{"location":"computing_resources/cernbatch/#advanced-features","text":"","title":"Advanced features"},{"location":"computing_resources/cernbatch/#spool-option","text":"The -spool option can be used at condor_submit level, e.g. condor_submit -spool htcondor.sub In this case, all the output files ( transfer_output_files ) and the error , log and output files are not generated once the jobs finishes, but only when requested by the user, after the job is over. Retrieval can be done via the following command: condor_transfer_data $LOGNAME -const 'JobStatus == 4' In the above example, the files from all completed job in each cluster will be retrieved. In this case, the jobs may not automatically disappear from condor_q . Job removal takes place after 10 days the job has finished.","title":"Spool Option"},{"location":"computing_resources/cernbatch/#submitting-jobs-to-htcondor-from-a-local-machine","text":"The recommended way of using HTCondor is to submit jobs by logging to lxplus.cern.ch. It is also possible to configure your own computer to manage HTCondor jobs, as described in this guide .","title":"Submitting Jobs to HTCondor from a local Machine"},{"location":"computing_resources/cernts/","text":"","title":"Cernts"},{"location":"computing_resources/hpc_cern/","text":"High Performance Computing clusters at CERN Since end 2017, two new HPC cluster have been setup here at CERN by the IT department for users of MPI applications. Access ABP members who need access to the cluster can request it by writing to abp.computing[AT]cern.ch in order to be added to the relevant e-group (service-hpc-be). Please note that these are dedicated facilities for parallel computing applications, i.e. applications running with MPI across more than 32 cores for each simulation (hence requiring multiple nodes). Simulations requiring less than 32 cores should be submitted to the CERN batch service . Documentation and other resources Documentation on the cluster and its usage can be found at: https://batchdocs.web.cern.ch/linuxhpc/index.html Support and further info can be found in the Service Portal: https://cern.service-now.com/service-portal?id=service_element&name=High-Performance-Computing A dedicated workshop in two sessions took place in 2020. The corresponding indico pages can be found at: https://indico.cern.ch/event/867459/ https://indico.cern.ch/event/916903/ Regular meetings are held by IT to share information on the facility development: https://indico.cern.ch/category/7950/ Some further information can be found in dedicated presentations in past ABP-CWG meetings: https://indico.cern.ch/event/676152 https://indico.cern.ch/event/724162/","title":"High Performance Computing clusters at CERN"},{"location":"computing_resources/hpc_cern/#high-performance-computing-clusters-at-cern","text":"Since end 2017, two new HPC cluster have been setup here at CERN by the IT department for users of MPI applications.","title":"High Performance Computing clusters at CERN"},{"location":"computing_resources/hpc_cern/#access","text":"ABP members who need access to the cluster can request it by writing to abp.computing[AT]cern.ch in order to be added to the relevant e-group (service-hpc-be). Please note that these are dedicated facilities for parallel computing applications, i.e. applications running with MPI across more than 32 cores for each simulation (hence requiring multiple nodes). Simulations requiring less than 32 cores should be submitted to the CERN batch service .","title":"Access"},{"location":"computing_resources/hpc_cern/#documentation-and-other-resources","text":"Documentation on the cluster and its usage can be found at: https://batchdocs.web.cern.ch/linuxhpc/index.html Support and further info can be found in the Service Portal: https://cern.service-now.com/service-portal?id=service_element&name=High-Performance-Computing A dedicated workshop in two sessions took place in 2020. The corresponding indico pages can be found at: https://indico.cern.ch/event/867459/ https://indico.cern.ch/event/916903/ Regular meetings are held by IT to share information on the facility development: https://indico.cern.ch/category/7950/ Some further information can be found in dedicated presentations in past ABP-CWG meetings: https://indico.cern.ch/event/676152 https://indico.cern.ch/event/724162/","title":"Documentation and other resources"},{"location":"computing_resources/hpc_cnaf/","text":"High Performance Computing clusters at INFN-CNAF An HPC cluster dedicated to CERN studies is available at INFN-CNAF (Bologna, Italy) allowing for MPI applications across multiple nodes. The cluster presently features a total of about 800 CPU-cores. Access Access to the cluster can be requested by sending an email to abp.computing[AT]cern.ch including the following information: The filled in form . Please add Daniele Cesini as contact person and \"Access to the hpc_acc Cluster to run High Energy Physics simulation codes\" as the reason. A copy of identity card or other valid identification document (e.g. passport). Usage Users should connect via ssh to bastion.cnaf.infn.it and from there access the cluster front-end called ui-hpc2.cr.cnaf.infn.it . The LSF system is installed on the cluster to manage job submissions. Jobs should be submitted to the \u201dhpc-acc\u201d queue. If you have inquiries about running your jobs on this cluster or need technical support, please write an email to hpc-support@listsNOSPAMPLEASE.cnaf.infn.it A node called hpc-201-11-35 is equipped with four V100 GPUs , and can be used interactively. Other links Relevant links to obtain further information on this cluster are: Presentation by Antonio Falabella at ABP-CWG meeting on 20 July, 2017 Presentation by Annalisa Romano on how to submit jobs at ABP-CWG meeting on 4 July 2017 Note on outcome of the acceptance test to proceed to the second phase of the cluster procurement and installation: Performance of space charge simulations using High Performance Computing (HPC) cluster CERN-ACC-NOTE-2017-0048 Support Please note that Antonio Falabella ( antonio.falabella@cnafNOSPAMPLEASE.infn.it ) is available to assist you, should you encounter any problem in connecting or running your jobs in the present configuration.","title":"High Performance Computing clusters at INFN-CNAF"},{"location":"computing_resources/hpc_cnaf/#high-performance-computing-clusters-at-infn-cnaf","text":"An HPC cluster dedicated to CERN studies is available at INFN-CNAF (Bologna, Italy) allowing for MPI applications across multiple nodes. The cluster presently features a total of about 800 CPU-cores.","title":"High Performance Computing clusters at INFN-CNAF"},{"location":"computing_resources/hpc_cnaf/#access","text":"Access to the cluster can be requested by sending an email to abp.computing[AT]cern.ch including the following information: The filled in form . Please add Daniele Cesini as contact person and \"Access to the hpc_acc Cluster to run High Energy Physics simulation codes\" as the reason. A copy of identity card or other valid identification document (e.g. passport).","title":"Access"},{"location":"computing_resources/hpc_cnaf/#usage","text":"Users should connect via ssh to bastion.cnaf.infn.it and from there access the cluster front-end called ui-hpc2.cr.cnaf.infn.it . The LSF system is installed on the cluster to manage job submissions. Jobs should be submitted to the \u201dhpc-acc\u201d queue. If you have inquiries about running your jobs on this cluster or need technical support, please write an email to hpc-support@listsNOSPAMPLEASE.cnaf.infn.it A node called hpc-201-11-35 is equipped with four V100 GPUs , and can be used interactively.","title":"Usage"},{"location":"computing_resources/hpc_cnaf/#other-links","text":"Relevant links to obtain further information on this cluster are: Presentation by Antonio Falabella at ABP-CWG meeting on 20 July, 2017 Presentation by Annalisa Romano on how to submit jobs at ABP-CWG meeting on 4 July 2017 Note on outcome of the acceptance test to proceed to the second phase of the cluster procurement and installation: Performance of space charge simulations using High Performance Computing (HPC) cluster CERN-ACC-NOTE-2017-0048","title":"Other links"},{"location":"computing_resources/hpc_cnaf/#support","text":"Please note that Antonio Falabella ( antonio.falabella@cnafNOSPAMPLEASE.infn.it ) is available to assist you, should you encounter any problem in connecting or running your jobs in the present configuration.","title":"Support"},{"location":"computing_resources/lxplus/","text":"LXPLUS service LXPLUS (Linux Public Login User Service) is the interactive logon service to Linux for all CERN users. The cluster LXPLUS consists of public machines provided by the IT Department for interactive work. Detailed documentation maintained by the IT Department is available here: https://lxplusdoc.web.cern.ch Access In order to access LXPLUS you need to request the activation of \"AFS Workspaces\" and of \"LXPLUS and linux\" for your account. This can be done on the CERN Resource Portal .","title":"LXPLUS service"},{"location":"computing_resources/lxplus/#lxplus-service","text":"LXPLUS (Linux Public Login User Service) is the interactive logon service to Linux for all CERN users. The cluster LXPLUS consists of public machines provided by the IT Department for interactive work. Detailed documentation maintained by the IT Department is available here: https://lxplusdoc.web.cern.ch","title":"LXPLUS service"},{"location":"computing_resources/lxplus/#access","text":"In order to access LXPLUS you need to request the activation of \"AFS Workspaces\" and of \"LXPLUS and linux\" for your account. This can be done on the CERN Resource Portal .","title":"Access"},{"location":"computing_resources/workstations/","text":"ABP workstations Workstations hosted at CERN The following workstations are avaiable for development and production. - Access is granted based on the use case. - For further information and to get access please contact abp.computing[AT]cern.ch. Host name CPU RAM GPU pcbe-abp-gpu001 AMD Ryzen Threadripper 2970WX 24-Core 128 GB 4x Titan V pcbe-abp-hpc001 AMD Ryzen Threadripper 3990X 64-Core 256 GB GeForce GTX 1660 Ti pcbe-abp-hpc002 AMD Ryzen Threadripper 3990X 64-Core 256 GB GeForce GTX 1660 Ti liupsgpu 2x Intel Xeon E5-2630 6-core 256 GB 3x Tesla C2075 When possible the CERN batch system (HTCondor) should be used for production studies while these workstation should be used for special cases (e.g. development, performance benchmarking, computations requiring very large amount of memory etc.). For administrators A user can change the password by simply typing passwd An administrator can add a user by: sudo adduser username An administrator can change the password for a user by typing: sudo passwd username An administrator can give sudo rights to an user by typing: usermod -aG sudo username Workstations hosted at INFN-CNAF GPUs are available also at CNAF (for users having a CNAF HPC account - more info available here ). You can launch GPU jobs interactively with the following steps: Connect with SSH into the login server: bastion.cnaf.infn.it ; Connect wiht SSH to the node that has the 4x Nvidia Tesla V100 GPUs: hpc-201-11-35 ; Enable newer versions of gcc, cmake etc. with scl enable devtoolset-7 bash","title":"ABP workstations"},{"location":"computing_resources/workstations/#abp-workstations","text":"","title":"ABP workstations"},{"location":"computing_resources/workstations/#workstations-hosted-at-cern","text":"The following workstations are avaiable for development and production. - Access is granted based on the use case. - For further information and to get access please contact abp.computing[AT]cern.ch. Host name CPU RAM GPU pcbe-abp-gpu001 AMD Ryzen Threadripper 2970WX 24-Core 128 GB 4x Titan V pcbe-abp-hpc001 AMD Ryzen Threadripper 3990X 64-Core 256 GB GeForce GTX 1660 Ti pcbe-abp-hpc002 AMD Ryzen Threadripper 3990X 64-Core 256 GB GeForce GTX 1660 Ti liupsgpu 2x Intel Xeon E5-2630 6-core 256 GB 3x Tesla C2075 When possible the CERN batch system (HTCondor) should be used for production studies while these workstation should be used for special cases (e.g. development, performance benchmarking, computations requiring very large amount of memory etc.).","title":"Workstations hosted at CERN"},{"location":"computing_resources/workstations/#for-administrators","text":"A user can change the password by simply typing passwd An administrator can add a user by: sudo adduser username An administrator can change the password for a user by typing: sudo passwd username An administrator can give sudo rights to an user by typing: usermod -aG sudo username","title":"For administrators"},{"location":"computing_resources/workstations/#workstations-hosted-at-infn-cnaf","text":"GPUs are available also at CNAF (for users having a CNAF HPC account - more info available here ). You can launch GPU jobs interactively with the following steps: Connect with SSH into the login server: bastion.cnaf.infn.it ; Connect wiht SSH to the node that has the 4x Nvidia Tesla V100 GPUs: hpc-201-11-35 ; Enable newer versions of gcc, cmake etc. with scl enable devtoolset-7 bash","title":"Workstations hosted at INFN-CNAF"},{"location":"guides/accpy/","text":"Acc-Py Information Creating Virtual Environments This is a page copying part of the content of https://wikis.cern.ch/display/ACCPY/Getting+started+with+Acc-Py To access it from home you can use sshuttle . In order to run your own application in the Acc-Py environment, it is likely you will want to install some specific packages which are suited to the task at hand. The most appropriate way to customise your Python environment is to use Python virtual environments. These are essentially a directory in which you have permission to install Python packages using the Python package manager, and which has its own Python executable and associated files. To enable it for the lifetime of your current shell a few key environment variables must be set by sourcing the setup script (from the technical network) source /acc/local/share/python/acc-py/pro/setup.sh Then you can create your virtual environment with a command similar to acc-py venv ~/venv/mypy and you can activate it by source ~/venv/mypy/bin/activate Now that you have a full Python environment with write permission, you can install Python packages into it: python -m pip install pyarrow That's it. Acc-Py Repository Packages and External Tools Recently, GPN functionality Python packages such as pyjapc , cmmnbuild-dep-manager , pjlsa , jpype1 and pytimber have been made installable from the acc-py repo only, which requires to install from inside the CERN GPN, and cannot be fetched from PyPI . However, some python projects need to reconcile using functionality from these packages while also being installable from outside the CERN GPN - to, say, be accessible to collaborators. These packages will find their CI/CD setup failing, and will also be faced with the impossibility of deploying to PyPI . As there are no plans to make packages from the acc-py repository installable from outside the CERN GPN in the foreseeable future, the current workaround is to declare these packages as optional dependencies (by creating an extra in your setup.py or pyproject.toml ), and to gate or mock their import whenever needed. This comes with the caveat that PyPI will only accept the deployment of a package if its dependencies are also registered on PyPI , so the workaround necessitates that the maintainers of pytimber , pjlsa or any python package deployed on the acc-py repository keep a shallow clone of their packages on PyPI . It is also recommended to keep shallow clones for security reasons , as it would be easy for any attacker to register a package under the same name on PyPI containing malicious code. Depending on the version number or the accessibility of the acc-py repository, this malware would then be installed instead of the acc-py package. For further reading, see this interesting article on medium.com and this article about PyPI removing malicious packages . An example of the mocking, as implemented in omc3 , can be seen below: import importlib class CERNNetworkMockPackage : \"\"\" Mock class to raise an error if the desired package functionality is called when the package is not actually installed. Designed for packages installable only from inside the CERN network, that are declared as an extra dependency. \"\"\" def __init__ ( self , name : str ): self . name = name def __getattr__ ( self , item ): raise ImportError ( f \"The ' { self . name } ' package does not seem to be installed but is needed for this function. \" \"Install it with the 'cern' extra dependency, which requires to be on the CERN network and to \" \"install from the acc-py package index. Refer to the documentation for more information.\" ) def cern_network_import ( package : str ): \"\"\" Convenience function to try and import packages only available (and installable) on the CERN network. If installed, the module is returned, otherwise a mock class is returned, which will raise an insightful ``ImportError`` on attempted use. Args: package (str): name of the package to try and import. \"\"\" try : return importlib . import_module ( package ) except ImportError : return CERNNetworkMockPackage ( package ) The usage is then: from your.mock.module import cern_network_import pytimber = cern_network_import ( \"pytimber\" ) db = pytimber . LoggingDB ( source = \"nxcals\" ) # will raise if pytimber not installed","title":"Acc-Py Information"},{"location":"guides/accpy/#acc-py-information","text":"","title":"Acc-Py Information"},{"location":"guides/accpy/#creating-virtual-environments","text":"This is a page copying part of the content of https://wikis.cern.ch/display/ACCPY/Getting+started+with+Acc-Py To access it from home you can use sshuttle . In order to run your own application in the Acc-Py environment, it is likely you will want to install some specific packages which are suited to the task at hand. The most appropriate way to customise your Python environment is to use Python virtual environments. These are essentially a directory in which you have permission to install Python packages using the Python package manager, and which has its own Python executable and associated files. To enable it for the lifetime of your current shell a few key environment variables must be set by sourcing the setup script (from the technical network) source /acc/local/share/python/acc-py/pro/setup.sh Then you can create your virtual environment with a command similar to acc-py venv ~/venv/mypy and you can activate it by source ~/venv/mypy/bin/activate Now that you have a full Python environment with write permission, you can install Python packages into it: python -m pip install pyarrow That's it.","title":"Creating Virtual Environments"},{"location":"guides/accpy/#acc-py-repository-packages-and-external-tools","text":"Recently, GPN functionality Python packages such as pyjapc , cmmnbuild-dep-manager , pjlsa , jpype1 and pytimber have been made installable from the acc-py repo only, which requires to install from inside the CERN GPN, and cannot be fetched from PyPI . However, some python projects need to reconcile using functionality from these packages while also being installable from outside the CERN GPN - to, say, be accessible to collaborators. These packages will find their CI/CD setup failing, and will also be faced with the impossibility of deploying to PyPI . As there are no plans to make packages from the acc-py repository installable from outside the CERN GPN in the foreseeable future, the current workaround is to declare these packages as optional dependencies (by creating an extra in your setup.py or pyproject.toml ), and to gate or mock their import whenever needed. This comes with the caveat that PyPI will only accept the deployment of a package if its dependencies are also registered on PyPI , so the workaround necessitates that the maintainers of pytimber , pjlsa or any python package deployed on the acc-py repository keep a shallow clone of their packages on PyPI . It is also recommended to keep shallow clones for security reasons , as it would be easy for any attacker to register a package under the same name on PyPI containing malicious code. Depending on the version number or the accessibility of the acc-py repository, this malware would then be installed instead of the acc-py package. For further reading, see this interesting article on medium.com and this article about PyPI removing malicious packages . An example of the mocking, as implemented in omc3 , can be seen below: import importlib class CERNNetworkMockPackage : \"\"\" Mock class to raise an error if the desired package functionality is called when the package is not actually installed. Designed for packages installable only from inside the CERN network, that are declared as an extra dependency. \"\"\" def __init__ ( self , name : str ): self . name = name def __getattr__ ( self , item ): raise ImportError ( f \"The ' { self . name } ' package does not seem to be installed but is needed for this function. \" \"Install it with the 'cern' extra dependency, which requires to be on the CERN network and to \" \"install from the acc-py package index. Refer to the documentation for more information.\" ) def cern_network_import ( package : str ): \"\"\" Convenience function to try and import packages only available (and installable) on the CERN network. If installed, the module is returned, otherwise a mock class is returned, which will raise an insightful ``ImportError`` on attempted use. Args: package (str): name of the package to try and import. \"\"\" try : return importlib . import_module ( package ) except ImportError : return CERNNetworkMockPackage ( package ) The usage is then: from your.mock.module import cern_network_import pytimber = cern_network_import ( \"pytimber\" ) db = pytimber . LoggingDB ( source = \"nxcals\" ) # will raise if pytimber not installed","title":"Acc-Py Repository Packages and External Tools"},{"location":"guides/docker/","text":"Docker Prepared by Guido Here I give few elements about dockers. I have a Mac laptop and I often need a UNIX system to test my workflow (UNIX oriented). Most of time I connect to lxplus or my UBUNTU box, but for some fast test is convenient to run locally. One way to achieve is using Docker . Dockers can be useful for cpython bindings like cpymad that are not so simple to be installed directly on Mac. In addition, it is a practical way to use some precooked installation from https://hub.docker.com and customize them at your needs. I also use it for JUAS and CAS, to share with the younger colleagues a predefined environment. Simple example After having installed the Docker Desktop - that is your docker manager, you can download the last jupyter/scipy environment from https://hub.docker.com/r/jupyter/scipy-notebook just with the command docker pull jupyter/scipy-notebook NB : docker commands recall the git syntax. There are several jupyter docker images available ( https://hub.docker.com/u/jupyter ). The first time you do that, it downloads the docker image locally. A docker image can be bulky (in this case ~1 GB) but you need to do that only once. Then, you can run the docker with docker run -p 8889:8888 jupyter/scipy-notebook This command will map the jupyter server to your localhost on port 8889 . You can access it via the browser. Pay attention to select the correct port and token. In the last line of the terminal you have the standard port 8888 but, in the browser, you need to give the port mapping of port 8888 ( 8889 in this case). For this example you need to copy/edit/paste on the browser the address http://127.0.0.1:8889/token=a546a6e4ba4a2b52dd08ff8f658bd65cf9f969372139e4fe In doing so you will access the docker jupyter on your browser. All documents you will create during the sessions will be lost. To maintain them in the local memory you can map the docker folder to your current terminal folder by docker run --rm -p 10000:8888 -e JUPYTER_ENABLE_LAB=yes -v \"$PWD\":/home/jovyan/work jupyter/scipy-notebook (in this case I am showing also some other sweet customisation). Here we map the /home/jovyan/work to the current path ( jovyan is the default user of Jupyter) and we launch jupyterlab instead of jupyter. The folder mapping allow you to see/use/handle your terminal pwd . Docker customisation Let us assume that now we want to customise the image and install cpymad and pyHeadTail . For that we have to prepare and input file using docker syntax (I will not explain it in the details but I think is quite self explanatory and easy to google, e.g. https://docs.docker.com/develop/develop-images/dockerfile_best-practices/ ). # reference: https://hub.docker.com/_/ubuntu/ FROM jupyter/scipy-notebook # Adds metadata to the image as a key value pair example LABEL version=\"1.0\" LABEL maintainer=\"Guido Sterbini <guido.sterbini@cern.ch>\" # create empty directory to attach volume USER root RUN sudo echo \"Europe/Rome\" > /etc/timezone && \\ sudo dpkg-reconfigure -f noninteractive tzdata RUN useradd -ms /bin/bash abpuser RUN apt-get update && apt-get install -y vim USER jovyan ########### RUN pip install cpymad RUN pip install PyHEADTAIL ########### USER abpuser WORKDIR /home/abpuser RUN mkdir abp ENV HOME=/abp VOLUME /abp WORKDIR /abp # Configure access to Jupyter CMD jupyter lab --no-browser --ip=0.0.0.0 --allow-root put it in the file Dockerfile on your current folder then build it with docker build . -t sterbini/abp_docker And finally you can run it with docker run -p 8889:8888 -v \"$PWD\":/abp sterbini/abp_docker You can upload your image to DockerHub to share it with the community. Like for Gitlab, you need to be registered. After registration on DockerHub , go to your terminal and do docker login and after that you will be asked for you credentials. Then push your image on DockerHub by docker push sterbini/abp_docker So you will see it on your personal DockerHub page. Now everyone can download it from DockerHub with the commands we already discussed before: docker pull sterbini/abp_docker and run it with docker run -p 8889:8888 -v \"$PWD\":/abp sterbini/abp_docker Please have a look to docker --help to see a summary of the docker functionalities (e.g., listing and removing the images on your pc). That's it folks.","title":"Docker"},{"location":"guides/docker/#docker","text":"Prepared by Guido Here I give few elements about dockers. I have a Mac laptop and I often need a UNIX system to test my workflow (UNIX oriented). Most of time I connect to lxplus or my UBUNTU box, but for some fast test is convenient to run locally. One way to achieve is using Docker . Dockers can be useful for cpython bindings like cpymad that are not so simple to be installed directly on Mac. In addition, it is a practical way to use some precooked installation from https://hub.docker.com and customize them at your needs. I also use it for JUAS and CAS, to share with the younger colleagues a predefined environment.","title":"Docker"},{"location":"guides/docker/#simple-example","text":"After having installed the Docker Desktop - that is your docker manager, you can download the last jupyter/scipy environment from https://hub.docker.com/r/jupyter/scipy-notebook just with the command docker pull jupyter/scipy-notebook NB : docker commands recall the git syntax. There are several jupyter docker images available ( https://hub.docker.com/u/jupyter ). The first time you do that, it downloads the docker image locally. A docker image can be bulky (in this case ~1 GB) but you need to do that only once. Then, you can run the docker with docker run -p 8889:8888 jupyter/scipy-notebook This command will map the jupyter server to your localhost on port 8889 . You can access it via the browser. Pay attention to select the correct port and token. In the last line of the terminal you have the standard port 8888 but, in the browser, you need to give the port mapping of port 8888 ( 8889 in this case). For this example you need to copy/edit/paste on the browser the address http://127.0.0.1:8889/token=a546a6e4ba4a2b52dd08ff8f658bd65cf9f969372139e4fe In doing so you will access the docker jupyter on your browser. All documents you will create during the sessions will be lost. To maintain them in the local memory you can map the docker folder to your current terminal folder by docker run --rm -p 10000:8888 -e JUPYTER_ENABLE_LAB=yes -v \"$PWD\":/home/jovyan/work jupyter/scipy-notebook (in this case I am showing also some other sweet customisation). Here we map the /home/jovyan/work to the current path ( jovyan is the default user of Jupyter) and we launch jupyterlab instead of jupyter. The folder mapping allow you to see/use/handle your terminal pwd .","title":"Simple example"},{"location":"guides/docker/#docker-customisation","text":"Let us assume that now we want to customise the image and install cpymad and pyHeadTail . For that we have to prepare and input file using docker syntax (I will not explain it in the details but I think is quite self explanatory and easy to google, e.g. https://docs.docker.com/develop/develop-images/dockerfile_best-practices/ ). # reference: https://hub.docker.com/_/ubuntu/ FROM jupyter/scipy-notebook # Adds metadata to the image as a key value pair example LABEL version=\"1.0\" LABEL maintainer=\"Guido Sterbini <guido.sterbini@cern.ch>\" # create empty directory to attach volume USER root RUN sudo echo \"Europe/Rome\" > /etc/timezone && \\ sudo dpkg-reconfigure -f noninteractive tzdata RUN useradd -ms /bin/bash abpuser RUN apt-get update && apt-get install -y vim USER jovyan ########### RUN pip install cpymad RUN pip install PyHEADTAIL ########### USER abpuser WORKDIR /home/abpuser RUN mkdir abp ENV HOME=/abp VOLUME /abp WORKDIR /abp # Configure access to Jupyter CMD jupyter lab --no-browser --ip=0.0.0.0 --allow-root put it in the file Dockerfile on your current folder then build it with docker build . -t sterbini/abp_docker And finally you can run it with docker run -p 8889:8888 -v \"$PWD\":/abp sterbini/abp_docker You can upload your image to DockerHub to share it with the community. Like for Gitlab, you need to be registered. After registration on DockerHub , go to your terminal and do docker login and after that you will be asked for you credentials. Then push your image on DockerHub by docker push sterbini/abp_docker So you will see it on your personal DockerHub page. Now everyone can download it from DockerHub with the commands we already discussed before: docker pull sterbini/abp_docker and run it with docker run -p 8889:8888 -v \"$PWD\":/abp sterbini/abp_docker Please have a look to docker --help to see a summary of the docker functionalities (e.g., listing and removing the images on your pc). That's it folks.","title":"Docker customisation"},{"location":"guides/eos_on_mac/","text":"(from an codiMD page of Ilias, https://codimd.web.cern.ch/s/vorpxehxj ) Using EOS in my Mac To effectively use my mac for LHC data analysis, I need to have access to EOS repositories: my personal one under /eos/home-i some project repositories in /eos/project-l From CERN IT there are two solutions proposed: use the CERNBOX client mount directly the EOS volumes I tried and have both running, below my experience from using them. Using the CERNBOX client The installation is straightforward, you can download the package and install it. Then you can configure the account and add folders to synchronize. pros You can have multiple folders synchronized with your Mac. Can be user folders, projects or other shared folders you have access to with the declared account. For each folder you can select which sub-folders you want to add to the syncronization. The program works well and I've very rarely see it crashing or not being able to do the synchronization. It is smart and can ignore certain types of files - there is an Ignored Files list that you can edit. cons Not reliable synchronization: sometimes takes long to trigger the synchronization, I've observed that sometimes some files are not included in the first synch loop, Very often \"small\" changes to files are not noticed. Eventually the synchronization works but is not immediate. For example editing locally the files using a smart editor (like VsCODE) and run my scripts in SWAN becomes a frustrating process. No dynamic synchronization: As user I have to select which files to synchronize and since my space in EOS can be as big as 1TB while in my Mac I have only 256GBytes, I need to constantly select and update the folders I synchronize. A better solution would have been a dynamic allocation of CERNBOX space in my Mac where files are included or removed upon use, as other programs do. This would also offer an access to the EOS directory structure which presently is limited to the ones selected for synchronization. Using FUSE for direct mount I followed the instructions found in some of the CERN IT pages. The installation is relatively easy: install OSXFUSE from osxfuse.github.io make a directory to be the local mount point for EOS can be either /eos or as in my case /User/ilias/eos then get the kerberos token using: kinit myuser@CERN.CH define the environment variables to EOS: export EOS_MGM_URL = root://eoshome-i.cern.ch or export EOS_MGM_URL = root://eosproject-l.cern.ch finally do the mount : eos fuse mount /Users/ilias/eos . That's it! In case the mount did not work you can restart it doing: killall eosd eos fuse umount /User/ilias/eos and then try mounting it again. pros Straightforward procedure, typically works You don't have to select directories to synchronize Full access to the directory tree Fast reaction to changes - the files become almost immediately available to remote Could access the EOS directories from my editor (vsCODE). cons It seems I have to redo the mount once the kerberos tokens expire I dit not manage to mount both a user directory and a project EOS space. Ilias suggested in the mattermost channel https://mattermost.web.cern.ch/abpcomputing/pl/5p54ieirofdxf8caejuy64emuy to use the following script #!/bin/bash # # Script to initialize and mount the EOS # # (c) Ilias Efthymiopoulos - Feb 2020 # # Arguments : [1] my CERN password # # # -- get access to EOS from previously stored kerberos password kinit -kt ~/.keytab efthymio@CERN.CH export EOS_MGM_URL = root://eoshome-e.cern.ch export EOS_FUSE_MGM_ALIAS = eoshome-e.cern.ch export EOS_FUSE_MOUNTDIR = /Users/iliasefthymiopoulos/eoshome-e eos fuse mount /Users/iliasefthymiopoulos/eoshome-e/ export EOS_MGM_URL = root://eosproject-l.cern.ch export EOS_FUSE_MGM_ALIAS = eosproject-l.cern.ch export EOS_FUSE_MOUNTDIR = /Users/iliasefthymiopoulos/eosproject-l eos fuse mount /Users/iliasefthymiopoulos/eosproject-l # --- define alias for unmount alias umounteos = 'function _umnteos() { killall eosd; eos fuse umount /Users/iliasefthymiopoulos/$1 }; _umnteos' and Joschua proposed to use also a function approach to simplify it function umounteos () { killall eosd ; eos fuse umount /Users/iliasefthymiopoulos/eos $1 } ; alias umounteosall = 'umounteos project-l; umounteos home-e' or even function mounteos () { export EOS_MGM_URL = root://eos $1 .cern.ch export EOS_FUSE_MGM_ALIAS = eos $1 .cern.ch export EOS_FUSE_MOUNTDIR = /Users/iliasefthymiopoulos/eos $1 eos fuse mount /Users/iliasefthymiopoulos/eos $1 } ; alias mounteosall = 'mounteos project-l; mounteos home-e'","title":"Eos on mac"},{"location":"guides/eos_on_mac/#using-eos-in-my-mac","text":"To effectively use my mac for LHC data analysis, I need to have access to EOS repositories: my personal one under /eos/home-i some project repositories in /eos/project-l From CERN IT there are two solutions proposed: use the CERNBOX client mount directly the EOS volumes I tried and have both running, below my experience from using them.","title":"Using EOS in my Mac"},{"location":"guides/eos_on_mac/#using-the-cernbox-client","text":"The installation is straightforward, you can download the package and install it. Then you can configure the account and add folders to synchronize.","title":"Using the CERNBOX client"},{"location":"guides/eos_on_mac/#pros","text":"You can have multiple folders synchronized with your Mac. Can be user folders, projects or other shared folders you have access to with the declared account. For each folder you can select which sub-folders you want to add to the syncronization. The program works well and I've very rarely see it crashing or not being able to do the synchronization. It is smart and can ignore certain types of files - there is an Ignored Files list that you can edit.","title":"pros"},{"location":"guides/eos_on_mac/#cons","text":"Not reliable synchronization: sometimes takes long to trigger the synchronization, I've observed that sometimes some files are not included in the first synch loop, Very often \"small\" changes to files are not noticed. Eventually the synchronization works but is not immediate. For example editing locally the files using a smart editor (like VsCODE) and run my scripts in SWAN becomes a frustrating process. No dynamic synchronization: As user I have to select which files to synchronize and since my space in EOS can be as big as 1TB while in my Mac I have only 256GBytes, I need to constantly select and update the folders I synchronize. A better solution would have been a dynamic allocation of CERNBOX space in my Mac where files are included or removed upon use, as other programs do. This would also offer an access to the EOS directory structure which presently is limited to the ones selected for synchronization.","title":"cons"},{"location":"guides/eos_on_mac/#using-fuse-for-direct-mount","text":"I followed the instructions found in some of the CERN IT pages. The installation is relatively easy: install OSXFUSE from osxfuse.github.io make a directory to be the local mount point for EOS can be either /eos or as in my case /User/ilias/eos then get the kerberos token using: kinit myuser@CERN.CH define the environment variables to EOS: export EOS_MGM_URL = root://eoshome-i.cern.ch or export EOS_MGM_URL = root://eosproject-l.cern.ch finally do the mount : eos fuse mount /Users/ilias/eos . That's it! In case the mount did not work you can restart it doing: killall eosd eos fuse umount /User/ilias/eos and then try mounting it again.","title":"Using FUSE for direct mount"},{"location":"guides/eos_on_mac/#pros_1","text":"Straightforward procedure, typically works You don't have to select directories to synchronize Full access to the directory tree Fast reaction to changes - the files become almost immediately available to remote Could access the EOS directories from my editor (vsCODE).","title":"pros"},{"location":"guides/eos_on_mac/#cons_1","text":"It seems I have to redo the mount once the kerberos tokens expire I dit not manage to mount both a user directory and a project EOS space. Ilias suggested in the mattermost channel https://mattermost.web.cern.ch/abpcomputing/pl/5p54ieirofdxf8caejuy64emuy to use the following script #!/bin/bash # # Script to initialize and mount the EOS # # (c) Ilias Efthymiopoulos - Feb 2020 # # Arguments : [1] my CERN password # # # -- get access to EOS from previously stored kerberos password kinit -kt ~/.keytab efthymio@CERN.CH export EOS_MGM_URL = root://eoshome-e.cern.ch export EOS_FUSE_MGM_ALIAS = eoshome-e.cern.ch export EOS_FUSE_MOUNTDIR = /Users/iliasefthymiopoulos/eoshome-e eos fuse mount /Users/iliasefthymiopoulos/eoshome-e/ export EOS_MGM_URL = root://eosproject-l.cern.ch export EOS_FUSE_MGM_ALIAS = eosproject-l.cern.ch export EOS_FUSE_MOUNTDIR = /Users/iliasefthymiopoulos/eosproject-l eos fuse mount /Users/iliasefthymiopoulos/eosproject-l # --- define alias for unmount alias umounteos = 'function _umnteos() { killall eosd; eos fuse umount /Users/iliasefthymiopoulos/$1 }; _umnteos' and Joschua proposed to use also a function approach to simplify it function umounteos () { killall eosd ; eos fuse umount /Users/iliasefthymiopoulos/eos $1 } ; alias umounteosall = 'umounteos project-l; umounteos home-e' or even function mounteos () { export EOS_MGM_URL = root://eos $1 .cern.ch export EOS_FUSE_MGM_ALIAS = eos $1 .cern.ch export EOS_FUSE_MOUNTDIR = /Users/iliasefthymiopoulos/eos $1 eos fuse mount /Users/iliasefthymiopoulos/eos $1 } ; alias mounteosall = 'mounteos project-l; mounteos home-e'","title":"cons"},{"location":"guides/gitinfo/","text":"","title":"Gitinfo"},{"location":"guides/htcondor/","text":"Submitting Jobs to HTCondor from a local Machine The recommended way of using HTCondor is to submit jobs by logging to lxplus.cern.ch. It is also possible to configure your own computer to manage HTCondor jobs, as described in this guide. These notes refer to Ubuntu 16.04 LTS, 18.04 LTS and 20.04 LTS and includes possible caveats. If you have a different Linux distribution, steps might be the same, but syntax may change. Sudo rights are needed. As pre-requisite, you will need to install a kerberos client on your desktop; afterwards, you can proceed with the installation of HTCondor Install kerberos install user and developer packages and add lxplus credential components (when asked, default realm is CERN.CH ): sudo apt install krb5-user libkrb5-dev libauthen-krb5-perl scp $USERNAME @lxplus.cern.ch:/usr/bin/batch_krb5_credential . chmod +x batch_krb5_credential sudo mv batch_krb5_credential /usr/bin/ scp $USERNAME @lxplus.cern.ch:/etc/ngauth_batch_crypt_pub.pem . sudo mv ngauth_batch_crypt_pub.pem /etc/ scp $USERNAME @lxplus.cern.ch:/etc/krb5.conf.no_rdns . sudo mv krb5.conf.no_rdns /etc/krb5.conf.no_rdns scp $USERNAME @lxplus.cern.ch:/etc/sysconfig/ngbauth-submit . sudo mkdir /etc/sysconfig/ sudo mv ngbauth-submit /etc/sysconfig/ Confirm installation Before this step make sure you have valid credentials already (i.e. run kinit ). Then check that the kerberos components are properly installed and set-up (the script will tell you the missing perl packages): /usr/bin/batch_krb5_credential There should be an output like: -----BEGIN NGAUTH COMPOSITE----- # LOTS OF LINES OF YOUR KEY -----END NGAUTH COMPOSITE----- and nothing else (i.e. no missing files or errors). Debugging the kerberos installation if the last step does not deliver the desired output, /usr/bin/batch_krb5_credential might have to be modified. Some things can be tried: change the line my $principalName = \"ngauth/SOMESERVER\" ; into my $principalName = \"ngauth/ngauth.cern.ch\" ; install missing perl components: perl -MCPAN -e 'install Authen::Krb5' on Ubuntu 20.04 neither of these steps helped, as the Authen:Krb5 package was not available. Try getting a new version of batch_krb5_credential directly from lxplus8 : scp $USERNAME @lxplus8.cern.ch:/usr/bin/batch_krb5_credential . chmod +x batch_krb5_credential sudo mv batch_krb5_credential /usr/bin/ or try manual fix of Authen:Krb5 issue by replacing the lines: my $newCreds = Authen::Krb5::cc_resolve ( \"FILE:\" . $tgt_fn ) ; $newCreds ->initialize ( $credCache ->get_principal ()) ; Authen::Krb5::cc_copy_creds ( $credCache , $newCreds ) ; with copy ( $tgt , $tgt_fn ) ; Install HTCondor On Ubuntu 18.04+ it is usually enough to install condor from the packaged version Ubuntu 20.04 sudo apt update sudo apt install htcondor Ubuntu 18.04 sudo apt-get update sudo apt-get install condor If you need a more recent version, or in case of Ubuntu 16.04, use the resources on the web-page of HTCondor . Debugging the HTCondor installation It may happen that at sudo apt-get update , you get the error message: N: Skipping acquire of configured file 'contrib/binary-i386/Packages' as repository 'http://research.cs.wisc.edu/htcondor/ubuntu/stable trusty InRelease' doesn 't support architecture ' i386 ' In case your system is actually 64bit, a common solution is to limit the research of the package distro to just 64 bit by introducing the [arch=amd64] in the list of sources (in /etc/apt/sources.list ), e.g. deb [ arch = amd64 ] http://research.cs.wisc.edu/htcondor/ubuntu/stable/ trusty contrib Configure HTCondor create the config file /etc/condor/config.d/10-local.config . Please set as scheduler ( SCHEDD_HOST ) the default one you get on lxplus , e.g. in your condor_q output. You can also find it out by running (on lxplus ): condor_config_val SCHEDD_HOST An example content is provided here: CONDOR_HOST = tweetybird03.cern.ch, tweetybird04.cern.ch COLLECTOR_HOST = tweetybird03.cern.ch, tweetybird04.cern.ch SCHEDD_HOST = bigbirdXX.cern.ch SCHEDD_NAME = $( SCHEDD_HOST ) SEC_CLIENT_AUTHENTICATION_METHODS = KERBEROS SEC_CREDENTIAL_PRODUCER = /usr/bin/batch_krb5_credential CREDD_HOST = $( SCHEDD_HOST ) FILESYSTEM_DOMAIN = cern.ch UID_DOMAIN = cern.ch restart HTCondor : /etc/init.d/condor restart Debugging the HTCondor configuration Useful: The full configuration can be checked by condor_config_val -dump If you have connection problems when running condor_q or condor_status , you might want to check your NETWORK_INTERFACE . condor_config_val NETWORK_INTERFACE In some cases it might be set to 127.0.0.1 or similar. Yet it should be set to * . If this is not the case, simply add the appropriate line at the end of your configuration file (from above): NETWORK_INTERFACE = * Don't forget to restart HTCondor . Pay attention to the couple COLLECTOR_HOST and SCHEDD_HOST , as, depending on the collector, you may be able to reach only a sub-set of the scheduler. To get the whole lists, please login to lxplus.cern.ch and type: schedulers condor_status -sched collectors condor_status -collector","title":"Submitting Jobs to HTCondor from a local Machine"},{"location":"guides/htcondor/#submitting-jobs-to-htcondor-from-a-local-machine","text":"The recommended way of using HTCondor is to submit jobs by logging to lxplus.cern.ch. It is also possible to configure your own computer to manage HTCondor jobs, as described in this guide. These notes refer to Ubuntu 16.04 LTS, 18.04 LTS and 20.04 LTS and includes possible caveats. If you have a different Linux distribution, steps might be the same, but syntax may change. Sudo rights are needed. As pre-requisite, you will need to install a kerberos client on your desktop; afterwards, you can proceed with the installation of HTCondor","title":"Submitting Jobs to HTCondor from a local Machine"},{"location":"guides/htcondor/#install-kerberos","text":"install user and developer packages and add lxplus credential components (when asked, default realm is CERN.CH ): sudo apt install krb5-user libkrb5-dev libauthen-krb5-perl scp $USERNAME @lxplus.cern.ch:/usr/bin/batch_krb5_credential . chmod +x batch_krb5_credential sudo mv batch_krb5_credential /usr/bin/ scp $USERNAME @lxplus.cern.ch:/etc/ngauth_batch_crypt_pub.pem . sudo mv ngauth_batch_crypt_pub.pem /etc/ scp $USERNAME @lxplus.cern.ch:/etc/krb5.conf.no_rdns . sudo mv krb5.conf.no_rdns /etc/krb5.conf.no_rdns scp $USERNAME @lxplus.cern.ch:/etc/sysconfig/ngbauth-submit . sudo mkdir /etc/sysconfig/ sudo mv ngbauth-submit /etc/sysconfig/","title":"Install kerberos"},{"location":"guides/htcondor/#confirm-installation","text":"Before this step make sure you have valid credentials already (i.e. run kinit ). Then check that the kerberos components are properly installed and set-up (the script will tell you the missing perl packages): /usr/bin/batch_krb5_credential There should be an output like: -----BEGIN NGAUTH COMPOSITE----- # LOTS OF LINES OF YOUR KEY -----END NGAUTH COMPOSITE----- and nothing else (i.e. no missing files or errors).","title":"Confirm installation"},{"location":"guides/htcondor/#debugging-the-kerberos-installation","text":"if the last step does not deliver the desired output, /usr/bin/batch_krb5_credential might have to be modified. Some things can be tried: change the line my $principalName = \"ngauth/SOMESERVER\" ; into my $principalName = \"ngauth/ngauth.cern.ch\" ; install missing perl components: perl -MCPAN -e 'install Authen::Krb5' on Ubuntu 20.04 neither of these steps helped, as the Authen:Krb5 package was not available. Try getting a new version of batch_krb5_credential directly from lxplus8 : scp $USERNAME @lxplus8.cern.ch:/usr/bin/batch_krb5_credential . chmod +x batch_krb5_credential sudo mv batch_krb5_credential /usr/bin/ or try manual fix of Authen:Krb5 issue by replacing the lines: my $newCreds = Authen::Krb5::cc_resolve ( \"FILE:\" . $tgt_fn ) ; $newCreds ->initialize ( $credCache ->get_principal ()) ; Authen::Krb5::cc_copy_creds ( $credCache , $newCreds ) ; with copy ( $tgt , $tgt_fn ) ;","title":"Debugging the kerberos installation"},{"location":"guides/htcondor/#install-htcondor","text":"On Ubuntu 18.04+ it is usually enough to install condor from the packaged version Ubuntu 20.04 sudo apt update sudo apt install htcondor Ubuntu 18.04 sudo apt-get update sudo apt-get install condor If you need a more recent version, or in case of Ubuntu 16.04, use the resources on the web-page of HTCondor .","title":"Install HTCondor"},{"location":"guides/htcondor/#debugging-the-htcondor-installation","text":"It may happen that at sudo apt-get update , you get the error message: N: Skipping acquire of configured file 'contrib/binary-i386/Packages' as repository 'http://research.cs.wisc.edu/htcondor/ubuntu/stable trusty InRelease' doesn 't support architecture ' i386 ' In case your system is actually 64bit, a common solution is to limit the research of the package distro to just 64 bit by introducing the [arch=amd64] in the list of sources (in /etc/apt/sources.list ), e.g. deb [ arch = amd64 ] http://research.cs.wisc.edu/htcondor/ubuntu/stable/ trusty contrib","title":"Debugging the HTCondor installation"},{"location":"guides/htcondor/#configure-htcondor","text":"create the config file /etc/condor/config.d/10-local.config . Please set as scheduler ( SCHEDD_HOST ) the default one you get on lxplus , e.g. in your condor_q output. You can also find it out by running (on lxplus ): condor_config_val SCHEDD_HOST An example content is provided here: CONDOR_HOST = tweetybird03.cern.ch, tweetybird04.cern.ch COLLECTOR_HOST = tweetybird03.cern.ch, tweetybird04.cern.ch SCHEDD_HOST = bigbirdXX.cern.ch SCHEDD_NAME = $( SCHEDD_HOST ) SEC_CLIENT_AUTHENTICATION_METHODS = KERBEROS SEC_CREDENTIAL_PRODUCER = /usr/bin/batch_krb5_credential CREDD_HOST = $( SCHEDD_HOST ) FILESYSTEM_DOMAIN = cern.ch UID_DOMAIN = cern.ch restart HTCondor : /etc/init.d/condor restart","title":"Configure HTCondor"},{"location":"guides/htcondor/#debugging-the-htcondor-configuration","text":"Useful: The full configuration can be checked by condor_config_val -dump If you have connection problems when running condor_q or condor_status , you might want to check your NETWORK_INTERFACE . condor_config_val NETWORK_INTERFACE In some cases it might be set to 127.0.0.1 or similar. Yet it should be set to * . If this is not the case, simply add the appropriate line at the end of your configuration file (from above): NETWORK_INTERFACE = * Don't forget to restart HTCondor . Pay attention to the couple COLLECTOR_HOST and SCHEDD_HOST , as, depending on the collector, you may be able to reach only a sub-set of the scheduler. To get the whole lists, please login to lxplus.cern.ch and type: schedulers condor_status -sched collectors condor_status -collector","title":"Debugging the HTCondor configuration"},{"location":"guides/m1_macbook/","text":"M1 Macbook compared to Intel Macbook From Hamish Graham Useful link to see which MacOs Apps are optmised for M1 Apple Silicon Macbooks: https://isapplesiliconready.com/ BE-ABP Docker I tried to use the docker https://gitlab.cern.ch/abpcomputing/sandbox/be-abp-docker with the M1. Docker has not been optimised yet for the M1 chip, at the moment it uses Rosetta 2 to operate When running a Docker image, a warning is produced: WARNING: The requested image ' s platform ( linux/amd64 ) does not match the detected host platform ( linux/arm64/v8 ) and no specific platform was requested The container will be running but not working properly, for example tmux does not work in the container, neither eos . In conclusion, I cannot at the moment use the BE-ABP Docker with my M1 Macbook. AFS Auristor works on the Macbook M1, however it is a bit more complicated to install vs the Intel Macbook This link provides the necessary steps for installing auristor on the Macbook M1: https://blog.auristor.com/2021/01/installing-auristorfs-clients-for-macos.html There is a bug for MacOs Big Sur where the Auristor icon does not show a \"tick\" even if it is running. This bug occurs for M1 and Intel Macbooks running Big Sur. Battery Life The 13 inch Macbook pro M1 has twice as long of a battery life compared to the Macbook pro Intel, according to Business Insider: https://www.businessinsider.com/macbook-pro-13-2020-m1-vs-macbook-pro-13-2020-intel?r=US&IR=T The battery life is estimated to be 20 hours for M1 and 10 hours for Intel (this depends what you are using the computer for, if docker is running for example, the battery life will be shorter)","title":"M1 MacBook experience"},{"location":"guides/m1_macbook/#m1-macbook-compared-to-intel-macbook","text":"From Hamish Graham Useful link to see which MacOs Apps are optmised for M1 Apple Silicon Macbooks: https://isapplesiliconready.com/","title":"M1 Macbook compared to Intel Macbook"},{"location":"guides/m1_macbook/#be-abp-docker","text":"I tried to use the docker https://gitlab.cern.ch/abpcomputing/sandbox/be-abp-docker with the M1. Docker has not been optimised yet for the M1 chip, at the moment it uses Rosetta 2 to operate When running a Docker image, a warning is produced: WARNING: The requested image ' s platform ( linux/amd64 ) does not match the detected host platform ( linux/arm64/v8 ) and no specific platform was requested The container will be running but not working properly, for example tmux does not work in the container, neither eos . In conclusion, I cannot at the moment use the BE-ABP Docker with my M1 Macbook.","title":"BE-ABP Docker"},{"location":"guides/m1_macbook/#afs","text":"Auristor works on the Macbook M1, however it is a bit more complicated to install vs the Intel Macbook This link provides the necessary steps for installing auristor on the Macbook M1: https://blog.auristor.com/2021/01/installing-auristorfs-clients-for-macos.html There is a bug for MacOs Big Sur where the Auristor icon does not show a \"tick\" even if it is running. This bug occurs for M1 and Intel Macbooks running Big Sur.","title":"AFS"},{"location":"guides/m1_macbook/#battery-life","text":"The 13 inch Macbook pro M1 has twice as long of a battery life compared to the Macbook pro Intel, according to Business Insider: https://www.businessinsider.com/macbook-pro-13-2020-m1-vs-macbook-pro-13-2020-intel?r=US&IR=T The battery life is estimated to be 20 hours for M1 and 10 hours for Intel (this depends what you are using the computer for, if docker is running for example, the battery life will be shorter)","title":"Battery Life"},{"location":"guides/mkdocs_site/","text":"How to create and manage an MkDocs website (like this one) Page by G. Gamba, G. Sterbini and N. Mounet Introduction The website content is hosted in the form of markdown files on GitLab . The MkDocs generator automatically can be used to generate the formatted website from the markdown file. The website is hosted by the CERN web services and The openshift cloud platform is used to automatically update the website when changes are made int he GitLab repository. Additional documentation Detailed documentation can be found: In the CERN IT documentation for MkDocs websites In the official MKdocs documentation In the documentation of the material theme (see in particular the Setup section) How to create the website To setup an MKDocs website, the steps proposed in the CERN MkDocs documentation can be followed: Create a GitLab repository. The easiest is to import an existing project as explained here . The example proposed ( https://gitlab.cern.ch/authoring/documentation/mkdocs-container-example.git ) is fine. Create a new project on webeos.cern.ch. To do so follow the instructions here . First create the project, then, staying in the Administrator view, go to Operators -> Installed Operators , then Gitlab Pages Site Operator . Make sure you are in the correct project, and to put the right url in Host (it is advised to end it with .docs.cern.ch for new sites) when you create the instance. If you made any mistake, you can modify after creation in the following way: go to Installed operators again (make sure to be in the Administrator view), click on Publish a static site from a Gitlab repository (gitlab.cern.ch) on the right, click on the name of your project in the middle, click on the YAML tab on top, manually modify the YAML file (for instance, the host url is in spec: host ). Back on your gitlab repository, set up the Gitlab Pages by following the instructions here . Depending how you created your gitlab repository, you might need to follow the instructions about building artefacts , or creating a pipeline, or at least running manually the pipeline the first time. Optionally, restrict the website access to certain e-group using the instructions here . That should be it. You can use this website as a template. Tip (from Sondre) : To access advanced customization features you might have to pin the theme version by including \"requirements.txt\" file. See for example the COMBI documentation website . Access from outside CERN By default, your website is accessible only from within CERN. To make it accessible from the internet, you need to change the site visibility from \"intranet\" to \"internet\". This can be done in the section Access and Permission on the site management page ( https://webservices.web.cern.ch -> manage my website -> select [your site]). How to edit the website Te website can be edited: - Directly on GitLab, within the browser. - Locally, using git to clone the repository and push the modifications to GitLab. Visual Studio Code , which is free and available on Windows, Linux and Mac, is convenient to do the editing, providing features like a spell checker and a realtime preview of the markdown rendering. Advanced features Run the website as a local server It is quite slow to edit your pages on gitlab website or even locally, then pushing them to git, and let the OpenShift server recompile the website for you. It is possible to have a local version of the site compiled directly. To do so you need to intall MkDocs within a recent python installation (python 3.5 or later). Install MkDocs Mkdocs can be installed with pip: pip install mkdocs Then please install some additional mkdocs theme: pip install mkdocs-material If you are using the multi-language package i18n (as in the example provided on CERN gitlab: https://gitlab.cern.ch/authoring/documentation/mkdocs-container-example.git ), then also install it: pip install mkdocs-i18n Tip : in this case, to get the contents on the left translated properly, you might need to modify mkdocs.yml given in https://gitlab.cern.ch/authoring/documentation/mkdocs-container-example.git , replacing nav_translations by translate_nav (see example ). Compile your website Clone a local version of your repository git clone https://gitlab.cern.ch/abpcomputing/abpcpweb.git Then move on the abpcpweb folder cd abpcpweb and start the mkdocs server by mkdocs serve You can now access the site on your browser at http://127.0.0.1:8000/ . You can also use another address (if e.g. this one is already in use) with e.g. mkdocs serve --dev-addr 127 .0.0.1:8001 Changes in the markdown files will be automatically propagated to the local version of the site. To stop the server, simply do Ctrl+c, or kill the process. To update the version on the internet, you will need to commit and push your changes using git. Adding support for latex activate mymdownx.arithmatex extension that is available by default in mkdocs container provided by CERN I sugesst also to activate admonition extension (and eventually other pymdownx extensions) for a nicer website. All this is done by adding the following lines in your mkdocs.yml configuration file: markdown_extensions: - pymdownx.arithmatex - admonition extra_javascript: - 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML' If you want to add local MathJs, then you need to: 1. download to your repository the latest version of MathJax. git clone https://github.com/mathjax/MathJax.git 2. add everything inside your docs folder, e.g. under docs/js/MathJax-2.7.5 3. replace the extra_javascript: configuration as following: extra_javascript: - 'js/MathJax-2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' Using different branch Edit your \"build\" YAML configuration file on openshift:","title":"Create a MkDocs website (like this one)"},{"location":"guides/mkdocs_site/#how-to-create-and-manage-an-mkdocs-website-like-this-one","text":"Page by G. Gamba, G. Sterbini and N. Mounet","title":"How to create and manage an MkDocs website (like this one)"},{"location":"guides/mkdocs_site/#introduction","text":"The website content is hosted in the form of markdown files on GitLab . The MkDocs generator automatically can be used to generate the formatted website from the markdown file. The website is hosted by the CERN web services and The openshift cloud platform is used to automatically update the website when changes are made int he GitLab repository.","title":"Introduction"},{"location":"guides/mkdocs_site/#additional-documentation","text":"Detailed documentation can be found: In the CERN IT documentation for MkDocs websites In the official MKdocs documentation In the documentation of the material theme (see in particular the Setup section)","title":"Additional documentation"},{"location":"guides/mkdocs_site/#how-to-create-the-website","text":"To setup an MKDocs website, the steps proposed in the CERN MkDocs documentation can be followed: Create a GitLab repository. The easiest is to import an existing project as explained here . The example proposed ( https://gitlab.cern.ch/authoring/documentation/mkdocs-container-example.git ) is fine. Create a new project on webeos.cern.ch. To do so follow the instructions here . First create the project, then, staying in the Administrator view, go to Operators -> Installed Operators , then Gitlab Pages Site Operator . Make sure you are in the correct project, and to put the right url in Host (it is advised to end it with .docs.cern.ch for new sites) when you create the instance. If you made any mistake, you can modify after creation in the following way: go to Installed operators again (make sure to be in the Administrator view), click on Publish a static site from a Gitlab repository (gitlab.cern.ch) on the right, click on the name of your project in the middle, click on the YAML tab on top, manually modify the YAML file (for instance, the host url is in spec: host ). Back on your gitlab repository, set up the Gitlab Pages by following the instructions here . Depending how you created your gitlab repository, you might need to follow the instructions about building artefacts , or creating a pipeline, or at least running manually the pipeline the first time. Optionally, restrict the website access to certain e-group using the instructions here . That should be it. You can use this website as a template. Tip (from Sondre) : To access advanced customization features you might have to pin the theme version by including \"requirements.txt\" file. See for example the COMBI documentation website .","title":"How to create the website"},{"location":"guides/mkdocs_site/#access-from-outside-cern","text":"By default, your website is accessible only from within CERN. To make it accessible from the internet, you need to change the site visibility from \"intranet\" to \"internet\". This can be done in the section Access and Permission on the site management page ( https://webservices.web.cern.ch -> manage my website -> select [your site]).","title":"Access from outside CERN"},{"location":"guides/mkdocs_site/#how-to-edit-the-website","text":"Te website can be edited: - Directly on GitLab, within the browser. - Locally, using git to clone the repository and push the modifications to GitLab. Visual Studio Code , which is free and available on Windows, Linux and Mac, is convenient to do the editing, providing features like a spell checker and a realtime preview of the markdown rendering.","title":"How to edit the website"},{"location":"guides/mkdocs_site/#advanced-features","text":"","title":"Advanced features"},{"location":"guides/mkdocs_site/#run-the-website-as-a-local-server","text":"It is quite slow to edit your pages on gitlab website or even locally, then pushing them to git, and let the OpenShift server recompile the website for you. It is possible to have a local version of the site compiled directly. To do so you need to intall MkDocs within a recent python installation (python 3.5 or later).","title":"Run the website as a local server"},{"location":"guides/mkdocs_site/#install-mkdocs","text":"Mkdocs can be installed with pip: pip install mkdocs Then please install some additional mkdocs theme: pip install mkdocs-material If you are using the multi-language package i18n (as in the example provided on CERN gitlab: https://gitlab.cern.ch/authoring/documentation/mkdocs-container-example.git ), then also install it: pip install mkdocs-i18n Tip : in this case, to get the contents on the left translated properly, you might need to modify mkdocs.yml given in https://gitlab.cern.ch/authoring/documentation/mkdocs-container-example.git , replacing nav_translations by translate_nav (see example ).","title":"Install MkDocs"},{"location":"guides/mkdocs_site/#compile-your-website","text":"Clone a local version of your repository git clone https://gitlab.cern.ch/abpcomputing/abpcpweb.git Then move on the abpcpweb folder cd abpcpweb and start the mkdocs server by mkdocs serve You can now access the site on your browser at http://127.0.0.1:8000/ . You can also use another address (if e.g. this one is already in use) with e.g. mkdocs serve --dev-addr 127 .0.0.1:8001 Changes in the markdown files will be automatically propagated to the local version of the site. To stop the server, simply do Ctrl+c, or kill the process. To update the version on the internet, you will need to commit and push your changes using git.","title":"Compile your website"},{"location":"guides/mkdocs_site/#adding-support-for-latex","text":"activate mymdownx.arithmatex extension that is available by default in mkdocs container provided by CERN I sugesst also to activate admonition extension (and eventually other pymdownx extensions) for a nicer website. All this is done by adding the following lines in your mkdocs.yml configuration file: markdown_extensions: - pymdownx.arithmatex - admonition extra_javascript: - 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML' If you want to add local MathJs, then you need to: 1. download to your repository the latest version of MathJax. git clone https://github.com/mathjax/MathJax.git 2. add everything inside your docs folder, e.g. under docs/js/MathJax-2.7.5 3. replace the extra_javascript: configuration as following: extra_javascript: - 'js/MathJax-2.7.5/MathJax.js?config=TeX-MML-AM_CHTML'","title":"Adding support for latex"},{"location":"guides/mkdocs_site/#using-different-branch","text":"Edit your \"build\" YAML configuration file on openshift:","title":"Using different branch"},{"location":"guides/nxcals_lxplus/","text":"Using NXCALS via pyspark on lxplus Prepared by Ilias Introduction Ilias installed the NXCALS bundle to our luminosity follow-up service account lumimod . You need to have granted the access to NXCALS service and be in the egroup it-hadoop-nxcals-pro-analytics . Please follow the instruction here . As the software/bundles are installed in a public area, they can be used from any lxplus account. Ilias provided some shell scripts to facilitate the use: Go in the your working folder (some link/subfolders will be created). Use the script: source /afs/cern.ch/user/l/lumimod/public/nxcals/nxcals_lumimod_conda.sh to initialize the conda installation, and then use the command: conda activate nxcals-lcg95-env to activate the environment. The two commands above need to be repeated for any new shell. The following command, instead, needs to be used only once to set in your folder the soft links to the nxcals bundle files: source /afs/cern.ch/user/l/lumimod/public/nxcals/nxcals_setupdir.sh It is done. Accessing NXCALS data In a configured directory with the commands above, one can use the pre-defined commands of the NXCALS bundle to: run an interactive session with pySpark ./spark-home/bin/pyspark that can be further configured with additional options available from the help menu ./spark-home/bin/pyspark --help A simple command to test could be: from cern.nxcals.api.extraction.data.builders import * data = DataQuery.builder(spark).byVariables() \\ .system('CMW') \\ .startTime('2018-07-20 13:38:00.000').endTime('2018-07-20 13:39:00.000') \\ .variable('LHC.BOFSU:TUNE_B1_V') \\ .build() data.show(5) run interactively a python script as standalone application (without using yarn ): ./spark-home/bin/spark-submit test.py that an be also put to a crontab job or run in HTC-condor. An example of test.py is the following: # # -- simple test file # from pyspark import SparkConf from pyspark import SparkContext from pyspark.sql import SparkSession from cern.nxcals.api.extraction.data.builders import * conf = SparkConf() # Possible master values (the location to run the application): # local: Run Spark locally with one worker thread (that is, no parallelism). # local[K]: Run Spark locally with K worker threads. (Ideally, set this to the number of cores on your host.) # local[*]: Run Spark locally with as many worker threads as logical cores on your host. # yarn: Run using a YARN cluster manager. # conf.setMaster('yarn') conf.setMaster('local[*]') conf.setAppName('spark-basic') sc = SparkContext(conf=conf) spark = SparkSession(sc) intensity = DevicePropertyDataQuery.builder(spark).system(\"CMW\") \\ .startTime(\"2018-06-17 00:00:00.000\").endTime(\"2018-06-20 00:00:00.000\") \\ .entity().parameter(\"PR.BCT/HotspotIntensity\").build() # count the data points print('>>> data : ', intensity.count()) intensity.show() run interactively a python script as standalone application (using yarn ) by using ./spark-home/bin/spark-submit --master yarn --executor-memory 4G --total-executor-cores 8 test_yarn.py An example of test_yarn.py is the following. from pyspark import SparkConf from pyspark import SparkContext from pyspark.sql import SparkSession from cern.nxcals.api.extraction.data.builders import * from pyspark.sql.functions import col from pyspark.sql.types import StructType from pyspark.sql.types import StructField from pyspark.sql.types import DoubleType from pyspark.sql.types import ArrayType from pyspark.sql.window import Window from pyspark.sql.functions import rank, col import pyspark.sql.functions as func import time from matplotlib import pyplot as plt import numpy as np import pandas as pd import os # --- Configure spark conf = SparkConf() # Possible master values (the location to run the application): # local: Run Spark locally with one worker thread (that is, no parallelism). # local[K]: Run Spark locally with K worker threads. (Ideally, set this to the number of cores on your host.) # local[*]: Run Spark locally with as many worker threads as logical cores on your host. # yarn: Run using a YARN cluster manager. conf.setMaster('yarn') # conf.setMaster('local[*]') conf.setAppName('spark-basic') sc = SparkContext(conf=conf) spark = SparkSession(sc) # --- initial settings tstart = pd.Timestamp('2018-10-23 11:22:40.094000101+0000', tz='UTC') tend = pd.Timestamp('2018-10-23 13:36:20.035000086+0000', tz='UTC') variable = 'LHC.BCTFR.A6R4.B1:BUNCH_FILL_PATTERN' variable_spark = 'LHC@BCTFR@A6R4@B1:BUNCH_FILL_PATTERN' t1 = tstart.tz_convert('UTC').tz_localize(None) t2 = tend.tz_convert('UTC').tz_localize(None) # --- get the data ds = DataQuery.builder(spark).byVariables().system(\"CMW\").startTime(t1).endTime(t2).variable(variable).buildDataset() auxdf = ds.select('nxcals_timestamp','nxcals_value.elements').withColumnRenamed('nxcals_timestamp','timestamp') \\ .withColumnRenamed('elements',variable_spark) print('\\n\\nThis is the result:\\n') print(auxdf.count()) Install your own NXCALS bundle If you want to install your own bundle you can follow the following steps. Install miniconda Start with a miniconda installation as minimal set. Follow the installation instructions from the conda web page and linux installation download the Miniconda3 Linux 64bit execute it with bash Miniconda-latest-Linux-x86_64.sh and follow the installer prompts use the ~lumimod/public/miniconda3 as the installation directory removed the conda initialization lines from .bashrc and put them in a separate file (see below) Install nxcals bundle Follow the instructions from NXCALS Documentation . Navigate to: Public APIs > Data Access Methods > NXCALS Spark bundle Created the installation script install_nxcals.sh #!/bin/bash curl -s -k -O http://photons-resources.cern.ch/downloads/nxcals_pro/spark/spark-nxcals.zip unzip spark-nxcals.zip rm spark-nxcals.zip cd spark-*-bin-hadoop2.7 and install the software in ~/lumimod/public/nxcals (to replace with your desired path) directory. To remain compatible with NXCALS installation in SWAN, you can configure the python environment as LCG95. Create a minimal set for our needs in lcg95_basic.txt pandas==0.23.3 numpy==1.14.2 matplotlib==2.2.2 pyarrow==0.8.0 Create the environment with: conda create -n nxcals-lcg95-env --file lcg95_basic.txt Then to use it with the commands: conda env list ! list the available environments conda activate nxcals-lcg95-env ! activate an environment conda deactivate ! when done to exit the environment That's it.","title":"Using NXCALS via pyspark on lxplus"},{"location":"guides/nxcals_lxplus/#using-nxcals-via-pyspark-on-lxplus","text":"Prepared by Ilias","title":"Using NXCALS via pyspark on lxplus"},{"location":"guides/nxcals_lxplus/#introduction","text":"Ilias installed the NXCALS bundle to our luminosity follow-up service account lumimod . You need to have granted the access to NXCALS service and be in the egroup it-hadoop-nxcals-pro-analytics . Please follow the instruction here . As the software/bundles are installed in a public area, they can be used from any lxplus account. Ilias provided some shell scripts to facilitate the use: Go in the your working folder (some link/subfolders will be created). Use the script: source /afs/cern.ch/user/l/lumimod/public/nxcals/nxcals_lumimod_conda.sh to initialize the conda installation, and then use the command: conda activate nxcals-lcg95-env to activate the environment. The two commands above need to be repeated for any new shell. The following command, instead, needs to be used only once to set in your folder the soft links to the nxcals bundle files: source /afs/cern.ch/user/l/lumimod/public/nxcals/nxcals_setupdir.sh It is done.","title":"Introduction"},{"location":"guides/nxcals_lxplus/#accessing-nxcals-data","text":"In a configured directory with the commands above, one can use the pre-defined commands of the NXCALS bundle to: run an interactive session with pySpark ./spark-home/bin/pyspark that can be further configured with additional options available from the help menu ./spark-home/bin/pyspark --help A simple command to test could be: from cern.nxcals.api.extraction.data.builders import * data = DataQuery.builder(spark).byVariables() \\ .system('CMW') \\ .startTime('2018-07-20 13:38:00.000').endTime('2018-07-20 13:39:00.000') \\ .variable('LHC.BOFSU:TUNE_B1_V') \\ .build() data.show(5) run interactively a python script as standalone application (without using yarn ): ./spark-home/bin/spark-submit test.py that an be also put to a crontab job or run in HTC-condor. An example of test.py is the following: # # -- simple test file # from pyspark import SparkConf from pyspark import SparkContext from pyspark.sql import SparkSession from cern.nxcals.api.extraction.data.builders import * conf = SparkConf() # Possible master values (the location to run the application): # local: Run Spark locally with one worker thread (that is, no parallelism). # local[K]: Run Spark locally with K worker threads. (Ideally, set this to the number of cores on your host.) # local[*]: Run Spark locally with as many worker threads as logical cores on your host. # yarn: Run using a YARN cluster manager. # conf.setMaster('yarn') conf.setMaster('local[*]') conf.setAppName('spark-basic') sc = SparkContext(conf=conf) spark = SparkSession(sc) intensity = DevicePropertyDataQuery.builder(spark).system(\"CMW\") \\ .startTime(\"2018-06-17 00:00:00.000\").endTime(\"2018-06-20 00:00:00.000\") \\ .entity().parameter(\"PR.BCT/HotspotIntensity\").build() # count the data points print('>>> data : ', intensity.count()) intensity.show() run interactively a python script as standalone application (using yarn ) by using ./spark-home/bin/spark-submit --master yarn --executor-memory 4G --total-executor-cores 8 test_yarn.py An example of test_yarn.py is the following. from pyspark import SparkConf from pyspark import SparkContext from pyspark.sql import SparkSession from cern.nxcals.api.extraction.data.builders import * from pyspark.sql.functions import col from pyspark.sql.types import StructType from pyspark.sql.types import StructField from pyspark.sql.types import DoubleType from pyspark.sql.types import ArrayType from pyspark.sql.window import Window from pyspark.sql.functions import rank, col import pyspark.sql.functions as func import time from matplotlib import pyplot as plt import numpy as np import pandas as pd import os # --- Configure spark conf = SparkConf() # Possible master values (the location to run the application): # local: Run Spark locally with one worker thread (that is, no parallelism). # local[K]: Run Spark locally with K worker threads. (Ideally, set this to the number of cores on your host.) # local[*]: Run Spark locally with as many worker threads as logical cores on your host. # yarn: Run using a YARN cluster manager. conf.setMaster('yarn') # conf.setMaster('local[*]') conf.setAppName('spark-basic') sc = SparkContext(conf=conf) spark = SparkSession(sc) # --- initial settings tstart = pd.Timestamp('2018-10-23 11:22:40.094000101+0000', tz='UTC') tend = pd.Timestamp('2018-10-23 13:36:20.035000086+0000', tz='UTC') variable = 'LHC.BCTFR.A6R4.B1:BUNCH_FILL_PATTERN' variable_spark = 'LHC@BCTFR@A6R4@B1:BUNCH_FILL_PATTERN' t1 = tstart.tz_convert('UTC').tz_localize(None) t2 = tend.tz_convert('UTC').tz_localize(None) # --- get the data ds = DataQuery.builder(spark).byVariables().system(\"CMW\").startTime(t1).endTime(t2).variable(variable).buildDataset() auxdf = ds.select('nxcals_timestamp','nxcals_value.elements').withColumnRenamed('nxcals_timestamp','timestamp') \\ .withColumnRenamed('elements',variable_spark) print('\\n\\nThis is the result:\\n') print(auxdf.count())","title":"Accessing NXCALS data"},{"location":"guides/nxcals_lxplus/#install-your-own-nxcals-bundle","text":"If you want to install your own bundle you can follow the following steps.","title":"Install your own NXCALS bundle"},{"location":"guides/nxcals_lxplus/#install-miniconda","text":"Start with a miniconda installation as minimal set. Follow the installation instructions from the conda web page and linux installation download the Miniconda3 Linux 64bit execute it with bash Miniconda-latest-Linux-x86_64.sh and follow the installer prompts use the ~lumimod/public/miniconda3 as the installation directory removed the conda initialization lines from .bashrc and put them in a separate file (see below)","title":"Install miniconda"},{"location":"guides/nxcals_lxplus/#install-nxcals-bundle","text":"Follow the instructions from NXCALS Documentation . Navigate to: Public APIs > Data Access Methods > NXCALS Spark bundle Created the installation script install_nxcals.sh #!/bin/bash curl -s -k -O http://photons-resources.cern.ch/downloads/nxcals_pro/spark/spark-nxcals.zip unzip spark-nxcals.zip rm spark-nxcals.zip cd spark-*-bin-hadoop2.7 and install the software in ~/lumimod/public/nxcals (to replace with your desired path) directory. To remain compatible with NXCALS installation in SWAN, you can configure the python environment as LCG95. Create a minimal set for our needs in lcg95_basic.txt pandas==0.23.3 numpy==1.14.2 matplotlib==2.2.2 pyarrow==0.8.0 Create the environment with: conda create -n nxcals-lcg95-env --file lcg95_basic.txt Then to use it with the commands: conda env list ! list the available environments conda activate nxcals-lcg95-env ! activate an environment conda deactivate ! when done to exit the environment That's it.","title":"Install nxcals bundle"},{"location":"guides/openafs/","text":"Installation of OpenAFS Installation on Debian/Ubuntu (prepared by V. Olsen - 2017-12-06) Tested on Debian Stretch Install Packages $ sudo apt install openafs-client openafs-modules-dkms openafs-krb5 krb5-user krb5-config Issue with obsolete packages (R. De Maria) Presently (Apr 2021) these packages installed above are obsolete and are incompatible with AFS at CERN (see Mattermost discussion . The problem can be solved by downloading a more recent version from the OpenAFS team. In Ubuntu 18.04 this can be done as follows: sudo apt install init-system-helpers/bionic-backports sudo add-apt-repository ppa:openafs/stable sudo apt-get update sudo apt-get upgrade In UBUNTU 20.04.1 it can be done as follows: sudo add-apt-repository ppa:openafs/stable sudo apt-get update sudo apt-get upgrade Configure AFS and Kerberos 1. Use \"cern.ch\" as default AFS cell $ echo \"cern.ch\" | sudo tee /etc/openafs/ThisCell 2. Set up Kerberos authentication Add the following lines to file /etc/krb5.conf : # settings for CERN.CH realm are taken from file # lxplus.cern.ch:/etc/krb5.conf [ libdefaults ] default_realm = CERN.CH [ realms ] CERN.CH = { default_domain = cern.ch kpasswd_server = cerndc.cern.ch admin_server = cerndc.cern.ch kdc = cerndc.cern.ch } [ domain_realm ] cern.ch = CERN.CH .cern.ch = CERN.CH 3. Restart OpenAFS client On Ubuntu 16.04 and above: $ sudo systemctl restart openafs-client.service On older versions: $ sudo service openafs-client restart 4. Login (optional, only needed to access protected paths): $ kinit $LOGNAME @CERN.CH # get kerberos ticket $ aklog # login to AFS cell Miscellanea Configuration steps 1) and 2) can be done with: $ sudo dpkg-reconfigure openafs-client $ sudo dpkg-reconfigure krb5-config It might be useful to set-up a crontab job (e.g. every 6h) to automatically renew the kerberos token: 0 -/6 - - - kinit -R ; aklog -c cern.ch -k CERN.CH Pay attention that kinit -R (i.e. renew existing token) won't require any password to be typed in; on the other hand, a token can be renew for a maximum of 5d after its generation, hence a kinit (with password) is needed. Anyway, if kinit is issued on Monday morning, so that for the rest of the week you don't have to bother with that. Reference: http://akorneev.web.cern.ch/akorneev/howto/openafs.txt Possible problems on Ubuntu If you have a recent Ubuntu installation, the above procedure might not entirely work as there could be a kernel incompatibility with the latest openafs. This is shown if you try aklog : it will then give the error aklog: a pioctl failed while obtaining tokens for cell cern.ch Furthermore, also a query of the openafs service with $ sudo systemctl status openafs-client.service will give errors: openafs-client-precheck [ 2963 ] : modprobe: FATAL: Module openafs not found in directory /lib/modules/4.10. openafs-client-precheck [ 2963 ] : Failed to load openafs.ko. Does it need to be built? I found a solution that worked for me, by adding a specific repository for openafs: $ sudo apt-get purge openafs-client $ sudo add-apt-repository ppa:openafs/stable $ sudo apt-get update $ sudo apt install openafs-client $ sudo apt install --reinstall openafs-modules-dkms Now we need to restart the service: $ sudo systemctl stop openafs-client.service $ kinit username@CERN.CH $ sudo systemctl start openafs-client.service You can check that the service is running as it should: $ sudo systemctl status openafs-client.service No more errors! Continue as before, aklog and possibly a crontab for kinit . Note: It may be enough to just run $ sudo dpkg-reconfigure openafs-modules-dkms Within Windows Subsystem Linux (WSL) The same error as above can occur in WSL, and while the solution is also to reconfigure the kernel modules, one needs to download the kernel source first and perform some of the build steps. A guide has been written in this gist from Joschua . Installation on MacOS (prepared by F. Van Der Veken - 2020-04-01) Mounting AFS on macOS can be a bit messy and is very poorly supported. In general it is preferred to access AFS via LxPlus and move your files to your local computer with scp . If you want to try mounting AFS anyway, there are two ways to proceed: using FUSE, or using OpenAFS. Tested on macOS Sierra (10.12.6) and Catalina (10.15.4). Using OpenAFS So far, it is not possible to install OpenAFS on macOS High Sierra (10.13). More importantly, when upgrading macOS to version 10.13 it is extremely important to deinstall OpenAFS completely before making the upgrade, to avoid a never-ending loop of kernel-panics which are due to the AFS local cache being converted to the new Apple File System (APFS). Installers can be downloaded from: macOS 10.12: http://download.sinenomine.net/openafs/bins/1.6.20/macos-10.12/ macOS 10.11: http://download.sinenomine.net/openafs/bins/1.6.20/macos-10.11/ These are not officially supported, but third-party binaries provided by Sine Nomine (more information at https://wiki.openafs.org/archive/BinaryThirdParty/ ). Older versions of macOS are officially supported and can be downloaded from http://www.openafs.org/macos.html . Update Auristor now has installers for later versions of macOS: https://www.auristor.com/openafs/client-installer/ This seems to work on Catalina, however, do not upgrade macOS without deinstalling OpenAFS first (which is then named Auristor ) to avoid disk crashes. During the installation, when asked for the local cell this is cern.ch . Give the installation permissions in System Preferences when needed. After installation, a reboot is needed. Luckily macOS comes with Kerberos pre-installed, so that's all. If you want to access protected paths, you'll have to login with Kerberos: $ kinit user@CERN.CH # get kerberos ticket $ aklog # login to AFS cell There is an OpenAFS/Auristor preference pane in your system preferences in which you can change the cell (not needed), auto-renew Kerberos tickets, and let OpenAFS start at boot. Using FUSE and SSHFS FUSE for mac and the SSHFS plugin can be downloaded from http://osxfuse.github.com . Alternatively, both can be installed using macports : $ sudo port install osxfuse ; sudo port install sshfs Verifying if OSXFUSE is installed can be done in the preferences pane, while checking if SSHFS is installed can be done by typing $ sshfs&nbsp; -h in the terminal. To mount the filesystem we have first to create a folder to hold its location. Normally all disks are mounted in /Volumes/ , however, from macOS 10.12 one needs to have root permissions to write to this location. But this is not really a problem, as a volume can be mounted at any location, so your home folder will do fine. Mounting the volume is then done by: $ mkdir ~/DISK_NAME $ sshfs USER@lxplus.cern.ch:AFSPATH ~/DISK_NAME -ovolname = DISK_NAME This can be simplified by making an alias in .bash_profile . Unmounting the volume is done in the usual way in Finder (clicking the unmount icon to the right of the volume). If for some reason unmounting does not work, the volume can be forcefully ejected by typing $ diskutil unmountDisk force ~/DISK_NAME If you want to automatically mount the drive on startup, have a look at: http://superuser.com/questions/134140/mount-an-sshfs-via-macfuse-at-boot .","title":"Installation of OpenAFS"},{"location":"guides/openafs/#installation-of-openafs","text":"","title":"Installation of OpenAFS"},{"location":"guides/openafs/#installation-on-debianubuntu","text":"(prepared by V. Olsen - 2017-12-06) Tested on Debian Stretch","title":"Installation on Debian/Ubuntu"},{"location":"guides/openafs/#install-packages","text":"$ sudo apt install openafs-client openafs-modules-dkms openafs-krb5 krb5-user krb5-config","title":"Install Packages"},{"location":"guides/openafs/#issue-with-obsolete-packages","text":"(R. De Maria) Presently (Apr 2021) these packages installed above are obsolete and are incompatible with AFS at CERN (see Mattermost discussion . The problem can be solved by downloading a more recent version from the OpenAFS team. In Ubuntu 18.04 this can be done as follows: sudo apt install init-system-helpers/bionic-backports sudo add-apt-repository ppa:openafs/stable sudo apt-get update sudo apt-get upgrade In UBUNTU 20.04.1 it can be done as follows: sudo add-apt-repository ppa:openafs/stable sudo apt-get update sudo apt-get upgrade","title":"Issue with obsolete packages"},{"location":"guides/openafs/#configure-afs-and-kerberos","text":"1. Use \"cern.ch\" as default AFS cell $ echo \"cern.ch\" | sudo tee /etc/openafs/ThisCell 2. Set up Kerberos authentication Add the following lines to file /etc/krb5.conf : # settings for CERN.CH realm are taken from file # lxplus.cern.ch:/etc/krb5.conf [ libdefaults ] default_realm = CERN.CH [ realms ] CERN.CH = { default_domain = cern.ch kpasswd_server = cerndc.cern.ch admin_server = cerndc.cern.ch kdc = cerndc.cern.ch } [ domain_realm ] cern.ch = CERN.CH .cern.ch = CERN.CH 3. Restart OpenAFS client On Ubuntu 16.04 and above: $ sudo systemctl restart openafs-client.service On older versions: $ sudo service openafs-client restart 4. Login (optional, only needed to access protected paths): $ kinit $LOGNAME @CERN.CH # get kerberos ticket $ aklog # login to AFS cell","title":"Configure AFS and Kerberos"},{"location":"guides/openafs/#miscellanea","text":"Configuration steps 1) and 2) can be done with: $ sudo dpkg-reconfigure openafs-client $ sudo dpkg-reconfigure krb5-config It might be useful to set-up a crontab job (e.g. every 6h) to automatically renew the kerberos token: 0 -/6 - - - kinit -R ; aklog -c cern.ch -k CERN.CH Pay attention that kinit -R (i.e. renew existing token) won't require any password to be typed in; on the other hand, a token can be renew for a maximum of 5d after its generation, hence a kinit (with password) is needed. Anyway, if kinit is issued on Monday morning, so that for the rest of the week you don't have to bother with that. Reference: http://akorneev.web.cern.ch/akorneev/howto/openafs.txt","title":"Miscellanea"},{"location":"guides/openafs/#possible-problems-on-ubuntu","text":"If you have a recent Ubuntu installation, the above procedure might not entirely work as there could be a kernel incompatibility with the latest openafs. This is shown if you try aklog : it will then give the error aklog: a pioctl failed while obtaining tokens for cell cern.ch Furthermore, also a query of the openafs service with $ sudo systemctl status openafs-client.service will give errors: openafs-client-precheck [ 2963 ] : modprobe: FATAL: Module openafs not found in directory /lib/modules/4.10. openafs-client-precheck [ 2963 ] : Failed to load openafs.ko. Does it need to be built? I found a solution that worked for me, by adding a specific repository for openafs: $ sudo apt-get purge openafs-client $ sudo add-apt-repository ppa:openafs/stable $ sudo apt-get update $ sudo apt install openafs-client $ sudo apt install --reinstall openafs-modules-dkms Now we need to restart the service: $ sudo systemctl stop openafs-client.service $ kinit username@CERN.CH $ sudo systemctl start openafs-client.service You can check that the service is running as it should: $ sudo systemctl status openafs-client.service No more errors! Continue as before, aklog and possibly a crontab for kinit . Note: It may be enough to just run $ sudo dpkg-reconfigure openafs-modules-dkms","title":"Possible problems on Ubuntu"},{"location":"guides/openafs/#within-windows-subsystem-linux-wsl","text":"The same error as above can occur in WSL, and while the solution is also to reconfigure the kernel modules, one needs to download the kernel source first and perform some of the build steps. A guide has been written in this gist from Joschua .","title":"Within Windows Subsystem Linux (WSL)"},{"location":"guides/openafs/#installation-on-macos","text":"(prepared by F. Van Der Veken - 2020-04-01) Mounting AFS on macOS can be a bit messy and is very poorly supported. In general it is preferred to access AFS via LxPlus and move your files to your local computer with scp . If you want to try mounting AFS anyway, there are two ways to proceed: using FUSE, or using OpenAFS. Tested on macOS Sierra (10.12.6) and Catalina (10.15.4).","title":"Installation on MacOS"},{"location":"guides/openafs/#using-openafs","text":"So far, it is not possible to install OpenAFS on macOS High Sierra (10.13). More importantly, when upgrading macOS to version 10.13 it is extremely important to deinstall OpenAFS completely before making the upgrade, to avoid a never-ending loop of kernel-panics which are due to the AFS local cache being converted to the new Apple File System (APFS). Installers can be downloaded from: macOS 10.12: http://download.sinenomine.net/openafs/bins/1.6.20/macos-10.12/ macOS 10.11: http://download.sinenomine.net/openafs/bins/1.6.20/macos-10.11/ These are not officially supported, but third-party binaries provided by Sine Nomine (more information at https://wiki.openafs.org/archive/BinaryThirdParty/ ). Older versions of macOS are officially supported and can be downloaded from http://www.openafs.org/macos.html . Update Auristor now has installers for later versions of macOS: https://www.auristor.com/openafs/client-installer/ This seems to work on Catalina, however, do not upgrade macOS without deinstalling OpenAFS first (which is then named Auristor ) to avoid disk crashes. During the installation, when asked for the local cell this is cern.ch . Give the installation permissions in System Preferences when needed. After installation, a reboot is needed. Luckily macOS comes with Kerberos pre-installed, so that's all. If you want to access protected paths, you'll have to login with Kerberos: $ kinit user@CERN.CH # get kerberos ticket $ aklog # login to AFS cell There is an OpenAFS/Auristor preference pane in your system preferences in which you can change the cell (not needed), auto-renew Kerberos tickets, and let OpenAFS start at boot.","title":"Using OpenAFS"},{"location":"guides/openafs/#using-fuse-and-sshfs","text":"FUSE for mac and the SSHFS plugin can be downloaded from http://osxfuse.github.com . Alternatively, both can be installed using macports : $ sudo port install osxfuse ; sudo port install sshfs Verifying if OSXFUSE is installed can be done in the preferences pane, while checking if SSHFS is installed can be done by typing $ sshfs&nbsp; -h in the terminal. To mount the filesystem we have first to create a folder to hold its location. Normally all disks are mounted in /Volumes/ , however, from macOS 10.12 one needs to have root permissions to write to this location. But this is not really a problem, as a volume can be mounted at any location, so your home folder will do fine. Mounting the volume is then done by: $ mkdir ~/DISK_NAME $ sshfs USER@lxplus.cern.ch:AFSPATH ~/DISK_NAME -ovolname = DISK_NAME This can be simplified by making an alias in .bash_profile . Unmounting the volume is done in the usual way in Finder (clicking the unmount icon to the right of the volume). If for some reason unmounting does not work, the volume can be forcefully ejected by typing $ diskutil unmountDisk force ~/DISK_NAME If you want to automatically mount the drive on startup, have a look at: http://superuser.com/questions/134140/mount-an-sshfs-via-macfuse-at-boot .","title":"Using FUSE and SSHFS"},{"location":"guides/openstack/","text":"Openstack virtual machines Introduction Virtual machines can be created using CERN's cloud services. The service is available at https://openstack.cern.ch/ Detailed documentation from CERN's IT department is available at: https://clouddocs.web.cern.ch/index.html In the following we will provide simple recipes for crating and accessing Windows or Linux Virtual Machines. Creating a Windows Virtual machine accessible from outside CERN (Prepared by G. Sterbini and G. Iadarola) You can create your Windows Virtual Machine using the Openstack web interface by following these instructions . To access from outside CERN, ssk for a direct connection following these instructions . After ~5 min you should receive the confirmation that your machine is setup. You can connect to the machine using Windows Remote Desktop (tested on Version 10), by opening the file provided by the previous step (remode desktop shortcut). Form inside the virtual machine, you can access also linux computers and services (e.g. lxplus), including those accessible only from inside CERN. The most efficient way is to use Start X-Win32 that can be installed using the CMF package installer from the CERN NICE services ( https://winservices.web.cern.ch/winservices/ ). Other Operating Systems CentOS CC7 CentOS CS8","title":"General"},{"location":"guides/openstack/#openstack-virtual-machines","text":"","title":"Openstack virtual machines"},{"location":"guides/openstack/#introduction","text":"Virtual machines can be created using CERN's cloud services. The service is available at https://openstack.cern.ch/ Detailed documentation from CERN's IT department is available at: https://clouddocs.web.cern.ch/index.html In the following we will provide simple recipes for crating and accessing Windows or Linux Virtual Machines.","title":"Introduction"},{"location":"guides/openstack/#creating-a-windows-virtual-machine-accessible-from-outside-cern","text":"(Prepared by G. Sterbini and G. Iadarola) You can create your Windows Virtual Machine using the Openstack web interface by following these instructions . To access from outside CERN, ssk for a direct connection following these instructions . After ~5 min you should receive the confirmation that your machine is setup. You can connect to the machine using Windows Remote Desktop (tested on Version 10), by opening the file provided by the previous step (remode desktop shortcut). Form inside the virtual machine, you can access also linux computers and services (e.g. lxplus), including those accessible only from inside CERN. The most efficient way is to use Start X-Win32 that can be installed using the CMF package installer from the CERN NICE services ( https://winservices.web.cern.ch/winservices/ ).","title":"Creating a Windows Virtual machine accessible from outside CERN"},{"location":"guides/openstack/#other-operating-systems","text":"CentOS CC7 CentOS CS8","title":"Other Operating Systems"},{"location":"guides/openstackCC7/","text":"Creating a CentOS C7 Virtual Machine (Prepared by A. Poyet) Step 1 : Create a new OpenStack instance The first step is to create an instance using the OpenStack service at CERN. From this website, click on 'Instances'. Set the details of your machine Simply choose a name that will be convenient for you. Choose a source We recommend to choose as source an image of the CC7 OS. Choose a flavour Here you can choose the number os CPUs, and memory you need, choosing between three possible sizes (small, medium large). For a fast creation, we recommend to request a small one. Create a key pair In order to access your machine by ssh, you need to create (actually import) a key pair. For that, go to your terminal and issue the following command: ssh-keygen -t rsa -f apoyet-lxplus.key NB: apoyet-lxplus.key corresponds here to the name of the key I'm creating Now import the key pair in your instance by clicking on 'IMPORT KEY PAIR' and copy pasting the public key you just created And launch your instance (~20 min). Step 2: Access your machine Once the instance is running, you should be able to access it via ssh as root: ssh -i apoyet-lxplus.key root@apoyet-lxplus It seems that a user corresponding to your NICE account is already created. ssh apoyet-lxplus And AFS is already mounted. Step 3: Mount CVMFS Access your machine as root. And issue: locmap --enable cvmfs locmap --configure cvmfs Step 4: Mount EOS Access your machine as root. And issue: locmap --enable eosclient locmap --configure eosclient Once connected as user on your machine, you will need to create a token to access EOS. This is done by issuing two commands: kinit aklog You can install git with sudo yum install git You can install jvm with sudo yum install java-1.8.0-openjdk This will be useful for pytimber and other codes. You can also install the standard development tools , for example, sudo yum install centos-release-scl sudo yum install devtoolset-8 to help standard compilations.","title":"CentOS CC7"},{"location":"guides/openstackCC7/#creating-a-centos-c7-virtual-machine","text":"(Prepared by A. Poyet)","title":"Creating a CentOS C7 Virtual Machine"},{"location":"guides/openstackCC7/#step-1-create-a-new-openstack-instance","text":"The first step is to create an instance using the OpenStack service at CERN. From this website, click on 'Instances'.","title":"Step 1 : Create a new OpenStack instance"},{"location":"guides/openstackCC7/#set-the-details-of-your-machine","text":"Simply choose a name that will be convenient for you.","title":"Set the details of your machine"},{"location":"guides/openstackCC7/#choose-a-source","text":"We recommend to choose as source an image of the CC7 OS.","title":"Choose a source"},{"location":"guides/openstackCC7/#choose-a-flavour","text":"Here you can choose the number os CPUs, and memory you need, choosing between three possible sizes (small, medium large). For a fast creation, we recommend to request a small one.","title":"Choose a flavour"},{"location":"guides/openstackCC7/#create-a-key-pair","text":"In order to access your machine by ssh, you need to create (actually import) a key pair. For that, go to your terminal and issue the following command: ssh-keygen -t rsa -f apoyet-lxplus.key NB: apoyet-lxplus.key corresponds here to the name of the key I'm creating Now import the key pair in your instance by clicking on 'IMPORT KEY PAIR' and copy pasting the public key you just created And launch your instance (~20 min).","title":"Create a key pair"},{"location":"guides/openstackCC7/#step-2-access-your-machine","text":"Once the instance is running, you should be able to access it via ssh as root: ssh -i apoyet-lxplus.key root@apoyet-lxplus It seems that a user corresponding to your NICE account is already created. ssh apoyet-lxplus And AFS is already mounted.","title":"Step 2: Access your machine"},{"location":"guides/openstackCC7/#step-3-mount-cvmfs","text":"Access your machine as root. And issue: locmap --enable cvmfs locmap --configure cvmfs","title":"Step 3: Mount CVMFS"},{"location":"guides/openstackCC7/#step-4-mount-eos","text":"Access your machine as root. And issue: locmap --enable eosclient locmap --configure eosclient Once connected as user on your machine, you will need to create a token to access EOS. This is done by issuing two commands: kinit aklog You can install git with sudo yum install git You can install jvm with sudo yum install java-1.8.0-openjdk This will be useful for pytimber and other codes. You can also install the standard development tools , for example, sudo yum install centos-release-scl sudo yum install devtoolset-8 to help standard compilations.","title":"Step 4: Mount EOS"},{"location":"guides/openstackCS8/","text":"Virtual Machine CentOS Stream 8 This guide is very similar to Creating a CentOS CC7 Virtual Machine , yet focuses on the CentOS Stream 8 image , which comes with less pre-configuration, than CC7. After this guide you will have a virtual CentOS Stream 8 machine, with afs , eos and htcondor enabled. Create SSH Key pair Create an ssh-public-private-key pair for authentication. ssh-keygen -t rsa -f your-key-name where your-key-name is the name you want to give your key, e.g. centos8key . The keys will be saved in ~/.ssh/ . Create virtual machine Go to CERN Openstack : Project -> Compute -> Instaces -> Launch Instance On Details choose a cern-unique name for your instance. It will be available under your-instance-name@cern.ch On Source select the CS8_x86_64 image On Flavour select the Volume and RAM size that you think you will need. On Key Pair select Import Key Pair and choose as ssh-key the in Step 1 created Public Key , e.g. centos8key.pub . Now you have to wait until your instance is created, you can see the progress in the Project -> Compute -> Instances view. When the Power State reads Running your machine is ready. Create new User Unlike in CC7 only the root user will be created. Login to your virtual machine as this user via ssh: ssh -i your-key-name root@your-machine-name.cern.ch where your-key-name from the examples above would be centos8key and your-machine-name would be mycentoscs8 . Create user with the same name as your cern name with sudo rights: export USERNAME = your-cern-username adduser $USERNAME passwd $USERNAME usermod -aG wheel $USERNAME The password does not need to match your CERN password. Allow ssh-authorization for this user for the same ssh-keys mkdir /home/ $USERNAME /.ssh cp .ssh/authorized_keys /home/ $USERNAME /.ssh/ chown -R $USERNAME : $USERNAME /home/ $USERNAME /.ssh/ And you're done. You will probably never use this root user again. So log out now. And you should now be able to login as: ssh -i your-key-name your-user-name@your-machine-name.cern.ch Disable SSH-Root-Login For security reasons it might make sense now to deactivate ssh-login via root-user account . Before you do this, make sure you can login as your-user-name and you have root-rights (e.g. sudo su works). You can now disallow login as root by modifying (with sudo ) the line in /etc/ssh/sshd_config PermitRootLogin yes to PermitRootLogin no and restart the ssh daemon systemctl restart sshd SSH Config To make your life easier you can add the following lines to your ssh-config ~/.ssh/config : # connect to virtual machine from inside GPN Host *your-machine-name HostName your-machine-name.cern.ch User your-user-name IdentityFile path/to/your-ssh-key-name # connect through proxy from outside GPN Host ext* ProxyJump lxtunnel.cern.ch with the your-xxxx-name s replaced accordingly. This allows you to ssh into your machine simply with inside GPN ssh your-machine-name outside GPN ssh extyour-machine-name Install afs/eos/etc. This follows the installation hints found on the CERN centos step-by-step installation guide . First install locmap which manages CERN installations and then let it install and reconfigure the CERN-default packages: sudo dnf install locmap-release sudo dnf install locmap for module in afs eosclient chrony cvmfs kerberos lpadmin postfix ssh sudo ; do sudo locmap --enable $module ; done sudo locmap --configure all Now you should have access to afs , eos and kerberos ( kinit and aklog ). Install HTCondor This is adapted from the HTCondor installation guide on this webpage, but with some modifications. First of all we don't need to install the kerberos packages, as this is done by locmap in the step above . Configure KERBEROS We only need to configure kerberos for HTCondor: export USERNAME=your-user-name scp $USERNAME@lxplus8.cern.ch:/usr/bin/batch_krb5_credential . chmod +x batch_krb5_credential sudo mv batch_krb5_credential /usr/bin/ scp $USERNAME@lxplus8.cern.ch:/etc/ngauth_batch_crypt_pub.pem . sudo mv ngauth_batch_crypt_pub.pem /etc/ scp $USERNAME@lxplus8.cern.ch:/etc/krb5.conf.no_rdns . sudo mv krb5.conf.no_rdns /etc/krb5.conf.no_rdns scp $USERNAME@lxplus8.cern.ch:/etc/sysconfig/ngbauth-submit . sudo mv ngbauth-submit /etc/sysconfig/ verify the installation via: /usr/bin/batch_krb5_credential There should be an output like: -----BEGIN NGAUTH COMPOSITE----- # LOTS OF LINES OF YOUR KEY -----END NGAUTH COMPOSITE----- and nothing else (i.e. no missing files or errors). Make sure you have valid credentials already (run kinit ). Also see the debugging help . Install HTCondor This follows the guide in the develop branch on the HTCondor website . The development brach is needed for now as CERN requires HTCondor v8.9.7+ as of June 2021. sudo yum install wget sudo wget https://research.cs.wisc.edu/htcondor/yum/RPM-GPG-KEY-HTCondor sudo rpm --import RPM-GPG-KEY-HTCondor cd /etc/yum.repos.d sudo wget https://research.cs.wisc.edu/htcondor/yum/repo.d/htcondor-development-rhel8.repo sudo yum install condor-all Configure HTCondor The configuration is then as in the default HTCondor guide . Create the config file /etc/condor/config.d/10-local.config . Please set as scheduler ( SCHEDD_HOST ) the default one you get on lxplus , e.g. in your condor_q output. You can also find it out by running (on lxplus ): condor_config_val SCHEDD_HOST An example content is provided here: CONDOR_HOST = tweetybird03.cern.ch, tweetybird04.cern.ch COLLECTOR_HOST = tweetybird03.cern.ch, tweetybird04.cern.ch SCHEDD_HOST = bigbirdXX.cern.ch SCHEDD_NAME = $( SCHEDD_HOST ) SEC_CLIENT_AUTHENTICATION_METHODS = KERBEROS SEC_CREDENTIAL_PRODUCER = /usr/bin/batch_krb5_credential CREDD_HOST = $( SCHEDD_HOST ) FILESYSTEM_DOMAIN = cern.ch UID_DOMAIN = cern.ch Start the service: sudo systemctl start condor sudo systemctl enable condor Check: condor_q See the debugging help . Congratulations! You have now a very lxplus-like machine at your own disposal.","title":"CentOS CS8"},{"location":"guides/openstackCS8/#virtual-machine-centos-stream-8","text":"This guide is very similar to Creating a CentOS CC7 Virtual Machine , yet focuses on the CentOS Stream 8 image , which comes with less pre-configuration, than CC7. After this guide you will have a virtual CentOS Stream 8 machine, with afs , eos and htcondor enabled.","title":"Virtual Machine CentOS Stream 8"},{"location":"guides/openstackCS8/#create-ssh-key-pair","text":"Create an ssh-public-private-key pair for authentication. ssh-keygen -t rsa -f your-key-name where your-key-name is the name you want to give your key, e.g. centos8key . The keys will be saved in ~/.ssh/ .","title":"Create SSH Key pair"},{"location":"guides/openstackCS8/#create-virtual-machine","text":"Go to CERN Openstack : Project -> Compute -> Instaces -> Launch Instance On Details choose a cern-unique name for your instance. It will be available under your-instance-name@cern.ch On Source select the CS8_x86_64 image On Flavour select the Volume and RAM size that you think you will need. On Key Pair select Import Key Pair and choose as ssh-key the in Step 1 created Public Key , e.g. centos8key.pub . Now you have to wait until your instance is created, you can see the progress in the Project -> Compute -> Instances view. When the Power State reads Running your machine is ready.","title":"Create virtual machine"},{"location":"guides/openstackCS8/#create-new-user","text":"Unlike in CC7 only the root user will be created. Login to your virtual machine as this user via ssh: ssh -i your-key-name root@your-machine-name.cern.ch where your-key-name from the examples above would be centos8key and your-machine-name would be mycentoscs8 . Create user with the same name as your cern name with sudo rights: export USERNAME = your-cern-username adduser $USERNAME passwd $USERNAME usermod -aG wheel $USERNAME The password does not need to match your CERN password. Allow ssh-authorization for this user for the same ssh-keys mkdir /home/ $USERNAME /.ssh cp .ssh/authorized_keys /home/ $USERNAME /.ssh/ chown -R $USERNAME : $USERNAME /home/ $USERNAME /.ssh/ And you're done. You will probably never use this root user again. So log out now. And you should now be able to login as: ssh -i your-key-name your-user-name@your-machine-name.cern.ch Disable SSH-Root-Login For security reasons it might make sense now to deactivate ssh-login via root-user account . Before you do this, make sure you can login as your-user-name and you have root-rights (e.g. sudo su works). You can now disallow login as root by modifying (with sudo ) the line in /etc/ssh/sshd_config PermitRootLogin yes to PermitRootLogin no and restart the ssh daemon systemctl restart sshd SSH Config To make your life easier you can add the following lines to your ssh-config ~/.ssh/config : # connect to virtual machine from inside GPN Host *your-machine-name HostName your-machine-name.cern.ch User your-user-name IdentityFile path/to/your-ssh-key-name # connect through proxy from outside GPN Host ext* ProxyJump lxtunnel.cern.ch with the your-xxxx-name s replaced accordingly. This allows you to ssh into your machine simply with inside GPN ssh your-machine-name outside GPN ssh extyour-machine-name","title":"Create new User"},{"location":"guides/openstackCS8/#install-afseosetc","text":"This follows the installation hints found on the CERN centos step-by-step installation guide . First install locmap which manages CERN installations and then let it install and reconfigure the CERN-default packages: sudo dnf install locmap-release sudo dnf install locmap for module in afs eosclient chrony cvmfs kerberos lpadmin postfix ssh sudo ; do sudo locmap --enable $module ; done sudo locmap --configure all Now you should have access to afs , eos and kerberos ( kinit and aklog ).","title":"Install afs/eos/etc."},{"location":"guides/openstackCS8/#install-htcondor","text":"This is adapted from the HTCondor installation guide on this webpage, but with some modifications. First of all we don't need to install the kerberos packages, as this is done by locmap in the step above .","title":"Install HTCondor"},{"location":"guides/openstackCS8/#configure-kerberos","text":"We only need to configure kerberos for HTCondor: export USERNAME=your-user-name scp $USERNAME@lxplus8.cern.ch:/usr/bin/batch_krb5_credential . chmod +x batch_krb5_credential sudo mv batch_krb5_credential /usr/bin/ scp $USERNAME@lxplus8.cern.ch:/etc/ngauth_batch_crypt_pub.pem . sudo mv ngauth_batch_crypt_pub.pem /etc/ scp $USERNAME@lxplus8.cern.ch:/etc/krb5.conf.no_rdns . sudo mv krb5.conf.no_rdns /etc/krb5.conf.no_rdns scp $USERNAME@lxplus8.cern.ch:/etc/sysconfig/ngbauth-submit . sudo mv ngbauth-submit /etc/sysconfig/ verify the installation via: /usr/bin/batch_krb5_credential There should be an output like: -----BEGIN NGAUTH COMPOSITE----- # LOTS OF LINES OF YOUR KEY -----END NGAUTH COMPOSITE----- and nothing else (i.e. no missing files or errors). Make sure you have valid credentials already (run kinit ). Also see the debugging help .","title":"Configure KERBEROS"},{"location":"guides/openstackCS8/#install-htcondor_1","text":"This follows the guide in the develop branch on the HTCondor website . The development brach is needed for now as CERN requires HTCondor v8.9.7+ as of June 2021. sudo yum install wget sudo wget https://research.cs.wisc.edu/htcondor/yum/RPM-GPG-KEY-HTCondor sudo rpm --import RPM-GPG-KEY-HTCondor cd /etc/yum.repos.d sudo wget https://research.cs.wisc.edu/htcondor/yum/repo.d/htcondor-development-rhel8.repo sudo yum install condor-all","title":"Install HTCondor"},{"location":"guides/openstackCS8/#configure-htcondor","text":"The configuration is then as in the default HTCondor guide . Create the config file /etc/condor/config.d/10-local.config . Please set as scheduler ( SCHEDD_HOST ) the default one you get on lxplus , e.g. in your condor_q output. You can also find it out by running (on lxplus ): condor_config_val SCHEDD_HOST An example content is provided here: CONDOR_HOST = tweetybird03.cern.ch, tweetybird04.cern.ch COLLECTOR_HOST = tweetybird03.cern.ch, tweetybird04.cern.ch SCHEDD_HOST = bigbirdXX.cern.ch SCHEDD_NAME = $( SCHEDD_HOST ) SEC_CLIENT_AUTHENTICATION_METHODS = KERBEROS SEC_CREDENTIAL_PRODUCER = /usr/bin/batch_krb5_credential CREDD_HOST = $( SCHEDD_HOST ) FILESYSTEM_DOMAIN = cern.ch UID_DOMAIN = cern.ch Start the service: sudo systemctl start condor sudo systemctl enable condor Check: condor_q See the debugging help . Congratulations! You have now a very lxplus-like machine at your own disposal.","title":"Configure HTCondor"},{"location":"guides/pyjapc/","text":"PyJapc and Accelerator data PyJapc is the official tool maintained by BE-CSS to access the CERN accelerator control system in Python using (JAPC)[ https://wikis.cern.ch/display/JAPC ). PyJapcScout is a wrapper over PyJapc developed by ABP. The main difference with respect to the plane PyJapc is the data conversion from JAPC to Python. This was necessary to allow for storing the acquired data to Parquet files using datascout package. datascout is a small package of helper functions to easy saving and loading data (dict, pandas , Awkward-array ) mainly into parquet files.","title":"PyJapc and Accelerator data"},{"location":"guides/pyjapc/#pyjapc-and-accelerator-data","text":"PyJapc is the official tool maintained by BE-CSS to access the CERN accelerator control system in Python using (JAPC)[ https://wikis.cern.ch/display/JAPC ). PyJapcScout is a wrapper over PyJapc developed by ABP. The main difference with respect to the plane PyJapc is the data conversion from JAPC to Python. This was necessary to allow for storing the acquired data to Parquet files using datascout package. datascout is a small package of helper functions to easy saving and loading data (dict, pandas , Awkward-array ) mainly into parquet files.","title":"PyJapc and Accelerator data"},{"location":"guides/python_inst/","text":"Install and configure python (Prepared by G. Iadarola) Introduction Some operative systems come equipped with a Python installation. Even in this case, it is often convenient to install a user-managed python environment to have full control of the configuration. In this page we describe two methods: Setup of a miniconda installation (easier) Compile and configure python from source code (necessary when thw installation needs to be compiled with a specific compiler, for example to be used with MPI). None of the two methods requires admin rights. Install and configure miniconda Step 1: Download and install the most recent version of Miniconda (from here ) Something like: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh (Provide the path for the installation, in my case /home/giadarol/Desktop/PyFRIENDS_python3/miniconda3 You will be asked whether you want to make this miniconda the default python installation for your computer (this is your choice :-) ) Step 2: Activate the miniconda installation source /home/giadarol/Desktop/PyFRIENDS_python3/miniconda3/bin/activate Step 2a (optional): Install compilers To install packages that need to compile code (e.g. PyECLOUD, PyHEADTAIL or Xsuite) you need C and/or FORTRAN compilers. If you don't have compilers available on your system you can install them using conda as discussed in this guide . For example on Linux you can install C, C++ and FORTRAN compilers by: conda install gcc_linux-64 conda install gxx_linux-64 conda install gfortran_linux-64 while on MAC you can use: conda install clang_osx-64 conda install clangxx_osx-64 conda install gfortran_osx-64 Step 3: Install useful packages pip install numpy scipy matplotlib pandas ipython Your python installation is ready to be used! Virtual environments Virtual environments can be useful to try different package versions and for developments. A detailed guide to conda's virtual environments can be found here: https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands Compile and configure python from source code Step 1: We move to the folder where we want to place our installation: /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3 Step 2: We download from the python website , compile and install the latest version of python 3: mkdir python_src cd python_src wget https://www.python.org/ftp/python/3.8.0/Python-3.8.0.tar.xz tar xvf Python-3.8.0.tar.xz cd Python-3.8.0 ./configure --prefix=/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/python3 make make install Possible issues with ssl: In order to be able to use pip at a later stage, python needs to be compiled with ssl support. Please check that this is the case: /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/python3/bin/python3 import ssl In case this gives an exception, please install ssl library and recompile python (in Ubuntu this can be done by sudo apt install libssl-dev). Information on how to get the ssl library without admin rights can be font here . Possible issue with ctypes and libffi: Check that ctypes is working (will be required for several reasons afterwards): /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/python3/bin/python3 import ctypes If you get an error, it is because the ffi library is not installed. You can download and install the library as follows: cd /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/python_src wget ftp://sourceware.org/pub/libffi/libffi-3.3.tar.gz tar -xvf libffi-3.3.tar.gz cd libffi-3.3/ mkdir /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/libffi ./configure --prefix=/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/libffi make make install The library location needs to be added to our library path: export LD_LIBRARY_PATH=/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/libffi/lib64:$LD_LIBRARY_PATH The python compilation needs some extra information: export PKG_CONFIG_PATH=/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/libffi/lib/pkgconfig ./configure --prefix=/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/python3 LDFLAGS=\"-L/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/libffi/lib/../lib64\" CFLAGS=\"-I/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/libffi/include\" The continue as normal: make make install Possible issues with sqlite3: In order to be able to use jupyter notebooks at a later stage, python needs to be compiled with sqlite3 support. Please check that this is the case: /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/python3/bin/python3 import sqlite3 In case this gives an exception, please install sqlite library and recompile python (in Ubuntu this can be done by sudo apt install libsqlite3-dev). Possible issues with tk: In order to be able to use ipython --pylab at a later stage, python needs to be compiled with tk support. Please check that this is the case: /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_p3/python3/bin/python3 import _tkinter In case this gives an exception, please install tk library and recompile python (in Ubuntu this can be done by sudo apt install tklib; sudo apt install tk-dev ). Step 3: We create a virtual environment cd /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3 mkdir venvs cd venvs /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/python3/bin/python3 -m venv py3 Step 4: We activate the virtual environment source /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/venvs/py3/bin/activate If we type: which python we get: /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/venvs/py3/bin/python Step 4b : In case libffi needed to be compiled, you should add libffi path also in activate script To do this, edit the activate script: nano /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/venvs/py3/bin/activate Add after the last line: export LD_LIBRARY_PATH=/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/libffi/lib64:$LD_LIBRARY_PATH Step 6 : We use pip to install the python modules that we need pip install numpy scipy cython ipython matplotlib Steps 5 and 6 are required only to run parallel simulations using MPI. Step 5 - Check that MPI is available: You can use the following commands to check that an MPI installation is available which mpicc which mpiexec In case it is not, please activate an MPI installation (Step 5.a) or install a new MPI installation (Step 5.b). Step 5.a - Activate MPI: On certain machines an MPI installation can be activated using module load (it is the case for CERN lxplus and CERN HPC cluster) or mpi_selector (it is the case for the INFN-CNAF cluster) Step 5.b - Install MPI: If we do not have an MPI installation we need to get one: cd /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/python_src wget https://www.open-mpi.org/software/ompi/v1.10/downloads/openmpi-1.10.2.tar.bz2 tar jxf openmpi-1.10.2.tar.bz2 cd openmpi-1.10.2 ./configure --prefix=/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/openmpi make all install We set the environment variable for the MPI compiler (pip will use the compiler pointed by your MPICC variable to compile mpi4py): export MPICC=/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/openmpi/bin/mpicc At this point be careful not to have other MPI paths in your environment (for example set in your .bashrc) Step 6: Install mpi4py pip install mpi4py Important: Python jobs using mpi4py must be run with the mpiexec corresponding to the MPI compiler that has been used. In our case: /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/openmpi/bin/mpiexec Of course we can add the folder to our PATH or create a shortcut. Step 7: Install h5py (if you need it) If you don't need parallel hdf5 I/O you can just: pip install h5py Instead, in case you need parallel hdf5 I/O and you want to compile your own hdf5 library (N.B. this is NOT needed for PyPARIS simulations): wget https://support.hdfgroup.org/ftp/HDF5/current/src/hdf5-1.10.5.tar.bz2 tar jxf hdf5-1.10.5.tar.bz2 cd hdf5-1.10.5/ ./configure --prefix=/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/hdf5lib --enable-parallel --enable-shared make make install CC=/usr/bin/mpicc HDF5_MPI=\"ON\" HDF5_DIR=/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/hdf5lib pip install --no-binary=h5py h5py In case you don't have libhdf5 installed, but you don't need parallel IO (PyPARIS case), you can: wget https://support.hdfgroup.org/ftp/HDF5/current/src/hdf5-1.10.5.tar.bz2 tar jxf hdf5-1.10.5.tar.bz2 cd hdf5-1.10.5/ ./configure --prefix=/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/hdf5lib --enable-shared make make install HDF5_DIR=/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/hdf5lib pip install --no-binary=h5py h5py A couple of references that helped with this task: [1] [[ https://www.open-mpi.org/faq/?category=building#easy-build ]] [2] [[ http://stackoverflow.com/questions/5506110/is-it-possible-to-install-another-version-of-python-to-virtualenv ]] [3] [[ https://chrisbebek.com/blog/?p=97 ]]","title":"Install and configure python"},{"location":"guides/python_inst/#install-and-configure-python","text":"(Prepared by G. Iadarola)","title":"Install and configure python"},{"location":"guides/python_inst/#introduction","text":"Some operative systems come equipped with a Python installation. Even in this case, it is often convenient to install a user-managed python environment to have full control of the configuration. In this page we describe two methods: Setup of a miniconda installation (easier) Compile and configure python from source code (necessary when thw installation needs to be compiled with a specific compiler, for example to be used with MPI). None of the two methods requires admin rights.","title":"Introduction"},{"location":"guides/python_inst/#install-and-configure-miniconda","text":"Step 1: Download and install the most recent version of Miniconda (from here ) Something like: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh (Provide the path for the installation, in my case /home/giadarol/Desktop/PyFRIENDS_python3/miniconda3 You will be asked whether you want to make this miniconda the default python installation for your computer (this is your choice :-) ) Step 2: Activate the miniconda installation source /home/giadarol/Desktop/PyFRIENDS_python3/miniconda3/bin/activate Step 2a (optional): Install compilers To install packages that need to compile code (e.g. PyECLOUD, PyHEADTAIL or Xsuite) you need C and/or FORTRAN compilers. If you don't have compilers available on your system you can install them using conda as discussed in this guide . For example on Linux you can install C, C++ and FORTRAN compilers by: conda install gcc_linux-64 conda install gxx_linux-64 conda install gfortran_linux-64 while on MAC you can use: conda install clang_osx-64 conda install clangxx_osx-64 conda install gfortran_osx-64 Step 3: Install useful packages pip install numpy scipy matplotlib pandas ipython Your python installation is ready to be used!","title":"Install and configure miniconda"},{"location":"guides/python_inst/#virtual-environments","text":"Virtual environments can be useful to try different package versions and for developments. A detailed guide to conda's virtual environments can be found here: https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands","title":"Virtual environments"},{"location":"guides/python_inst/#compile-and-configure-python-from-source-code","text":"Step 1: We move to the folder where we want to place our installation: /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3 Step 2: We download from the python website , compile and install the latest version of python 3: mkdir python_src cd python_src wget https://www.python.org/ftp/python/3.8.0/Python-3.8.0.tar.xz tar xvf Python-3.8.0.tar.xz cd Python-3.8.0 ./configure --prefix=/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/python3 make make install Possible issues with ssl: In order to be able to use pip at a later stage, python needs to be compiled with ssl support. Please check that this is the case: /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/python3/bin/python3 import ssl In case this gives an exception, please install ssl library and recompile python (in Ubuntu this can be done by sudo apt install libssl-dev). Information on how to get the ssl library without admin rights can be font here . Possible issue with ctypes and libffi: Check that ctypes is working (will be required for several reasons afterwards): /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/python3/bin/python3 import ctypes If you get an error, it is because the ffi library is not installed. You can download and install the library as follows: cd /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/python_src wget ftp://sourceware.org/pub/libffi/libffi-3.3.tar.gz tar -xvf libffi-3.3.tar.gz cd libffi-3.3/ mkdir /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/libffi ./configure --prefix=/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/libffi make make install The library location needs to be added to our library path: export LD_LIBRARY_PATH=/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/libffi/lib64:$LD_LIBRARY_PATH The python compilation needs some extra information: export PKG_CONFIG_PATH=/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/libffi/lib/pkgconfig ./configure --prefix=/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/python3 LDFLAGS=\"-L/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/libffi/lib/../lib64\" CFLAGS=\"-I/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/libffi/include\" The continue as normal: make make install Possible issues with sqlite3: In order to be able to use jupyter notebooks at a later stage, python needs to be compiled with sqlite3 support. Please check that this is the case: /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/python3/bin/python3 import sqlite3 In case this gives an exception, please install sqlite library and recompile python (in Ubuntu this can be done by sudo apt install libsqlite3-dev). Possible issues with tk: In order to be able to use ipython --pylab at a later stage, python needs to be compiled with tk support. Please check that this is the case: /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_p3/python3/bin/python3 import _tkinter In case this gives an exception, please install tk library and recompile python (in Ubuntu this can be done by sudo apt install tklib; sudo apt install tk-dev ). Step 3: We create a virtual environment cd /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3 mkdir venvs cd venvs /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/python3/bin/python3 -m venv py3 Step 4: We activate the virtual environment source /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/venvs/py3/bin/activate If we type: which python we get: /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/venvs/py3/bin/python Step 4b : In case libffi needed to be compiled, you should add libffi path also in activate script To do this, edit the activate script: nano /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/venvs/py3/bin/activate Add after the last line: export LD_LIBRARY_PATH=/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/libffi/lib64:$LD_LIBRARY_PATH Step 6 : We use pip to install the python modules that we need pip install numpy scipy cython ipython matplotlib Steps 5 and 6 are required only to run parallel simulations using MPI. Step 5 - Check that MPI is available: You can use the following commands to check that an MPI installation is available which mpicc which mpiexec In case it is not, please activate an MPI installation (Step 5.a) or install a new MPI installation (Step 5.b). Step 5.a - Activate MPI: On certain machines an MPI installation can be activated using module load (it is the case for CERN lxplus and CERN HPC cluster) or mpi_selector (it is the case for the INFN-CNAF cluster) Step 5.b - Install MPI: If we do not have an MPI installation we need to get one: cd /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/python_src wget https://www.open-mpi.org/software/ompi/v1.10/downloads/openmpi-1.10.2.tar.bz2 tar jxf openmpi-1.10.2.tar.bz2 cd openmpi-1.10.2 ./configure --prefix=/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/openmpi make all install We set the environment variable for the MPI compiler (pip will use the compiler pointed by your MPICC variable to compile mpi4py): export MPICC=/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/openmpi/bin/mpicc At this point be careful not to have other MPI paths in your environment (for example set in your .bashrc) Step 6: Install mpi4py pip install mpi4py Important: Python jobs using mpi4py must be run with the mpiexec corresponding to the MPI compiler that has been used. In our case: /afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/openmpi/bin/mpiexec Of course we can add the folder to our PATH or create a shortcut. Step 7: Install h5py (if you need it) If you don't need parallel hdf5 I/O you can just: pip install h5py Instead, in case you need parallel hdf5 I/O and you want to compile your own hdf5 library (N.B. this is NOT needed for PyPARIS simulations): wget https://support.hdfgroup.org/ftp/HDF5/current/src/hdf5-1.10.5.tar.bz2 tar jxf hdf5-1.10.5.tar.bz2 cd hdf5-1.10.5/ ./configure --prefix=/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/hdf5lib --enable-parallel --enable-shared make make install CC=/usr/bin/mpicc HDF5_MPI=\"ON\" HDF5_DIR=/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/hdf5lib pip install --no-binary=h5py h5py In case you don't have libhdf5 installed, but you don't need parallel IO (PyPARIS case), you can: wget https://support.hdfgroup.org/ftp/HDF5/current/src/hdf5-1.10.5.tar.bz2 tar jxf hdf5-1.10.5.tar.bz2 cd hdf5-1.10.5/ ./configure --prefix=/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/hdf5lib --enable-shared make make install HDF5_DIR=/afs/cern.ch/work/g/giadarol/sim_workspace_mpi_py3/hdf5lib pip install --no-binary=h5py h5py A couple of references that helped with this task: [1] [[ https://www.open-mpi.org/faq/?category=building#easy-build ]] [2] [[ http://stackoverflow.com/questions/5506110/is-it-possible-to-install-another-version-of-python-to-virtualenv ]] [3] [[ https://chrisbebek.com/blog/?p=97 ]]","title":"Compile and configure python from source code"},{"location":"guides/python_packages/","text":"Guidelines for python tools The worldwide python community has defined clear guidelines for the packaging of python products. Very detailed documentation on this aspect can be found at https://packaging.python.org . Following such standards provides numerous advantages, in particular in simplifying the installation process. Template for ABP projects For python packages maintained by ABP, please follow the template available at: https://github.com/PyCOMPLETE/pypkgexample which includes instructions on how to include compiled extensions in C/C++ or Fortran. A readme file with detailed exaplanations is available in the repository. See in particular the sections on the package structure and installation .","title":"Guidelines for python tools"},{"location":"guides/python_packages/#guidelines-for-python-tools","text":"The worldwide python community has defined clear guidelines for the packaging of python products. Very detailed documentation on this aspect can be found at https://packaging.python.org . Following such standards provides numerous advantages, in particular in simplifying the installation process.","title":"Guidelines for python tools"},{"location":"guides/python_packages/#template-for-abp-projects","text":"For python packages maintained by ABP, please follow the template available at: https://github.com/PyCOMPLETE/pypkgexample which includes instructions on how to include compiled extensions in C/C++ or Fortran. A readme file with detailed exaplanations is available in the repository. See in particular the sections on the package structure and installation .","title":"Template for ABP projects"},{"location":"guides/pytimber_nxcals/","text":"PyTIMBER and NXCALS As of 1 Feb 2021 the old accelerator logging system (CALS) has been made unaccessible and has been replaced by the new NXCALS system. A quick-start guide to NXCALS is available at this address: http://nxcals-docs.web.cern.ch/current/user-guide/data-access/quickstart/ To get access to the data you need to request access using this CCDE form or writing an e-mail to the logging support (acc-logging-support[AT]cern.ch) The pytimber package has been adapted to work with the new system. Please note that the latest version of PyTIMBER is not anymore made available on PyPI but needs to be retrieved from the internal CERN package index. A working setup could be obtained as follows (starting from a fresh miniconda installation): # Get, install and activate miniconda cd /afs/cern.ch/work/l/lhcecld wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh -b -p /afs/cern.ch/work/l/lhcecld/miniconda source miniconda/bin/activate # Get standard packages # (to have all spark functionalities pandas needs to be installed before pytimber) pip install numpy scipy matplotlib ipython pandas # Change python package index to CERN index pip install git+https://gitlab.cern.ch/acc-co/devops/python/acc-py-pip-config.git # Install pytimber pip install pytimber # Change python package index back to default pip uninstall acc-py-pip-config A pytimber object accessing NXCALS can be instantiated by: ldb = pytimber . LoggingDB ( source = \"nxcals\" )","title":"PyTIMBER and NXCALS"},{"location":"guides/pytimber_nxcals/#pytimber-and-nxcals","text":"As of 1 Feb 2021 the old accelerator logging system (CALS) has been made unaccessible and has been replaced by the new NXCALS system. A quick-start guide to NXCALS is available at this address: http://nxcals-docs.web.cern.ch/current/user-guide/data-access/quickstart/ To get access to the data you need to request access using this CCDE form or writing an e-mail to the logging support (acc-logging-support[AT]cern.ch) The pytimber package has been adapted to work with the new system. Please note that the latest version of PyTIMBER is not anymore made available on PyPI but needs to be retrieved from the internal CERN package index. A working setup could be obtained as follows (starting from a fresh miniconda installation): # Get, install and activate miniconda cd /afs/cern.ch/work/l/lhcecld wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh -b -p /afs/cern.ch/work/l/lhcecld/miniconda source miniconda/bin/activate # Get standard packages # (to have all spark functionalities pandas needs to be installed before pytimber) pip install numpy scipy matplotlib ipython pandas # Change python package index to CERN index pip install git+https://gitlab.cern.ch/acc-co/devops/python/acc-py-pip-config.git # Install pytimber pip install pytimber # Change python package index back to default pip uninstall acc-py-pip-config A pytimber object accessing NXCALS can be instantiated by: ldb = pytimber . LoggingDB ( source = \"nxcals\" )","title":"PyTIMBER and NXCALS"},{"location":"guides/readthedocs/","text":"Sphinx - Read the Docs documentation G. Iadarola and R. De Maria For python packages, we found particularly convenient to use sphinx to document the code and to host the documentation on Read the Docs . The API documentation can be generated by sphinx from the docstrings in the code. The main steps to create a sphinx-rtd documentation are the following: Install sphinx and rtd-theme Install sphinx and the rdt-theme: $ pip install sphinx $ pip install sphinx-rtd-theme Start documentation pages The code to be documented needs to be hosted in a GitHub or GitLab account Add requirements.txt to your repository. In order to get documentation from the docstrings, Read the Docs will need to import your python package in a python virtual environment. It will install the dependencies based on the requirements file, which simply consists in the list of packages that need to be installed. For example: numpy scipy Create a docs folder inside your repository and populate it, for example with these example files . Edit conf.py to add to the python path the folder containing the python package to be documented (sphinx needs to import the package to access the docstrings).: import sys sys . path . insert ( 0 , os . path . abspath ( '../' )) # path relative to conf.py Many other customizations are possible using conf.py . Compile your project locally by typing: $ make html This generates a simple preview of your documentation (less advanced than rtd) in the folder _build . You can open _build/index.html in your browser to navigate the preview. On readthedocs.org Create an account on https://readthedocs.org (the easiest is to link it to your github account), it you don't already have one. Add a project linked to your GitHub/GitLab repository. Add requirements file by specifying requirements.txt under Admin > Advanced Settings -> Requirements file. Edit your documentation A lot of information about the RST format used in the documentation and about the sphinx doc generation can be found in these tutorials: https://sphinx-rtd-tutorial.readthedocs.io/en/latest/index.html https://sphinx-rtd-tutorial.readthedocs.io An example of documentation from our team can be found at: https://xsuite.readthedocs.io/en/latest/","title":"Sphinx - RTD documentation"},{"location":"guides/readthedocs/#sphinx-read-the-docs-documentation","text":"G. Iadarola and R. De Maria For python packages, we found particularly convenient to use sphinx to document the code and to host the documentation on Read the Docs . The API documentation can be generated by sphinx from the docstrings in the code. The main steps to create a sphinx-rtd documentation are the following:","title":"Sphinx - Read the Docs documentation"},{"location":"guides/readthedocs/#install-sphinx-and-rtd-theme","text":"Install sphinx and the rdt-theme: $ pip install sphinx $ pip install sphinx-rtd-theme","title":"Install sphinx and rtd-theme"},{"location":"guides/readthedocs/#start-documentation-pages","text":"The code to be documented needs to be hosted in a GitHub or GitLab account Add requirements.txt to your repository. In order to get documentation from the docstrings, Read the Docs will need to import your python package in a python virtual environment. It will install the dependencies based on the requirements file, which simply consists in the list of packages that need to be installed. For example: numpy scipy Create a docs folder inside your repository and populate it, for example with these example files . Edit conf.py to add to the python path the folder containing the python package to be documented (sphinx needs to import the package to access the docstrings).: import sys sys . path . insert ( 0 , os . path . abspath ( '../' )) # path relative to conf.py Many other customizations are possible using conf.py . Compile your project locally by typing: $ make html This generates a simple preview of your documentation (less advanced than rtd) in the folder _build . You can open _build/index.html in your browser to navigate the preview.","title":"Start documentation pages"},{"location":"guides/readthedocs/#on-readthedocsorg","text":"Create an account on https://readthedocs.org (the easiest is to link it to your github account), it you don't already have one. Add a project linked to your GitHub/GitLab repository. Add requirements file by specifying requirements.txt under Admin > Advanced Settings -> Requirements file.","title":"On readthedocs.org"},{"location":"guides/readthedocs/#edit-your-documentation","text":"A lot of information about the RST format used in the documentation and about the sphinx doc generation can be found in these tutorials: https://sphinx-rtd-tutorial.readthedocs.io/en/latest/index.html https://sphinx-rtd-tutorial.readthedocs.io An example of documentation from our team can be found at: https://xsuite.readthedocs.io/en/latest/","title":"Edit your documentation"},{"location":"guides/softguide/","text":"Guidelines for code distribution and maintenance in ABP Minimal requirements Software projects within ABP should comply with the following basic guidelines: The source code should be hosted in an online git repository . Common choices are: The CERN GitLab platform ( https://gitlab.cern.ch ). Project hosted there can be public, restricted to people with a CERN account, or to smaller groups. Contributions (pull requests) can be submitted only by people having a CERN account. The GitHub platform ( http://github.com ), which is used by millions of developers in thr world and hosts several major open source projects (e.g. linux kernel, python). The source code for projects hosted on GitHub needs to be public, as CERN does not provide GitHub pro accounts. Contributions (pull requests) can be submitted by anybody. Production code should not be hosted in personal GitLab or GitHub profiles. GitHub organizations or GitLab groups should be used instead. More information about the usage of git for collaborative code development can be found here . One or more \"ready-to-run\" examples should be made available in the repository, illustrating the main features of the code. Please make sure that the example are still updated and working when making modifications to the code! A simple \"Getting started guide\" should be made available, illustrating how to install the code and run an example. This can be provided as: A simple README file within the repository in text or markdown (both GitLab and GitHub render mardkown files). For larger documentation an MkDocs site like the present one can be used (instructions can be found here ). Alternatively the GiHub wiki space associated with the repository can be used. A general paper describing a set of good computing practices that every researcher can adopt, regardless of their current level of computational skill is \"Good enough practices in scientific computing\" . Python packages Python tools should follow a standardized package structure. More details and a template package can be found in the dedicated section . Versioning It is recommended to use version numbers associated to changes in the code. A convenient choice is to version numbers in the form X.Y.Z (e.g. v2.2.1) where X is the major version number (used only for major changes, typically backward incompatible), Y is the minor number, Z is used for bug-fixes and patches. Year based version numbers e.g. 2021.1.3 are a valid alternative. Versions should be associated to releases in GitHub or GitLab, making it easy to retrieve previous versions. Testing The examples discussed above can provide a first-level set of checks to validate new versions. Automatic testing (e.g. unit testing ) and continuos integration are strongly encouraged.","title":"Guidelines for ABP software"},{"location":"guides/softguide/#guidelines-for-code-distribution-and-maintenance-in-abp","text":"","title":"Guidelines for code distribution and maintenance in ABP"},{"location":"guides/softguide/#minimal-requirements","text":"Software projects within ABP should comply with the following basic guidelines: The source code should be hosted in an online git repository . Common choices are: The CERN GitLab platform ( https://gitlab.cern.ch ). Project hosted there can be public, restricted to people with a CERN account, or to smaller groups. Contributions (pull requests) can be submitted only by people having a CERN account. The GitHub platform ( http://github.com ), which is used by millions of developers in thr world and hosts several major open source projects (e.g. linux kernel, python). The source code for projects hosted on GitHub needs to be public, as CERN does not provide GitHub pro accounts. Contributions (pull requests) can be submitted by anybody. Production code should not be hosted in personal GitLab or GitHub profiles. GitHub organizations or GitLab groups should be used instead. More information about the usage of git for collaborative code development can be found here . One or more \"ready-to-run\" examples should be made available in the repository, illustrating the main features of the code. Please make sure that the example are still updated and working when making modifications to the code! A simple \"Getting started guide\" should be made available, illustrating how to install the code and run an example. This can be provided as: A simple README file within the repository in text or markdown (both GitLab and GitHub render mardkown files). For larger documentation an MkDocs site like the present one can be used (instructions can be found here ). Alternatively the GiHub wiki space associated with the repository can be used. A general paper describing a set of good computing practices that every researcher can adopt, regardless of their current level of computational skill is \"Good enough practices in scientific computing\" .","title":"Minimal requirements"},{"location":"guides/softguide/#python-packages","text":"Python tools should follow a standardized package structure. More details and a template package can be found in the dedicated section .","title":"Python packages"},{"location":"guides/softguide/#versioning","text":"It is recommended to use version numbers associated to changes in the code. A convenient choice is to version numbers in the form X.Y.Z (e.g. v2.2.1) where X is the major version number (used only for major changes, typically backward incompatible), Y is the minor number, Z is used for bug-fixes and patches. Year based version numbers e.g. 2021.1.3 are a valid alternative. Versions should be associated to releases in GitHub or GitLab, making it easy to retrieve previous versions.","title":"Versioning"},{"location":"guides/softguide/#testing","text":"The examples discussed above can provide a first-level set of checks to validate new versions. Automatic testing (e.g. unit testing ) and continuos integration are strongly encouraged.","title":"Testing"},{"location":"guides/spack/","text":"","title":"Spack"},{"location":"guides/sshtunnel/","text":"SSH tunnel to browse CERN internal websites In the following you find a few options to access web pages as from the CERN General Public Network while being outside CERN. According to CERN recommendations lxtunnel.cern.ch is to be preferred over lxplus.cern.ch for tunneling, as this is its only purpose while lxplus provides a fully usable environment. Using sshuttle sshuttle allows forwarding of specific connections through the CERN network. It requires some configuration to forward the correct connections: from Davide #!/bin/sh # From https://codimd.web.cern.ch/vjC8BHbTS7etHwJve-K2Uw case $1 in connect ) sshuttle --dns -v --remote dgamba@lxplus.cern.ch 128 .141.0.0/16 128 .142.0.0/16 137 .138.0.0/16 172 .18.0.0/16 185 .249.56.0/22 188 .0.0.0/8 192 .65.196.0/23 192 .91.242.0/24 194 .12.128.0/18 2001 :1458::/32 2001 :1459::/32 --daemon --pidfile /tmp/sshuttle.pid shift ;; disconnect ) kill ` cat /tmp/sshuttle.pid ` shift ;; * ) # unknown option echo \"Unknown option\\nUsage:\" echo \"\\t $0 connect : to start VPN-like connection to CERN\" echo \"\\t $0 disconnect : to stop it\" ;; esac from Riccardo #!/bin/bash kinit IP = ` host lxtunnel.cern.ch | awk 'NR==2 {print $4}' ` echo $IP sshuttle --dns -x $IP --remote = $IP \\ --pidfile /tmp/sshuttle.pid --python = python \\ --ssh-cmd 'ssh -o GSSAPIAuthentication=yes -o GSSAPIDelegateCredentials=yes' \\ 10 .0.0.0/8 \\ 100 .64.0.0/10 \\ 10 .100.0.0/16 \\ 10 .254.0.0/16 \\ 10 .76.0.0/15 \\ 128 .141.0.0/16 \\ 128 .142.0.0/16 \\ 137 .138.0.0/16 \\ 172 .16.0.0/12 \\ 185 .249.56.0/22 \\ 188 .184.0.0/15 \\ 188 .184.0.0/16 \\ 188 .185.0.0/15 \\ 188 .185.0.0/16 \\ 192 .16.155.0/24 \\ 192 .16.156.0/22 \\ 192 .16.160.0/22 \\ 192 .16.164.0/23 \\ 192 .16.166.0/24 \\ 192 .65.183.0/24 \\ 192 .65.184.0/21 \\ 192 .65.192.0/22 \\ 192 .65.196.0/23 \\ 192 .91.236.0/22 \\ 192 .91.240.0/22 \\ 192 .91.242.0/24 \\ 192 .91.244.0/23 \\ 192 .91.246.0/24 \\ 194 .12.128.0/18 SSH tunnel through lxplus/lxtunnel (from cern.ch/bblumi) you can use the following can create an ssh tunnel through the lxplus service . This can be useful to access wikis.cern.ch, timber.cern.ch, issues.cern.ch, ect. You can used SSH to create the tunnel (in a terminal): lxtunnel ssh -D 8888 lxtunnel.cern.ch lxplus ssh -D 8888 lxplus.cern.ch Then set as SOCKS proxy in your network configuration localhost:8888 . Other OS On MacOS 10.15 this is done by (System Preferences) -> Network -> Advanced -> Proxies . It should be easy also on other operative systems, please Google :-) Hint: Browser Plugins To only forward certain webpages through this tunnel, one can use browser plugins like SwitchyOmega (for Chrome , FireFox ) which allow you manual filtering. Often we need to access our pc at CERN from the internet via 'lxplus'. To avoid to make two ssh's you can configure a new host by adding these lines on '~/.ssh/config': Hint: One Time Command If you only want to connect once and not change your ssh-config, you can use ssh -J my_nice_username@lxtunnel.cern.ch my_local_username@my_office_pc.cern.ch via ProxyJump Host lxtunnel HostName lxtunnel.cern.ch User my_nice_username Host office_cern User my_local_username HostName my_office_pc.cern.ch ProxyJump lxtunnel via ProxyCommand Host lxtunnel HostName lxtunnel.cern.ch User my_nice_username Host office_cern ProxyCommand ssh -q lxtunnel nc my_office_pc.cern.ch 22 where you have to replace my_nice_username , my_local_username and my_office_pc . And then simply type from the terminal ssh office_cern In that case first you need to enter you my_nice_username and my_office_pc passwords, unless you delegate your Kerberos Credentials (for more details see again CERN recommendations ) and/or have a public key authentication for your office-pc set up. ssh-config example # Delegate Kerberos credentials to all things CERN Host *.cern.ch lxplus lxplus? lxtunnel cs-ccr-dev? cs-ccr-optics? dev? optics? User my_nice_username GSSAPITrustDns yes GSSAPIAuthentication yes GSSAPIDelegateCredentials yes ServerAliveInterval 60 # shorthands, e.g. `ssh lxplus` `ssh lxplus8` Host lxplus? lxplus lxtunnel cs-ccr-dev? cs-ccr-optics? Hostname %h.cern.ch # shorthands, e.g. `ssh dev3` Host dev? optics? Hostname cs-ccr-%h.cern.ch # connect to office from inside GPN Host *office_cern HostName my_office_pc.cern.ch User my_local_username IdentityFile path_to_office_pc_private_key # remove if not set up # connect to office from home Host extern_* ProxyJump lxplus.cern.ch Then you can connect from the GPN via ssh office_cern and from home ssh extern_office_cern . Warning ssh extern_dev3 will not work with this setup, as this will try to resolve cs-ccr-extern_dev3.cern.ch See also more info on the ssh config file .","title":"SSH tunnel"},{"location":"guides/sshtunnel/#ssh-tunnel-to-browse-cern-internal-websites","text":"In the following you find a few options to access web pages as from the CERN General Public Network while being outside CERN. According to CERN recommendations lxtunnel.cern.ch is to be preferred over lxplus.cern.ch for tunneling, as this is its only purpose while lxplus provides a fully usable environment.","title":"SSH tunnel to browse CERN internal websites"},{"location":"guides/sshtunnel/#using-sshuttle","text":"sshuttle allows forwarding of specific connections through the CERN network. It requires some configuration to forward the correct connections: from Davide #!/bin/sh # From https://codimd.web.cern.ch/vjC8BHbTS7etHwJve-K2Uw case $1 in connect ) sshuttle --dns -v --remote dgamba@lxplus.cern.ch 128 .141.0.0/16 128 .142.0.0/16 137 .138.0.0/16 172 .18.0.0/16 185 .249.56.0/22 188 .0.0.0/8 192 .65.196.0/23 192 .91.242.0/24 194 .12.128.0/18 2001 :1458::/32 2001 :1459::/32 --daemon --pidfile /tmp/sshuttle.pid shift ;; disconnect ) kill ` cat /tmp/sshuttle.pid ` shift ;; * ) # unknown option echo \"Unknown option\\nUsage:\" echo \"\\t $0 connect : to start VPN-like connection to CERN\" echo \"\\t $0 disconnect : to stop it\" ;; esac from Riccardo #!/bin/bash kinit IP = ` host lxtunnel.cern.ch | awk 'NR==2 {print $4}' ` echo $IP sshuttle --dns -x $IP --remote = $IP \\ --pidfile /tmp/sshuttle.pid --python = python \\ --ssh-cmd 'ssh -o GSSAPIAuthentication=yes -o GSSAPIDelegateCredentials=yes' \\ 10 .0.0.0/8 \\ 100 .64.0.0/10 \\ 10 .100.0.0/16 \\ 10 .254.0.0/16 \\ 10 .76.0.0/15 \\ 128 .141.0.0/16 \\ 128 .142.0.0/16 \\ 137 .138.0.0/16 \\ 172 .16.0.0/12 \\ 185 .249.56.0/22 \\ 188 .184.0.0/15 \\ 188 .184.0.0/16 \\ 188 .185.0.0/15 \\ 188 .185.0.0/16 \\ 192 .16.155.0/24 \\ 192 .16.156.0/22 \\ 192 .16.160.0/22 \\ 192 .16.164.0/23 \\ 192 .16.166.0/24 \\ 192 .65.183.0/24 \\ 192 .65.184.0/21 \\ 192 .65.192.0/22 \\ 192 .65.196.0/23 \\ 192 .91.236.0/22 \\ 192 .91.240.0/22 \\ 192 .91.242.0/24 \\ 192 .91.244.0/23 \\ 192 .91.246.0/24 \\ 194 .12.128.0/18","title":"Using sshuttle"},{"location":"guides/sshtunnel/#ssh-tunnel-through-lxpluslxtunnel","text":"(from cern.ch/bblumi) you can use the following can create an ssh tunnel through the lxplus service . This can be useful to access wikis.cern.ch, timber.cern.ch, issues.cern.ch, ect. You can used SSH to create the tunnel (in a terminal): lxtunnel ssh -D 8888 lxtunnel.cern.ch lxplus ssh -D 8888 lxplus.cern.ch Then set as SOCKS proxy in your network configuration localhost:8888 . Other OS On MacOS 10.15 this is done by (System Preferences) -> Network -> Advanced -> Proxies . It should be easy also on other operative systems, please Google :-) Hint: Browser Plugins To only forward certain webpages through this tunnel, one can use browser plugins like SwitchyOmega (for Chrome , FireFox ) which allow you manual filtering. Often we need to access our pc at CERN from the internet via 'lxplus'. To avoid to make two ssh's you can configure a new host by adding these lines on '~/.ssh/config': Hint: One Time Command If you only want to connect once and not change your ssh-config, you can use ssh -J my_nice_username@lxtunnel.cern.ch my_local_username@my_office_pc.cern.ch via ProxyJump Host lxtunnel HostName lxtunnel.cern.ch User my_nice_username Host office_cern User my_local_username HostName my_office_pc.cern.ch ProxyJump lxtunnel via ProxyCommand Host lxtunnel HostName lxtunnel.cern.ch User my_nice_username Host office_cern ProxyCommand ssh -q lxtunnel nc my_office_pc.cern.ch 22 where you have to replace my_nice_username , my_local_username and my_office_pc . And then simply type from the terminal ssh office_cern In that case first you need to enter you my_nice_username and my_office_pc passwords, unless you delegate your Kerberos Credentials (for more details see again CERN recommendations ) and/or have a public key authentication for your office-pc set up. ssh-config example # Delegate Kerberos credentials to all things CERN Host *.cern.ch lxplus lxplus? lxtunnel cs-ccr-dev? cs-ccr-optics? dev? optics? User my_nice_username GSSAPITrustDns yes GSSAPIAuthentication yes GSSAPIDelegateCredentials yes ServerAliveInterval 60 # shorthands, e.g. `ssh lxplus` `ssh lxplus8` Host lxplus? lxplus lxtunnel cs-ccr-dev? cs-ccr-optics? Hostname %h.cern.ch # shorthands, e.g. `ssh dev3` Host dev? optics? Hostname cs-ccr-%h.cern.ch # connect to office from inside GPN Host *office_cern HostName my_office_pc.cern.ch User my_local_username IdentityFile path_to_office_pc_private_key # remove if not set up # connect to office from home Host extern_* ProxyJump lxplus.cern.ch Then you can connect from the GPN via ssh office_cern and from home ssh extern_office_cern . Warning ssh extern_dev3 will not work with this setup, as this will try to resolve cs-ccr-extern_dev3.cern.ch See also more info on the ssh config file .","title":"SSH tunnel through lxplus/lxtunnel"},{"location":"guides/vim_autocomplete/","text":"Customize VIM for Python development (with autocomplete) Prepared by G. Iadarola This recipe was tested on Linux (Ubuntu). It is partially based on this recipe found here . Optional: Install your own vim (without sudo) YouCompleteMe requires a recent version of Vim compiled with python support (see requirements here ). You can install it by the following steps: cd ~ mkdir vim_source cd vim_source/ git clone https://github.com/vim/vim.git cd vim ./configure --enable-python3interp --prefix = /home/giadarol/myvim make make install (here /home/giadarol is my home folder). Add myvim/bin to your PATH, for example by adding in your .bashrc file: export PATH = /home/giadarol/myvim/bin: $PATH Install Vundle Based on this guide Clone Vundle git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim Place the following content at the beginning of your \".vimrc\" file ( ~/.vimrc - create the file if not existing): set nocompatible \" be iMproved, required filetype off \" required \" set the runtime path to include Vundle and initialize set rtp +=~ /.vim/ bundle/Vundle. vim call vundle#begin () \" alternatively, pass a path where Vundle should install plugins \"call vundle#begin('~/some/path/here') \" let Vundle manage Vundle, required Plugin 'VundleVim/Vundle.vim' \" Keep Plugin commands between vundle#begin/end. \" All of your Plugins must be added before the following line call vundle# end () \" required filetype plugin indent on \" required \" To ignore plugin indent changes, instead use: \"filetype plugin on \" \" Brief help \" :PluginList - lists configured plugins \" :PluginInstall - installs plugins; append `!` to update or just :PluginUpdate \" :PluginSearch foo - searches for foo; append `!` to refresh local cache \" :PluginClean - confirms removal of unused plugins; append `!` to auto-approve removal \" \" see :h vundle for more details or wiki for FAQ \" Put your non-Plugin stuff after this line Install vim plugins vim +PluginInstall +qall Install YouCompleteMe Based on this guide Note that cmake is required! Add in \".vimrc\" the line : Plugin 'ycm-core/YouCompleteMe' before the line call vundle#end() . Again install VIM plugins: vim +PluginInstall +qall (It can take a minute or so). Run YCM install script : cd ~/.vim/bundle/YouCompleteMe python3 install.py Some further customization You can add the following to your \".vimrc\" file: set number set hlsearch syntax on set backspace = indent , eol , start set background = dark set showcmd let python_highlight_all = 1 Final .vimrc configuration For reference, the final \".vimrc\" files looks like this: set nocompatible \" be iMproved, required filetype off \" required \" set the runtime path to include Vundle and initialize set rtp +=~ /.vim/ bundle/Vundle. vim call vundle#begin () \" alternatively, pass a path where Vundle should install plugins \"call vundle#begin('~/some/path/here') \" let Vundle manage Vundle, required Plugin 'VundleVim/Vundle.vim' \" Keep Plugin commands between vundle#begin/end. Plugin 'ycm-core/YouCompleteMe' \" All of your Plugins must be added before the following line call vundle# end () \" required filetype plugin indent on \" required \" To ignore plugin indent changes, instead use: \"filetype plugin on \" \" Brief help \" :PluginList - lists configured plugins \" :PluginInstall - installs plugins; append `!` to update or just :PluginUpdate \" :PluginSearch foo - searches for foo; append `!` to refresh local cache \" :PluginClean - confirms removal of unused plugins; append `!` to auto-approve removal \" \" see :h vundle for more details or wiki for FAQ \" Put your non-Plugin stuff after this line set number set hlsearch syntax on set backspace = indent , eol , start set background = dark set showcmd let python_highlight_all = 1","title":"Configure Vim for Python development"},{"location":"guides/vim_autocomplete/#customize-vim-for-python-development-with-autocomplete","text":"Prepared by G. Iadarola This recipe was tested on Linux (Ubuntu). It is partially based on this recipe found here .","title":"Customize VIM for Python development (with autocomplete)"},{"location":"guides/vim_autocomplete/#optional-install-your-own-vim-without-sudo","text":"YouCompleteMe requires a recent version of Vim compiled with python support (see requirements here ). You can install it by the following steps: cd ~ mkdir vim_source cd vim_source/ git clone https://github.com/vim/vim.git cd vim ./configure --enable-python3interp --prefix = /home/giadarol/myvim make make install (here /home/giadarol is my home folder). Add myvim/bin to your PATH, for example by adding in your .bashrc file: export PATH = /home/giadarol/myvim/bin: $PATH","title":"Optional: Install your own vim (without sudo)"},{"location":"guides/vim_autocomplete/#install-vundle","text":"Based on this guide Clone Vundle git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim Place the following content at the beginning of your \".vimrc\" file ( ~/.vimrc - create the file if not existing): set nocompatible \" be iMproved, required filetype off \" required \" set the runtime path to include Vundle and initialize set rtp +=~ /.vim/ bundle/Vundle. vim call vundle#begin () \" alternatively, pass a path where Vundle should install plugins \"call vundle#begin('~/some/path/here') \" let Vundle manage Vundle, required Plugin 'VundleVim/Vundle.vim' \" Keep Plugin commands between vundle#begin/end. \" All of your Plugins must be added before the following line call vundle# end () \" required filetype plugin indent on \" required \" To ignore plugin indent changes, instead use: \"filetype plugin on \" \" Brief help \" :PluginList - lists configured plugins \" :PluginInstall - installs plugins; append `!` to update or just :PluginUpdate \" :PluginSearch foo - searches for foo; append `!` to refresh local cache \" :PluginClean - confirms removal of unused plugins; append `!` to auto-approve removal \" \" see :h vundle for more details or wiki for FAQ \" Put your non-Plugin stuff after this line Install vim plugins vim +PluginInstall +qall","title":"Install Vundle"},{"location":"guides/vim_autocomplete/#install-youcompleteme","text":"Based on this guide Note that cmake is required! Add in \".vimrc\" the line : Plugin 'ycm-core/YouCompleteMe' before the line call vundle#end() . Again install VIM plugins: vim +PluginInstall +qall (It can take a minute or so). Run YCM install script : cd ~/.vim/bundle/YouCompleteMe python3 install.py","title":"Install YouCompleteMe"},{"location":"guides/vim_autocomplete/#some-further-customization","text":"You can add the following to your \".vimrc\" file: set number set hlsearch syntax on set backspace = indent , eol , start set background = dark set showcmd let python_highlight_all = 1","title":"Some further customization"},{"location":"guides/vim_autocomplete/#final-vimrc-configuration","text":"For reference, the final \".vimrc\" files looks like this: set nocompatible \" be iMproved, required filetype off \" required \" set the runtime path to include Vundle and initialize set rtp +=~ /.vim/ bundle/Vundle. vim call vundle#begin () \" alternatively, pass a path where Vundle should install plugins \"call vundle#begin('~/some/path/here') \" let Vundle manage Vundle, required Plugin 'VundleVim/Vundle.vim' \" Keep Plugin commands between vundle#begin/end. Plugin 'ycm-core/YouCompleteMe' \" All of your Plugins must be added before the following line call vundle# end () \" required filetype plugin indent on \" required \" To ignore plugin indent changes, instead use: \"filetype plugin on \" \" Brief help \" :PluginList - lists configured plugins \" :PluginInstall - installs plugins; append `!` to update or just :PluginUpdate \" :PluginSearch foo - searches for foo; append `!` to refresh local cache \" :PluginClean - confirms removal of unused plugins; append `!` to auto-approve removal \" \" see :h vundle for more details or wiki for FAQ \" Put your non-Plugin stuff after this line set number set hlsearch syntax on set backspace = indent , eol , start set background = dark set showcmd let python_highlight_all = 1","title":"Final .vimrc configuration"}]}